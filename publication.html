<!DOCTYPE html>
<html lang="en">

<head>
	<!-- Required meta tags -->
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
	<!-- Bootstrap core CSS -->
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/css/bootstrap.min.css" integrity="sha384-/Y6pD6FV/Vv2HJnA6t+vslU6fwYXjCFtcEpHbNJ0lyAFsXTsjBbfaDjzALeQsN6M" crossorigin="anonymous">
	<!-- https://fontawesome.com/cheatsheet -->
	<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons/css/academicons.min.css">
	<!-- Custom styles for this template -->
	<link rel="stylesheet" href="css/style.css">
	<link rel="icon" href="picture/hku.png">
    <script src="js/main.js"></script>
    <script src="js/scroll.js"></script>
    <meta name="keywords" content="Hengshuang Zhao, HKU, MIT, Oxford, CUHK, HUST, Computer Vision, Machine, Learning, Artificial Intelligence"> 
	<meta name="description" content="Personal page of Hengshuang Zhao">
</head>

<title>Hengshuang Zhao</title>

<body>
	<nav class="navbar navbar-expand-md navbar-dark fixed-top bg-dark" id="Home">
		<div class="container">
		<a class="navbar-brand" href="index.html">Hengshuang Zhao</a>
		<div class="collapse navbar-collapse" id="navbarToggle">
			<ul class="navbar-nav ml-auto">
				<li class="nav-item">
					<a class="nav-link" href="index.html">Home</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="publication.html">Publication</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="lab.html">Lab</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="award.html">Award</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="service.html">Service</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="talk.html">Talk</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="more.html">More</a>
				</li>
			</ul>
		</div>
		</div>
	</nav>

	<!-- Publication -->
	<div class="container" style="padding-top: 20px; font-size: 17px;">
		<h3 id="Publications" style="padding-top: 80px; margin-top: -80px;">Publications</h3>
		<p><a href="http://scholar.google.com/citations?user=4uE10I0AAAAJ&hl">Google Scholar</a> and <a href="paper/bib.txt">Bib</a>.</p>

		<div class="row">
			<div class="col-md-3"><div class="badge">SIGGRAPH</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""><source src="teaser/videoanydoor.mp4" type="video/mp4"></video></div>
			<div class="col-md-9">
			<b><font color="black">VideoAnydoor : High-fidelity Video Object Insertion with Precise Motion Control</font></b><br>
			Yuanpeng Tu, Hao Luo, Xi Chen, Sihui Ji, Xiang Bai, <b>Hengshuang Zhao</b>.<br>
			<b>SIGGRAPH</b>, 2025.</br>
			[<a href="https://videoanydoor.github.io">Project</a>]
			[<a href="https://arxiv.org/abs/2501.01427">Paper</a>]
			[Code]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">SIGGRAPH</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""><source src="teaser/layerflow.mp4" type="video/mp4"></video></div>
			<div class="col-md-9">
			<b><font color="black">LayerFlow: A Unified Model for Layer-aware Video Generation</font></b><br>
			Sihui Ji, Hao Luo, Xi Chen, Yuanpeng Tu, Yiyang Wang, <b>Hengshuang Zhao</b>.<br>
			<b>SIGGRAPH</b>, 2025.</br>
			[Project]
			[Paper]
			[Code]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">SIGGRAPH</div><img class="img-fluid img-rounded" src="teaser/fashioncomposer.gif"></div>
			<div class="col-md-9">
			<b><font color="black">FashionComposer: Compositional Fashion Image Generation</font></b><br>
			Sihui Ji, Yiyang Wang, Xi Chen, Xiaogang Xu, Hao Luo, <b>Hengshuang Zhao</b>.<br>
			<b>SIGGRAPH</b>, 2025.</br>
			[<a href="https://sihuiji.github.io/FashionComposer-Page">Project</a>]
			[<a href="https://arxiv.org/abs/2412.14168">Paper</a>]
			[<a href="https://github.com/SihuiJi/FashionComposer">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">SIGGRAPH</div><img class="img-fluid img-rounded" src="teaser/dreammask.png"></div>
			<div class="col-md-9">
			<b><font color="black">DreamMask: Boosting Open-vocabulary Panoptic Segmentation with Synthetic Data</font></b><br>
			Yuanpeng Tu, Xi Chen, Ser-Nam Lim, <b>Hengshuang Zhao</b>.<br>
			<b>SIGGRAPH</b>, 2025.</br>
			[<a href="https://yuanpengtu.github.io/Dreammask-Page">Project</a>]
			[<a href="https://arxiv.org/abs/2501.02048">Paper</a>]
			[Code]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ICML</div><img class="img-fluid img-rounded" src="teaser/vip.png"></div>
			<div class="col-md-9">
			<b><font color="black">VIP: Vision Instructed Pre-training for Robotic Manipulation</font></b><br>
			Zhuoling Li, Liangliang Ren, Jinrong Yang, Yong Zhao, Xiaoyang Wu, Zhenhua Xu, Xiang Bai, <b>Hengshuang Zhao</b>.<br>
			International Conference on Machine Learning (<b>ICML</b>), 2025.</br>
			[<a href="">Project</a>]
			[<a href="https://arxiv.org/abs/2410.07169">Paper</a>]
			[<a href="">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ICML</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""><source src="teaser/larm.mp4" type="video/mp4"></video></div>
			<div class="col-md-9">
			<b><font color="black">LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence</font></b><br>
			Zhuoling Li, Xiaogang Xu, Zhenhua Xu, Ser-Nam Lim, <b>Hengshuang Zhao</b>.<br>
			International Conference on Machine Learning (<b>ICML</b>), 2025.</br>
			[<a href="https://lizhuoling.github.io/LARM_webpage">Project</a>]
			[<a href="https://arxiv.org/pdf/2405.17424">Paper</a>]
			[<a href="https://www.youtube.com/watch?v=7ajieVAuob8">Demo</a>]
			[<a href="https://www.youtube.com/watch?v=JxaqPpDPnec">Video</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ICML</div><img class="img-fluid img-rounded" src="teaser/haplovl.png"></div>
			<div class="col-md-9">
			<b><font color="black">HaploVL: A Single-Transformer Baseline for Multi-Modal Understanding</font></b><br>
			Rui Yang, Lin Song, Yicheng Xiao, Runhui Huang, Yixiao Ge, Ying Shan, <b>Hengshuang Zhao</b>.<br>
			International Conference on Machine Learning (<b>ICML</b>), 2025.</br>
			[<a href="https://arxiv.org/abs/2503.14694">Paper</a>]
			[<a href="https://github.com/Tencent/HaploVLM">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ICML</div><img class="img-fluid img-rounded" src="teaser/bood.png"></div>
			<div class="col-md-9">
			<b><font color="black">BOOD: Boundary-based Out-Of-Distribution Data Generation</font></b><br>
			Qilin Liao, Shuo Yang, Bo Zhao, Ping Luo, <b>Hengshuang Zhao</b>.<br>
			International Conference on Machine Learning (<b>ICML</b>), 2025.</br>
			[Paper]
			[Code]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ICML</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""><source src="teaser/orientanything.mp4" type="video/mp4"></video></div>
			<div class="col-md-9">
			<b><font color="black">Orient Anything: Learning Robust Object Orientation Estimation from Rendering 3D Models</font></b><br>
			Zehan Wang, Ziang Zhang, Tianyu Pang, Chao Du, <b>Hengshuang Zhao</b>, Zhou Zhao.<br>
			International Conference on Machine Learning (<b>ICML</b>), 2025.</br>
			[<a href="https://orient-anything.github.io">Project</a>]
			[<a href="https://arxiv.org/abs/2412.18605">Paper</a>]
			[<a href="https://github.com/SpatialVision/Orient-Anything">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ICML</div><img class="img-fluid img-rounded" src="teaser/tgdpo.png"></div>
			<div class="col-md-9">
			<b><font color="black">TGDPO: Harnessing Token-Level Reward Guidance for Enhancing Direct Preference Optimization</font></b><br>
			Mingkang Zhu, Xi Chen, Zhongdao Wang, Bei Yu, <b>Hengshuang Zhao</b>, Jiaya Jia.<br>
			International Conference on Machine Learning (<b>ICML</b>), 2025.</br>
			[Paper]
			[Code]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR Highlight</div><img class="img-fluid img-rounded" src="teaser/unireal.png"></div>
			<div class="col-md-9">
			<b><font color="black">UniReal: Universal Image Generation and Editing via Learning Real-world Dynamics</font></b><br>
			Xi Chen, Zhifei Zhang, He Zhang, Yuqian Zhou, Soo Ye Kim, Qing Liu, Yijun Li, Jianming Zhang, Nanxuan Zhao, Yilin Wang, Hui Ding, Zhe Lin, <b>Hengshuang Zhao</b>.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2025. <b><font color="firebrick">Highlight</font></b></br>
			[<a href="https://xavierchen34.github.io/UniReal-Page">Project</a>]
			[<a href="https://arxiv.org/abs/2412.07774">Paper</a>]
			[Code]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR Highlight</div><img class="img-fluid img-rounded" src="teaser/drivegpt4v2.png"></div>
			<div class="col-md-9">
			<b><font color="black">DriveGPT4-V2: Harnessing Large Language Model Capabilities for Enhanced Closed-Loop Autonomous Driving</font></b><br>
			Zhenhua Xu, Yan Bai, Yujia Zhang, Zhuoling Li, Fei Xia, Kwan-Yee K. Wong, Jianqiang Wang, <b>Hengshuang Zhao</b>.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2025. <b><font color="firebrick">Highlight</font></b></br>
			[Project]
			[Paper]
			[Code]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR Highlight</div><img class="img-fluid img-rounded" src="teaser/sonata.png"></div>
			<div class="col-md-9">
			<b><font color="black">Sonata: Self-Supervised Learning of Reliable Point Representations</font></b><br>
			Xiaoyang Wu, Daniel DeTone, Duncan Frost, Tianwei Shen, Chris Xie, Nan Yang, Jakob Engel, Richard Newcombe, <b>Hengshuang Zhao</b>, Julian Straub.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2025. <b><font color="firebrick">Highlight</font></b></br>
			[<a href="https://xywu.me/sonata">Project</a>]
			[<a href="https://arxiv.org/abs/2503.16429">Paper</a>]
			[<a href="https://github.com/facebookresearch/sonata">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/spatialclip.png"></div>
			<div class="col-md-9">
			<b><font color="black">SpatialCLIP: Learning 3D-aware Image Representations from Spatially Discriminative Language</font></b><br>
			Zehan Wang, Sashuai zhou, Shaoxuan He, Haifeng Huang, Lihe Yang, Ziang Zhang, Xize Cheng, Shengpeng Ji, Tao Jin, <b>Hengshuang Zhao</b>, Zhou Zhao.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2025.</br>
			[Project]
			[Paper]
			[Code]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/panda.png"></div>
			<div class="col-md-9">
			<b><font color="black">PanDA: Towards Panoramic Depth Anything with Unlabeled Panoramas and Mobius Spatial Augmentation</font></b><br>
			Zidong Cao, Jinjing Zhu, Weiming Zhang, Hao Ai, Haotian Bai, <b>Hengshuang Zhao</b>, Lin Wang.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2025.</br>
			[<a href="https://caozidong.github.io/PanDA_Depth">Project</a>]
			[<a href="https://arxiv.org/abs/2406.13378">Paper</a>]
			[<a href="https://github.com/caozidong/PanDA">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/view2cap.png"></div>
			<div class="col-md-9">
			<b><font color="black">Empowering Large Language Models with 3D Situation Awareness</font></b><br>
			Zhihao Yuan, Yibo Peng, Jinke Ren, Yinghong Liao, Yatong Han, Chun-Mei Feng, <b>Hengshuang Zhao</b>, Guanbin Li, Shuguang Cui, Zhen Li.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2025.</br>
			[<a href="https://arxiv.org/abs/2503.23024">Paper</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/hiresllava.png"></div>
			<div class="col-md-9">
			<b><font color="black">HiRes-LLaVA: Restoring Fragmentation Input in High-Resolution Large Vision-Language Models</font></b><br>
			Runhui Huang, Xinpeng Ding, Chunwei Wang, Jianhua Han, Yulong Liu, <b>Hengshuang Zhao</b>, Hang Xu, Lu Hou, Wei Zhang, Xiaodan Liang.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2025.</br>
			[<a href="https://arxiv.org/abs/2407.08706">Paper</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/emova.png"></div>
			<div class="col-md-9">
			<b><font color="black">EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions</font></b><br>
			Kai Chen, Yunhao Gou, Runhui Huang, Zhili Liu, Daxin Tan, Jing Xu, Chunwei Wang, Yi Zhu, Yihan Zeng, Kuo Yang, Dingdong Wang, Kun Xiang, Haoyuan Li, Haoli Bai, Jianhua Han, Xiaohui Li, Weike Jin, Nian Xie, Yu Zhang, James T. Kwok, <b>Hengshuang Zhao</b>, Xiaodan Liang, Dit-Yan Yeung, Xiao Chen, Zhenguo Li, Wei Zhang, Qun Liu, Jun Yao, Lanqing Hong, Lu Hou, Hang Xu.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2025.</br>
			[<a href="https://emova-ollm.github.io">Project</a>]
			[<a href="https://arxiv.org/abs/2409.18042">Paper</a>]
			[<a href="https://github.com/emova-ollm/EMOVA">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ICLR</div><img class="img-fluid img-rounded" src="teaser/omnibind.png"></div>
			<div class="col-md-9">
			<b><font color="black">OmniBind: Large-scale Omni Multimodal Representation via Binding Spaces</font></b><br>
			Zehan Wang, Ziang Zhang, Minjie Hong, Hang Zhang, Luping Liu, Rongjie Huang, Xize Cheng, Shengpeng Ji, Tao Jin, <b>Hengshuang Zhao</b>, Zhou Zhao.<br>
			International Conference on Machine Learning (<b>ICLR</b>), 2025.</br>
			[<a href="https://omnibind.github.io">Project</a>]
			[<a href="https://openreview.net/pdf?id=l2izo0z7gu">Paper</a>]
			[<a href="https://github.com/zehanwang01/OmniBind">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">TPAMI</div><img class="img-fluid img-rounded" src="teaser/anydoor2.png"></div>
			<div class="col-md-9">
			<b><font color="black">AnyDoor: Zero-shot Image Customization with Region-to-region Reference</font></b><br>
			Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, <b>Hengshuang Zhao</b>.<br>
			IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>), 2025.</br>
			[<a href="https://ali-vilab.github.io/AnyDoor-Page">Project</a>]
			[<a href="https://ieeexplore.ieee.org/document/10976616">Paper</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">TPAMI</div><img class="img-fluid img-rounded" src="teaser/unimode2.png"></div>
			<div class="col-md-9">
			<b><font color="black">Towards Unified 3D Object Detection via Algorithm and Data Unification</font></b><br>
			Zhuoling Li, Xiaogang Xu, Ser-Nam Lim, <b>Hengshuang Zhao</b>.<br>
			IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>), 2025.</br>
			[<a href="https://lizhuoling.github.io/UniMODE_webpage">Project</a>]
			[<a href="https://arxiv.org/abs/2402.18573">Paper</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">TPAMI</div><img class="img-fluid img-rounded" src="teaser/unimatchv2.png"></div>
			<div class="col-md-9">
			<b><font color="black">UniMatch V2: Pushing the Limit of Semi-Supervised Semantic Segmentation</font></b><br>
			Lihe Yang, Zhen Zhao, <b>Hengshuang Zhao</b>.<br>
			IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>), 2025.</br>
			[<a href="https://arxiv.org/abs/2410.10777">Paper</a>]
			[<a href="https://github.com/LiheYoung/UniMatch-V2">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">TPAMI</div><img class="img-fluid img-rounded" src="teaser/dreamcomposer2.png"></div>
			<div class="col-md-9">
			<b><font color="black">DreamComposer++: Empowering Diffusion Models with Multi-View Conditions for 3D Content Generation</font></b><br>
			Yunhan Yang, Shuo Chen, Yukun Huang, Xiaoyang Wu, Yuan-Chen Guo, Edmund Y. Lam, <b>Hengshuang Zhao</b>, Tong He, Xihui Liu.<br>
			IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>), 2025.</br>
			[<a href="https://ieeexplore.ieee.org/document/10993319">Paper</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">TPAMI</div><img class="img-fluid img-rounded" src="teaser/ponderv2.png"></div>
			<div class="col-md-9">
			<b><font color="black">PonderV2: Improved 3D Representation with A Universal Pre-training Paradigm</font></b><br>
			Haoyi Zhu, Honghui Yang, Xiaoyang Wu, Di Huang, Sha Zhang, Xianglong He, <b>Hengshuang Zhao</b>, Chunhua Shen, Yu Qiao, Tong He, Wanli Ouyang.<br>
			IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>), 2025.</br>
			[<a href="https://ieeexplore.ieee.org/document/10969802">Paper</a>]
			[<a href="https://github.com/OpenGVLab/PonderV2">Code</a>]
            </div>
		</div><hr>
		
		<div class="row">
			<div class="col-md-3"><div class="badge">NeurIPS</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""><source src="teaser/depthanythingv2.mp4" type="video/mp4"></video></div>
			<div class="col-md-9">
			<b><font color="black">Depth Anything V2</font></b><br>
			Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, <b>Hengshuang Zhao</b>.<br>
			Neural Information Processing Systems (<b>NeurIPS</b>), 2024.</br>
			[<a href="https://depth-anything-v2.github.io">Project</a>]
			[<a href="https://arxiv.org/abs/2406.09414">Paper</a>]
			[<a href="https://github.com/DepthAnything/Depth-Anything-V2">Code</a>]
			[<a href="https://huggingface.co/spaces/depth-anything/Depth-Anything-V2">Demo</a>]
			[<a href="https://x.com/_akhaliq/status/1801432403665125738">Media</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">NeurIPS</div><img class="img-fluid img-rounded" src="teaser/mimicbrush.gif"></div>
			<div class="col-md-9">
			<b><font color="black">Zero-shot Image Editing with Reference Imitation</font></b><br>
			Xi Chen, Yutong Feng, Mengting Chen, Yiyang Wang, Shilong Zhang, Yu Liu, Yujun Shen, <b>Hengshuang Zhao</b>.<br>
			Neural Information Processing Systems (<b>NeurIPS</b>), 2024.</br>
			[<a href="https://xavierchen34.github.io/MimicBrush-Page">Project</a>]
			[<a href="https://arxiv.org/abs/2406.07547">Paper</a>]
			[<a href="https://github.com/ali-vilab/MimicBrush">Code</a>]
			[<a href="https://huggingface.co/spaces/xichenhku/MimicBrush">Demo</a>]
			[<a href="https://x.com/_akhaliq/status/1800726257098760584">Media</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">NeurIPS</div><img class="img-fluid img-rounded" src="teaser/lit.jpg"></div>
			<div class="col-md-9">
			<b><font color="black">LiT: Unifying LiDAR "Languages" with LiDAR Translator</font></b><br>
			Yixing Lao, Tao Tang, Xiaoyang Wu, Peng Chen, Kaicheng Yu, <b>Hengshuang Zhao</b>.<br>
			Neural Information Processing Systems (<b>NeurIPS</b>), 2024.</br>
			[<a href="https://yxlao.github.io/lit">Project</a>]
			[<a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/aa76025af7f8d69338c4b5ee29f66e70-Paper-Conference.pdf">Paper</a>]
			[<a href="https://github.com/yxlao/lit">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">NeurIPS</div><img class="img-fluid img-rounded" src="teaser/syncvis.jpg"></div>
			<div class="col-md-9">
			<b><font color="black">SyncVIS: Synchronized Video Instance Segmentation</font></b><br>
			Rongkun Zheng, Lu Qi, Xi Chen, Yi Wang, Kun Wang, Yu Qiao, <b>Hengshuang Zhao</b>.<br>
			Neural Information Processing Systems (<b>NeurIPS</b>), 2024.</br>
			[<a href="https://arxiv.org/abs/2412.00882">Paper</a>]
			[<a href="https://github.com/rkzheng99/SyncVIS">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">NeurIPS</div><img class="img-fluid img-rounded" src="teaser/onedet3d.png"></div>
			<div class="col-md-9">
			<b><font color="black">One for All: Multi-Domain Joint Training for Point Cloud Based 3D Object Detection</font></b><br>
			Zhenyu Wang, Yali Li, <b>Hengshuang Zhao<sup>†</sup></b>, Shengjin Wang. (†: corresponding)<br>
			Neural Information Processing Systems (<b>NeurIPS</b>), 2024.</br>
			[<a href="https://arxiv.org/abs/2411.01584">Paper</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">NeurIPS</div><img class="img-fluid img-rounded" src="teaser/lion.jpg"></div>
			<div class="col-md-9">
			<b><font color="black">LION: Linear Group RNN for 3D Object Detection in Point Clouds</font></b><br>
			Zhe Liu, Jinghua Hou, Xinyu Wang, Xiaoqing Ye, Jingdong Wang, <b>Hengshuang Zhao</b>, Xiang Bai.<br>
			Neural Information Processing Systems (<b>NeurIPS</b>), 2024.</br>
			[<a href="https://happinesslz.github.io/projects/LION">Project</a>]
			[<a href="https://arxiv.org/abs/2407.18232">Paper</a>]
			[<a href="https://github.com/happinesslz/LION">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ECCV</div><img class="img-fluid img-rounded" src="teaser/livephoto.gif"></div>
			<div class="col-md-9">
			<b><font color="black">LivePhoto: Real Image Animation with Text-guided Motion Control</font></b><br>
			Xi Chen, Zhiheng Liu, Mengting Chen, Yutong Feng, Yu Liu, Yujun Shen, <b>Hengshuang Zhao</b>.<br>
			European Conference on Computer Vision (<b>ECCV</b>), 2024.<br>
			[<a href="https://xavierchen34.github.io/LivePhoto-Page">Project</a>]
			[<a href="https://arxiv.org/abs/2312.02928">Paper</a>]
			[<a href="https://github.com/XavierCHEN34/LivePhoto">Code</a>]
			[<a href="https://www.youtube.com/watch?v=M2vzrTYAsQI">Video</a>]
            </div>
		</div><hr>

		<div class="row">
			<!-- <div class="col-md-3"><div class="badge">ECCV</div><img class="img-fluid img-rounded" src="teaser/pixelgs.png"></div> -->
			<div class="col-md-3"><div class="badge">ECCV</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""><source src="teaser/pixelgs.mp4" type="video/mp4"></video></div>
			<div class="col-md-9">
			<b><font color="black">Pixel-GS: Density Control with Pixel-aware Gradient for 3D Gaussian Splatting</font></b><br>
			Zheng Zhang, Wenbo Hu, Yixing Lao, Tong He, <b>Hengshuang Zhao</b>.<br>
			European Conference on Computer Vision (<b>ECCV</b>), 2024.<br>
			[<a href="https://pixelgs.github.io">Project</a>]
			[<a href="https://arxiv.org/abs/2403.15530">Paper</a>]
			[<a href="https://github.com/zhengzhang01/Pixel-GS">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ECCV</div><img class="img-fluid img-rounded" src="teaser/insmapper.png"></div>
			<div class="col-md-9">
			<b><font color="black">InsMapper: Exploring Inner-instance Information for Vectorized HD Mapping</font></b><br>
			Zhenhua Xu, Kwan-Yee. K. Wong, <b>Hengshuang Zhao</b>.<br>
			European Conference on Computer Vision (<b>ECCV</b>), 2024.<br>
			[<a href="https://tonyxuqaq.github.io/InsMapper">Project</a>]
			[<a href="https://arxiv.org/abs/2308.08543">Paper</a>]
			[<a href="https://github.com/TonyXuQAQ/InsMapper">Code</a>]
			[<a href="https://youtu.be/MapB7TrNnLY">Video</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ECCV</div><img class="img-fluid img-rounded" src="teaser/ovuni3detr.png"></div>
			<div class="col-md-9">
			<b><font color="black">OV-Uni3DETR: Towards Unified Open-Vocabulary 3D Object Detection via Cycle-Modality Propagation</font></b><br>
			Zhenyu Wang, Yali Li, Taichi Liu, <b>Hengshuang Zhao<sup>†</sup></b>, Shengjin Wang. (†: corresponding)<br>
			European Conference on Computer Vision (<b>ECCV</b>), 2024.<br>
			[<a href="https://arxiv.org/abs/2403.19580">Paper</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ECCV</div><img class="img-fluid img-rounded" src="teaser/logosticker.png"></div>
			<div class="col-md-9">
			<b><font color="black">LogoSticker: Inserting Logos into Diffusion Models for Customized Generation</font></b><br>
			Mingkang Zhu, Xi Chen, Zhongdao Wang, <b>Hengshuang Zhao</b>, Jiaya Jia.<br>
			European Conference on Computer Vision (<b>ECCV</b>), 2024.<br>
			[<a href="https://mingkangz.github.io/logosticker">Project</a>]
			[<a href="https://arxiv.org/abs/2407.13752">Paper</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ECCV</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""><source src="teaser/openins3d.mp4" type="video/mp4"></video></div>
			<div class="col-md-9">
			<b><font color="black">OpenIns3D: Snap and Lookup for 3D Open-vocabulary Instance Segmentation</font></b><br>
			Zhening Huang, Xiaoyang Wu, Xi Chen, <b>Hengshuang Zhao<sup>†</sup></b>, Lei Zhu, Joan Lasenby. (†: corresponding)<br>
			European Conference on Computer Vision (<b>ECCV</b>), 2024.<br>
			[<a href="https://zheninghuang.github.io/OpenIns3D">Project</a>]
			[<a href="https://arxiv.org/abs/2309.00616">Paper</a>]
			[<a href="https://github.com/Pointcept/OpenIns3D">Code</a>]
			[<a href="https://www.youtube.com/watch?v=kwlMJkEfTyY">Video</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ECCV</div><img class="img-fluid img-rounded" src="teaser/diki.png"></div>
			<div class="col-md-9">
			<b><font color="black">Mind the Interference: Retaining Pre-trained Knowledge in Parameter Efficient Continual Learning of Vision-Language Models</font></b><br>
			Longxiang Tang, Zhuotao Tian, Kai Li, Chunming He, Hantao Zhou, <b>Hengshuang Zhao</b>, Xiu Li, Jiaya Jia.<br>
			European Conference on Computer Vision (<b>ECCV</b>), 2024.<br>
			[<a href="https://arxiv.org/abs/2407.05342">Paper</a>]
			[<a href="https://github.com/lloongx/DIKI">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""><source src="teaser/depthanything.mp4" type="video/mp4"></video></div>
			<div class="col-md-9">
			<b><font color="black">Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data</font></b><br>
			Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, <b>Hengshuang Zhao</b>.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024.</br>
			[<a href="https://depth-anything.github.io">Project</a>]
			[<a href="https://arxiv.org/abs/2401.10891">Paper</a>]
			[<a href="https://github.com/LiheYoung/Depth-Anything">Code</a>]
			[<a href="https://huggingface.co/spaces/LiheYoung/Depth-Anything">Demo</a>]
			[<a href="https://twitter.com/_akhaliq/status/1749284669936275463">Media</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/anydoor.gif"></div>
			<div class="col-md-9">
			<b><font color="black">AnyDoor: Zero-shot Object-level Image Customization</font></b><br>
			Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, <b>Hengshuang Zhao</b>.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024.</br>
			[<a href="https://ali-vilab.github.io/AnyDoor-Page">Project</a>]
			[<a href="https://arxiv.org/abs/2307.09481">Paper</a>]
			[<a href="https://github.com/ali-vilab/AnyDoor">Code</a>]
			[<a href="https://huggingface.co/spaces/xichenhku/AnyDoor-online">Demo</a>]
			[<a href="https://twitter.com/_akhaliq/status/1738772616142303728">Media</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR Oral</div><img class="img-fluid img-rounded" src="teaser/pointtransformerv3.png"></div>
			<div class="col-md-9">
			<b><font color="black">Point Transformer V3: Simpler, Faster, Stronger</font></b><br>
			Xiaoyang Wu, Li Jiang, Peng-Shuai Wang, Zhijian Liu, Xihui Liu, Yu Qiao, Wanli Ouyang, Tong He, <b>Hengshuang Zhao</b>.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024. <b><font color="firebrick">Oral</font></b></br>
			Ranked 1st place in the CVPR 2024 <a href="https://waymo.com/open/challenges">Waymo 3D Semantic Segmentation Challenge</a>.</br>
			[<a href="https://arxiv.org/abs/2312.10035">Paper</a>]
			[<a href="https://github.com/Pointcept/PointTransformerV3">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/ppt.png"></div>
			<div class="col-md-9">
			<b><font color="black">Towards Large-scale 3D Representation Learning with Multi-dataset Point Prompt Training</font></b><br>
			Xiaoyang Wu, Zhuotao Tian, Xin Wen, Bohao Peng, Xihui Liu, Kaicheng Yu, <b>Hengshuang Zhao</b>.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024.</br>
			[<a href="https://arxiv.org/abs/2308.09718">Paper</a>]
			[<a href="https://github.com/Pointcept/Pointcept">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR Highlight</div><img class="img-fluid img-rounded" src="teaser/gpt4point.png"></div>
			<div class="col-md-9">
			<b><font color="black">GPT4Point: A Unified Framework for Point-Language Understanding and Generation</font></b><br>
			Zhangyang Qi, Ye Fang, Zeyi Sun, Xiaoyang Wu, Tong Wu, Jiaqi Wang, Dahua Lin, <b>Hengshuang Zhao</b>.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024. <b><font color="firebrick">Highlight</font></b></br>
			[<a href="https://gpt4point.github.io">Project</a>]
			[<a href="https://arxiv.org/abs/2312.02980">Paper</a>]
			[<a href="https://github.com/Pointcept/GPT4Point">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR Highlight</div><img class="img-fluid img-rounded" src="teaser/unimode.png"></div>
			<div class="col-md-9">
			<b><font color="black">UniMODE: Universal Monocular 3D Object Detection</font></b><br>
			Zhuoling Li, Xiaogang Xu, Ser-Nam Lim, <b>Hengshuang Zhao</b>.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024. <b><font color="firebrick">Highlight</font></b></br>
			[<a href="https://arxiv.org/abs/2402.18573">Paper</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/groupcontrast.png"></div>
			<div class="col-md-9">
			<b><font color="black">GroupContrast: Semantic-aware Self-supervised Representation Learning for 3D Understanding</font></b><br>
			Chengyao Wang, Li Jiang, Xiaoyang Wu, Zhuotao Tian, Bohao Peng, <b>Hengshuang Zhao<sup>†</sup></b>, Jiaya Jia. (†: corresponding)<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024.</br>
			[<a href="https://arxiv.org/abs/2403.09639">Paper</a>]
			[<a href="https://github.com/dvlab-research/GroupContrast">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/oacnns.png"></div>
			<div class="col-md-9">
			<b><font color="black">OA-CNNs: Omni-Adaptive Sparse CNNs for 3D Semantic Segmentation</font></b><br>
			Bohao Peng, Xiaoyang Wu, Li Jiang, Yukang Chen, <b>Hengshuang Zhao</b>, Zhuotao Tian, Jiaya Jia.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024.</br>
			[<a href="https://arxiv.org/abs/2403.14418">Paper</a>]
			[<a href="https://github.com/Pointcept/Pointcept">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""><source src="teaser/dreamcomposer.mov" type="video/mp4"></video></div>
			<div class="col-md-9">
			<b><font color="black">DreamComposer: Controllable 3D Object Generation via Multi-View Conditions</font></b><br>
			Yunhan Yang, Yukun Huang, Xiaoyang Wu, Yuan-Chen Guo, Song-Hai Zhang, <b>Hengshuang Zhao</b>, Tong He, Xihui Liu.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024.</br>
			[<a href="https://yhyang-myron.github.io/DreamComposer">Project</a>]
			[<a href="https://arxiv.org/abs/2312.03611">Paper</a>]
			[<a href="https://github.com/yhyang-myron/DreamComposer">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/zsvg3d.png"></div>
			<div class="col-md-9">
			<b><font color="black">Visual Programming for Zero-shot Open-Vocabulary 3D Visual Grounding</font></b><br>
			Zhihao Yuan, Jinke Ren, Chun-Mei Feng, <b>Hengshuang Zhao</b>, Shuguang Cui, Zhen Li.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024.</br>
			[<a href="https://curryyuan.github.io/ZSVG3D">Project</a>]
			[<a href="https://arxiv.org/abs/2311.15383">Paper</a>]
			[<a href="https://github.com/CurryYuan/ZSVG3D">Code</a>]
			[<a href="https://youtu.be/YGP1R3IgVWU">Video</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/unipad.png"></div>
			<div class="col-md-9">
			<b><font color="black">UniPAD: A Universal Pre-training Paradigm for Autonomous Driving</font></b><br>
			Honghui Yang, Sha Zhang, Di Huang, Xiaoyang Wu, Haoyi Zhu, Tong He, Shixiang Tang, <b>Hengshuang Zhao</b>, Qibo Qiu, Binbin Lin, Xiaofei He, Wanli Ouyang.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024.</br>
			[<a href="https://arxiv.org/abs/2310.08370">Paper</a>]
			[<a href="https://github.com/Nightmare-n/UniPAD">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ICLR Highlight</div><img class="img-fluid img-rounded" src="teaser/iba.png"></div>
			<div class="col-md-9">
			<b><font color="black">Influencer Backdoor Attack on Semantic Segmentation</font></b><br>
			Haoheng Lan, Jindong Gu, Philip Torr, <b>Hengshuang Zhao</b>.<br>
			International Conference on Learning Representations (<b>ICLR</b>), 2024. <b><font color="firebrick">Highlight</font></b></br>
			[<a href="https://arxiv.org/abs/2303.12054">Paper</a>]
			[<a href="https://github.com/Maxproto/IBA">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">3DV</div><img class="img-fluid img-rounded" src="teaser/ocbev.png"></div>
			<div class="col-md-9">
			<b><font color="black">OCBEV: Object-Centric BEV Transformer for Multi-View 3D Object Detection</font></b><br>
			Zhangyang Qi, Jiaqi Wang, Xiaoyang Wu, <b>Hengshuang Zhao</b>.<br>
			International Conference on 3D Vision (<b>3DV</b>), 2024.</br>
			[<a href="https://arxiv.org/abs/2306.01738">Paper</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">TPAMI</div><img class="img-fluid img-rounded" src="teaser/lavt2.png"></div>
			<div class="col-md-9">
			<b><font color="black">Language-Aware Vision Transformer for Referring Segmentation</font></b><br>
			Zhao Yang, Jiaqi Wang, Xubing Ye, Yansong Tang, Kai Chen, <b>Hengshuang Zhao</b>, Philip Torr.<br>
			IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>), 2024.</br>
			[<a href="https://ieeexplore.ieee.org/document/10694805">Paper</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">TPAMI</div><img class="img-fluid img-rounded" src="teaser/unidetector2.png"></div>
			<div class="col-md-9">
			<b><font color="black">UniDetector: Towards Universal Object Detection with Heterogeneous Supervision</font></b><br>
			Zhenyu Wang, Yali Li, Xi Chen, Ser-Nam Lim, Antonio Torralba, <b>Hengshuang Zhao<sup>†</sup></b>, Shengjin Wang. (†: corresponding)<br>
			IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>), 2024.</br>
			[<a href="https://ieeexplore.ieee.org/document/10552883">Paper</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">RA-L</div><img class="img-fluid img-rounded" src="teaser/drivegpt4.gif"></div>
			<div class="col-md-9">
			<b><font color="black">DriveGPT4: Interpretable End-to-end Autonomous Driving via Large Language Model</font></b><br>
			Zhenhua Xu, Yujia Zhang, Enze Xie, Zhen Zhao, Yong Guo, Kwan-Yee. K. Wong, Zhenguo Li, <b>Hengshuang Zhao</b>.<br>
			IEEE Robotics and Automation Letters (<b>RA-L</b>), 2024.</br>
			[<a href="https://tonyxuqaq.github.io/projects/DriveGPT4">Project</a>]
			[<a href="https://arxiv.org/abs/2310.01412">Paper</a>]
			[<a href="https://drive.google.com/drive/folders/1PsGL7ZxMMz1ZPDS5dZSjzjfPjuPHxVL5">Code</a>]
			[<a href="https://youtu.be/CPoskBNjHlk">Video</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">RA-L</div><img class="img-fluid img-rounded" src="teaser/grouplane.png"></div>
			<div class="col-md-9">
			<b><font color="black">GroupLane: End-to-End 3D Lane Detection With Channel-Wise Grouping</font></b><br>
			Zhuoling Li, Chunrui Han, Zheng Ge, Jinrong Yang, En Yu, Haoqian Wang, Xiangyu Zhang, <b>Hengshuang Zhao</b>.<br>
			IEEE Robotics and Automation Letters (<b>RA-L</b>), 2024.</br>
			[<a href="https://ieeexplore.ieee.org/abstract/document/10706836">Paper</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">NeurIPS</div><img class="img-fluid img-rounded" src="teaser/freemask.png"></div>
			<div class="col-md-9">
			<b><font color="black">FreeMask: Synthetic Images with Dense Annotations Make Stronger Segmentation Models</font></b><br>
			Lihe Yang, Xiaogang Xu, Bingyi Kang, Yinghuan Shi, <b>Hengshuang Zhao</b>.<br>
			Neural Information Processing Systems (<b>NeurIPS</b>), 2023.</br>
			[<a href="https://arxiv.org/abs/2310.15160">Paper</a>]
			[<a href="https://github.com/LiheYoung/FreeMask">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">NeurIPS</div><img class="img-fluid img-rounded" src="teaser/uni3detr.png"></div>
			<div class="col-md-9">
			<b><font color="black">Uni3DETR: Unified 3D Detection Transformer</font></b><br>
			Zhenyu Wang, Yali Li, Xi Chen, <b>Hengshuang Zhao<sup>†</sup></b>, Shengjin Wang. (†: corresponding)<br>
			Neural Information Processing Systems (<b>NeurIPS</b>), 2023.</br>
			[<a href="https://arxiv.org/abs/2310.05699">Paper</a>]
			[<a href="https://github.com/zhenyuw16/Uni3DETR">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">NeurIPS</div><img class="img-fluid img-rounded" src="teaser/tmtvis.png"></div>
			<div class="col-md-9">
			<b><font color="black">TMT-VIS: Taxonomy-aware Multi-dataset Joint Training for Video Instance Segmentation</font></b><br>
			Rongkun Zheng, Lu Qi, Xi Chen, Yi Wang, Kun Wang, Yu Qiao, <b>Hengshuang Zhao</b>.<br>
			Neural Information Processing Systems (<b>NeurIPS</b>), 2023.</br>
			[<a href="https://arxiv.org/abs/2312.06630">Paper</a>]
			[<a href="https://github.com/rkzheng99/TMT-VIS">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">NeurIPS</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""><source src="teaser/corresnerf.mp4" type="video/mp4"></video></div>
			<div class="col-md-9">
			<b><font color="black">CorresNeRF: Image Correspondence Priors for Neural Radiance Fields</font></b><br>
			Yixing Lao, Xiaogang Xu, Zhipeng Cai, Xihui Liu, <b>Hengshuang Zhao</b>.<br>
			Neural Information Processing Systems (<b>NeurIPS</b>), 2023.</br>
			[<a href="https://yxlao.github.io/corres-nerf">Project</a>]
			[<a href="https://arxiv.org/abs/2312.06642">Paper</a>]
			[<a href="https://github.com/yxlao/corres-nerf">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ICCV</div><img class="img-fluid img-rounded" src="teaser/opsnet.gif"></div>
			<div class="col-md-9">
			<b><font color="black">Open-vocabulary Panoptic Segmentation with Embedding Modulation</font></b><br>
			Xi Chen, Shuang Li, Ser-Nam Lim, Antonio Torralba, <b>Hengshuang Zhao</b>.<br>
			International Conference on Computer Vision (<b>ICCV</b>), 2023.</br>
			[<a href="https://opsnet-page.github.io">Project</a>]
			[<a href="https://arxiv.org/abs/2303.11324">Paper</a>]
			[<a href="https://github.com/XavierCHEN34/OPSNet">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ICCV</div><img class="img-fluid img-rounded" src="teaser/shrinkmatch.png"></div>
			<div class="col-md-9">
			<b><font color="black">Shrinking Class Space for Enhanced Certainty in Semi-Supervised Learning</font></b><br>
			Lihe Yang, Zhen Zhao, Lei Qi, Yu Qiao, Yinghuan Shi, <b>Hengshuang Zhao</b>.<br>
			International Conference on Computer Vision (<b>ICCV</b>), 2023.</br>
			[<a href="https://arxiv.org/abs/2308.06777">Paper</a>]
			[<a href="https://github.com/LiheYoung/ShrinkMatch">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ICCV</div><img class="img-fluid img-rounded" src="teaser/bt2.png"></div>
			<div class="col-md-9">
			<b><font color="black">BT<sup>2</sup>: Backward-compatible Training with Basis Transformation</font></b><br>
			Yifei Zhou, Zilu Li, Abhinav Shrivastava, <b>Hengshuang Zhao</b>, Antonio Torralba, Taipeng Tian, Ser-Nam Lim.<br>
			International Conference on Computer Vision (<b>ICCV</b>), 2023.</br>
			[<a href="https://arxiv.org/abs/2211.03989">Paper</a>]
			[<a href="https://github.com/YifeiZhou02/BT-2">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ICCVW</div><img class="img-fluid img-rounded" src="teaser/sam3d.png"></div>
			<div class="col-md-9">
			<b><font color="black">SAM3D: Segment Anything in 3D Scenes</font></b><br>
			Yunhan Yang, Xiaoyang Wu, Tong He, <b>Hengshuang Zhao</b>, Xihui Liu.<br>
			International Conference on Computer Vision Workshop (<b>ICCVW</b>), 2023.</br>
			[<a href="https://arxiv.org/abs/2306.03908">Paper</a>]
			[<a href="https://github.com/Pointcept/SegmentAnything3D">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/msc.png"></div>
			<div class="col-md-9">
			<b><font color="black">Masked Scene Contrast: A Scalable Framework for Unsupervised 3D Representation Learning</font></b><br>
			Xiaoyang Wu, Xin Wen, Xihui Liu, <b>Hengshuang Zhao</b>.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2023.</br>
			[<a href="https://arxiv.org/abs/2303.14191">Paper</a>]
			[<a href="https://github.com/Pointcept/Pointcept">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/unidetector.png"></div>
			<div class="col-md-9">
			<b><font color="black">Detecting Everything in the Open World: Towards Universal Object Detection</font></b><br>
			Zhenyu Wang, Yali Li, Xi Chen, Ser-Nam Lim, Antonio Torralba, <b>Hengshuang Zhao<sup>†</sup></b>, Shengjin Wang. (†: corresponding)<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2023.</br>
			[<a href="https://arxiv.org/abs/2303.11749">Paper</a>]
			[<a href="https://github.com/zhenyuw16/UniDetector">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/modsquad.png"></div>
			<div class="col-md-9">
			<b><font color="black">Mod-Squad: Designing Mixtures of Experts As Modular Multi-Task Learners</font></b><br>
			Zitian Chen, Yikang Shen, Mingyu Ding, Zhenfang Chen, <b>Hengshuang Zhao</b>, Erik Learned-Miller, Chuang Gan.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2023.</br>
			[<a href="https://vis-www.cs.umass.edu/mod-squad">Proj</a>]
			[<a href="https://arxiv.org/abs/2212.08066">Paper</a>]
			[<a href="https://github.com/UMass-Foundation-Model/Mod-Squad">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">AAAI</div><img class="img-fluid img-rounded" src="teaser/sadlr.png"></div>
			<div class="col-md-9">
			<b><font color="black">Semantics-Aware Dynamic Localization and Refinement for Referring Image Segmentation</font></b><br>
			Zhao Yang, Jiaqi Wang, Yansong Tang, Kai Chen, <b>Hengshuang Zhao</b>, Philip Torr.<br>
			AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2023.</br>
			[<a href="https://arxiv.org/abs/2303.06345">Paper</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">IJCAI</div><img class="img-fluid img-rounded" src="teaser/uada.png"></div>
			<div class="col-md-9">
			<b><font color="black">Universal Adaptive Data Augmentation</font></b><br>
			Xiaogang Xu, <b>Hengshuang Zhao</b>.<br>
			International Joint Conferences on Artificial Intelligence (<b>IJCAI</b>), 2023.</br>
			[<a href="https://arxiv.org/abs/2207.06658">Paper</a>]
			[<a href="https://github.com/xiaogang00/UADA">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">IJCV</div><img class="img-fluid img-rounded" src="teaser/physformer++.png"></div>
			<div class="col-md-9">
			<b><font color="black">PhysFormer++: Facial Video-based Physiological Measurement with SlowFast Temporal Difference Transformer</font></b><br>
			Zitong Yu, Yuming Shen, Jingang Shi, <b>Hengshuang Zhao</b>, Yawen Cui, Jiehua Zhang, Philip Torr, Guoying Zhao.<br>
			International Journal of Computer Vision (<b>IJCV</b>), 2023.</br>
			[<a href="https://arxiv.org/abs/2302.03548">Paper</a>]
			[<a href="https://github.com/ZitongYu/PhysFormer">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">NeurIPS</div><img class="img-fluid img-rounded" src="teaser/pointtransformerv2.png"></div>
			<div class="col-md-9">
			<b><font color="black">Point Transformer V2: Grouped Vector Attention and Partition-based Pooling</font></b><br>
			Xiaoyang Wu, Yixing Lao, Li Jiang, Xihui Liu, <b>Hengshuang Zhao</b>.<br>
			Neural Information Processing Systems (<b>NeurIPS</b>), 2022.</br>
			[<a href="https://arxiv.org/abs/2210.05666">Paper</a>]
			[<a href="https://github.com/Gofinge/PointTransformerV2">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ECCV</div><img class="img-fluid img-rounded" src="teaser/mtformer.png"></div>
			<div class="col-md-9">
			<b><font color="black">MTFormer: Multi-Task Learning via Transformer and Cross-Task Reasoning</font></b><br>
			Xiaogang Xu*, <b>Hengshuang Zhao*</b>, Vibhav Vineet, Ser-Nam Lim, Antonio Torralba. (*: equal contribution)<br>
			European Conference on Computer Vision (<b>ECCV</b>), 2022.</br>
			[<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136870299.pdf">Paper</a>]			
			[<a href="https://github.com/xiaogang00/MTFormer">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ECCV</div><img class="img-fluid img-rounded" src="teaser/segpgd.png"></div>
			<div class="col-md-9">
			<b><font color="black">SegPGD: An Effective and Efficient Adversarial Attack for Evaluating and Boosting Segmentation Robustness</font></b><br>
			Jindong Gu, <b>Hengshuang Zhao</b>, Volker Tresp, Philip Torr.<br>
			European Conference on Computer Vision (<b>ECCV</b>), 2022.</br>
			[<a href="https://arxiv.org/abs/2207.12391">Paper</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ECCV</div><img class="img-fluid img-rounded" src="teaser/decouplenet.png"></div>
			<div class="col-md-9">
			<b><font color="black">DecoupleNet: Decoupled Network for Domain Adaptive Semantic Segmentation</font></b><br>
			Xin Lai, Zhuotao Tian, Xiaogang Xu, Yingcong Chen, Shu Liu, <b>Hengshuang Zhao</b>, Liwei wang, Jiaya Jia.<br>
			European Conference on Computer Vision (<b>ECCV</b>), 2022.</br>
			[<a href="https://arxiv.org/abs/2207.09988">Paper</a>]
			[<a href="https://github.com/dvlab-research/DecoupleNet">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">RSSW</div><img class="img-fluid img-rounded" src="teaser/rsswvsn.png"></div>
			<div class="col-md-9">
			<b><font color="black">Towards Visual Social Navigation in Photo-realistic Indoor Scenes</font></b><br>
			Feng Gao, <b>Hengshuang Zhao</b>, Yu Wang.<br>
			Robotics: Science and Systems (<b>RSS</b>) Workshop, 2022.</br>
			[<a href="https://social-intelligence-human-ai.github.io/RSS2022/docs/camready_3.pdf">Paper</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/focalclick.png"></div>
			<div class="col-md-9">
			<b><font color="black">FocalClick: Towards Practical Interactive Image Segmentation</font></b><br>
			Xi Chen, Zhiyan Zhao, Yilei Zhang, Manni Duan, Donglian Qi, <b>Hengshuang Zhao</b>.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2022.</br>
			[<a href="https://arxiv.org/abs/2204.02574">Paper</a>]
			[<a href="https://github.com/XavierCHEN34/ClickSEG">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/lavt.png"></div>
			<div class="col-md-9">
			<b><font color="black">LAVT: Language-Aware Vision Transformer for Referring Image Segmentation</font></b><br>
			Zhao Yang, Jiaqi Wang, Yansong Tang, Kai Chen, <b>Hengshuang Zhao</b>, Philip Torr.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2022.</br>
			[<a href="https://arxiv.org/abs/2112.02244">Paper</a>]
			[<a href="https://github.com/yz93/LAVT-RIS">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/gfsseg.png"></div>
			<div class="col-md-9">
			<b><font color="black">Generalized Few-shot Semantic Segmentation</font></b><br>
			Zhuotao Tian, Xin Lai, Li Jiang, Michelle Shu, <b>Hengshuang Zhao</b>, Jiaya Jia.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2022.</br>
			[<a href="https://arxiv.org/abs/2010.05210">Paper</a>]
			[<a href="https://github.com/dvlab-research/GFS-Seg">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/physformer.png"></div>
			<div class="col-md-9">
			<b><font color="black">PhysFormer: Facial Video-based Physiological Measurement with Temporal Difference Transformer</font></b><br>
			Zitong Yu, Yuming Shen, Jingang Shi, <b>Hengshuang Zhao</b>, Philip Torr, Guoying Zhao.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2022.</br>
			[<a href="https://arxiv.org/abs/2111.12082">Paper</a>]
			[<a href="https://github.com/ZitongYu/PhysFormer">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/stratifiedtransformer.png"></div>
			<div class="col-md-9">
			<b><font color="black">Stratified Transformer for 3D Point Cloud Segmentation</font></b><br>
			Xin Lai, Jianhui Liu, Li Jiang, Liwei Wang, <b>Hengshuang Zhao</b>, Shu Liu, Xiaojuan Qi, Jiaya Jia.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2022.</br>
			[<a href="https://arxiv.org/abs/2203.14508">Paper</a>]
			[<a href="https://github.com/dvlab-research/Stratified-Transformer">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ICRA</div><img class="img-fluid img-rounded" src="teaser/pvcl.png"></div>
			<div class="col-md-9">
			<b><font color="black">Prototype-Voxel Contrastive Learning for LiDAR Point Cloud Panoptic Segmentation</font></b><br>
			Minzhe Liu, Zhou Qiang, <b>Hengshuang Zhao</b>, Jianing Li, Yuan Du, Kurt Keutzer, Li Du, Shanghang Zhang.<br>
			International Conference on Robotics and Automation (<b>ICRA</b>), 2022.</br>
			[<a href="paper/icra22_pvcl.pdf">Paper</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">TPAMI</div><img class="img-fluid img-rounded" src="teaser/panopticfcn2.png"></div>
			<div class="col-md-9">
			<b><font color="black">Fully Convolutional Networks for Panoptic Segmentation with Point-based Supervision</font></b><br>
			Yanwei Li, <b>Hengshuang Zhao</b>, Xiaojuan Qi, Yukang Chen, Lu Qi, Liwei Wang, Zeming Li, Jian Sun, Jiaya Jia.<br>
			IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>), 2022. </br>
			[<a href="https://arxiv.org/abs/2108.07682">Paper</a>]
			[<a href="https://github.com/dvlab-research/PanopticFCN">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">TPAMI</div><img class="img-fluid img-rounded" src="teaser/entity.png"></div>
			<div class="col-md-9">
			<b><font color="black">Open World Entity Segmentation</font></b><br>
			Lu Qi, Jason Kuen, Yi Wang, Jiuxiang Gu, <b>Hengshuang Zhao</b>, Philip Torr, Zhe Lin, Jiaya Jia.<br>
			IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>), 2022. </br>
			[<a href="http://luqi.info/Entity_Web">Project</a>]
			[<a href="https://arxiv.org/abs/2107.14228">Paper</a>]
			[<a href="https://github.com/dvlab-research/Entity">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">TPAMI</div><img class="img-fluid img-rounded" src="teaser/apd.png"></div>
			<div class="col-md-9">
			<b><font color="black">Adaptive Perspective Distillation for Semantic Segmentation</font></b><br>
			Zhuotao Tian, Pengguang Chen, Xin Lai, Li Jiang, Shu Liu, <b>Hengshuang Zhao</b>, Bei Yu, Ming-Chang Yang, Jiaya Jia.<br>
			IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>), 2022. </br>
			[<a href="paper/tpami22_apd.pdf">Paper</a>]
			[<a href="https://github.com/dvlab-research/APD">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">TPAMI</div><img class="img-fluid img-rounded" src="teaser/set.png"></div>
			<div class="col-md-9">
			<b><font color="black">Patch-based Separable Transformer for Visual Recognition</font></b><br>
			Shuyang Sun, Xiaoyu Yue, <b>Hengshuang Zhao</b>, Philip Torr, Song Bai.<br>
			IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>), 2022. </br>
			[<a href="paper/tpami22_set.pdf">Paper</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">NeurIPS</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""><source src="teaser/unitrack.mp4" type="video/mp4"></video></div>
			<div class="col-md-9">
			<b><font color="black">Do Different Tracking Tasks Require Different Appearance Models?</font></b><br>
			Zhongdao Wang, <b>Hengshuang Zhao</b>, Yali Li, Shengjin Wang, Philip Torr, Luca Bertinetto.<br>
			Neural Information Processing Systems (<b>NeurIPS</b>), 2021.</br>
			[<a href="https://zhongdao.github.io/UniTrack">Project</a>]
			[<a href="https://arxiv.org/abs/2107.02156">Paper</a>]
			[<a href="https://github.com/Zhongdao/UniTrack">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">BMVC</div><img class="img-fluid img-rounded" src="teaser/hinet.gif"></div>
			<div class="col-md-9">
			<b><font color="black">Hierarchical Interaction Network for Video Object Segmentation from Referring Expressions</font></b><br>
			Zhao Yang, Yansong Tang, Luca Bertinetto, <b>Hengshuang Zhao</b>, Philip Torr.<br>
			British Machine Vision Conference (<b>BMVC</b>), 2021.</br>
			[<a href="https://www.bmvc2021-virtualconference.com/conference/papers/paper_0386.html">Paper</a>]
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ICCV Oral</div><img class="img-fluid img-rounded" src="teaser/pointtransformer.png"></div>
			<div class="col-md-9">
			<b><font color="black">Point Transformer</font></b><br>
			<b>Hengshuang Zhao</b>, Li Jiang, Jiaya Jia, Philip Torr, Vladlen Koltun.<br>
			International Conference on Computer Vision (<b>ICCV</b>), 2021. <b><font color="firebrick">Oral</font></b></br>
			[<a href="http://arxiv.org/abs/2012.09164">Paper</a>]
			[<a href="https://github.com/Gofinge/PointTransformerV2/tree/main/pcr/models/point_transformer">Code</a>]
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ICCV</div><img class="img-fluid img-rounded" src="teaser/ddcat.png"></div>
			<div class="col-md-9">
			<b><font color="black">Dynamic Divide-and-Conquer Adversarial Training for Robust Semantic Segmentation</font></b><br>
			Xiaogang Xu, <b>Hengshuang Zhao</b>, Jiaya Jia.<br>
			International Conference on Computer Vision (<b>ICCV</b>), 2021.</br>
			[<a href="http://arxiv.org/abs/2003.06555">Paper</a>]
			[<a href="https://github.com/dvlab-research/Robust-Semantic-Segmentation">Code</a>]
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR Oral</div><img class="img-fluid img-rounded" src="teaser/bpnet.png"></div>
			<div class="col-md-9">
			<b><font color="black">Bidirectional Projection Network for Cross Dimension Scene Understanding</font></b><br>
			Wenbo Hu*, <b>Hengshuang Zhao*</b>, Li Jiang, Jiaya Jia, Tien-Tsin Wong. (*: equal contribution)<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2021. <b><font color="firebrick">Oral</font></b></br>
			[<a href="https://wbhu.github.io/project/BPNet">Project</a>]
			[<a href="https://arxiv.org/abs/2103.14326">Paper</a>]
			[<a href="https://github.com/wbhu/BPNet">Code</a>]
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR Oral</div><img class="img-fluid img-rounded" src="teaser/panopticfcn.png"></div>
			<div class="col-md-9">
			<b><font color="black">Fully Convolutional Networks for Panoptic Segmentation</font></b><br>
			Yanwei Li, <b>Hengshuang Zhao</b>, Xiaojuan Qi, Liwei Wang, Zeming Li, Jian Sun, Jiaya Jia.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2021. <b><font color="firebrick">Oral</font></b></br>
			[<a href="http://arxiv.org/abs/2012.00720">Paper</a>]
			[<a href="http://github.com/yanwei-li/PanopticFCN">Code</a>]
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/reviewkd.png"></div>
			<div class="col-md-9">
			<b><font color="black">Distilling Knowledge via Knowledge Review</font></b><br>
			Pengguang Chen, Shu Liu, <b>Hengshuang Zhao</b>, Jiaya Jia.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2021.</br>
			[<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Distilling_Knowledge_via_Knowledge_Review_CVPR_2021_paper.pdf">Paper</a>]
			[<a href="https://github.com/dvlab-research/ReviewKD">Code</a>]
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/paconv.png"></div>
			<div class="col-md-9">
			<b><font color="black">PAConv: Position Adaptive Convolution with Dynamic Kernel Assembling on Point Clouds</font></b><br>
			Mutian Xu, Runyu Ding, <b>Hengshuang Zhao</b>, Xiaojuan Qi.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2021.</br>
			[<a href="https://arxiv.org/abs/2103.14635">Paper</a>]
			[<a href="https://github.com/CVMI-Lab/PAConv">Code</a>]
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/setr.png"></div>
			<div class="col-md-9">
			<b><font color="black">Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers</font></b><br>
			Sixiao Zheng, Jiachen Lu, <b>Hengshuang Zhao</b>, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip Torr, Li Zhang.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2021.</br>
			[<a href="https://fudan-zvg.github.io/SETR">Project</a>]
			[<a href="https://arxiv.org/abs/2012.15840">Paper</a>]
			[<a href="https://github.com/fudan-zvg/SETR">Code</a>]
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/cac.png"></div>
			<div class="col-md-9">
			<b><font color="black">Semi-supervised Semantic Segmentation with Directional Context-aware Consistency</font></b><br>
			Xin Lai, Zhuotao Tian, Li Jiang, Shu Liu, <b>Hengshuang Zhao</b>, Liwei Wang, Jiaya Jia.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2021.</br>
			[<a href="https://arxiv.org/abs/2106.14133">Paper</a>]
			[<a href="https://github.com/dvlab-research/Context-Aware-Consistency">Code</a>]
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">IJCAI</div><img class="img-fluid img-rounded" src="teaser/dccdn.png"></div>
			<div class="col-md-9">
			<b><font color="black">Dual-Cross Central Difference Network for Face Anti-Spoofing</font></b><br>
			Zitong Yu, Yunxiao Qin, <b>Hengshuang Zhao</b>, Xiaobai Li, Guoying Zhao.<br>
			International Joint Conference on Artificial Intelligence (<b>IJCAI</b>), 2021.</br>
			[<a href="https://arxiv.org/abs/2105.01290">Paper</a>]
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/san.png"></div>
			<div class="col-md-9">
			<b><font color="black">Exploring Self-attention for Image Recognition</font></b><br>
			<b>Hengshuang Zhao</b>, Jiaya Jia, Vladlen Koltun.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2020.</br>
			[<a href="paper/cvpr20_san.pdf">Paper</a>]
			[<a href="http://github.com/hszhao/SAN">Code</a>]
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR Oral</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""><source src="teaser/pointgroup.mp4" type="video/mp4"></video></div>
			<div class="col-md-9">
			<b><font color="black">PointGroup: Dual-Set Point Grouping for 3D Instance Segmentation</font></b><br>
			Li Jiang*, <b>Hengshuang Zhao*</b>, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, Jiaya Jia. (*: equal contribution)<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2020. <b><font color="firebrick">Oral</font></b></br>
			[<a href="http://arxiv.org/abs/2004.01658">Paper</a>]
			[<a href="http://github.com/dvlab-research/PointGroup">Code</a>]
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">TPAMI</div><img class="img-fluid img-rounded" src="teaser/pfenet.png"></div>
			<div class="col-md-9">
			<b><font color="black">Prior Guided Feature Enrichment Network for Few-Shot Segmentation</font></b><br>
			Zhuotao Tian, <b>Hengshuang Zhao<sup>†</sup></b>, Michelle Shu, Zhicheng Yang, Ruiyu Li, Jiaya Jia. (†: corresponding)<br>
			IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>), 2020. </br>
			[<a href="paper/tpami20_pfenet.pdf">Paper</a>]
			[<a href="https://github.com/dvlab-research/PFENet">Code</a>]
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">arXiv</div><img class="img-fluid img-rounded" src="teaser/gridmask.png"></div>
			<div class="col-md-9">
			<b><font color="black">GridMask Data Augmentation</font></b><br>
			Pengguang Chen, Shu Liu, <b>Hengshuang Zhao</b>, Jiaya Jia.<br>
			arXiv, 2020.</br>
			[<a href="http://arxiv.org/abs/2001.04086">Paper</a>]
			[<a href="https://github.com/dvlab-research/GridMask">Code</a>]
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ICCV</div><img class="img-fluid img-rounded" src="teaser/pointedge.png"></div>
			<div class="col-md-9">
			<b><font color="black">Hierarchical Point-Edge Interaction Network for Point Cloud Semantic Segmentation</font></b><br>
			Li Jiang, <b>Hengshuang Zhao</b>, Shu Liu, Xiaoyong Shen, Chi-Wing Fu, Jiaya Jia.<br>
			International Conference on Computer Vision (<b>ICCV</b>), 2019. </br>
			[<a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Jiang_Hierarchical_Point-Edge_Interaction_Network_for_Point_Cloud_Semantic_Segmentation_ICCV_2019_paper.pdf">Paper</a>]
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""><source src="teaser/pointweb.mp4" type="video/mp4"></video></div>
			<div class="col-md-9">
			<b><font color="black">PointWeb: Enhancing Local Neighborhood Features for Point Cloud Processing</font></b><br>
			<b>Hengshuang Zhao*</b>, Li Jiang*, Chi-Wing Fu, and Jiaya Jia. (*: equal contribution)<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2019.</br>
			[<a href="paper/cvpr19_pointweb.pdf">Paper</a>]
			[<a href="http://github.com/hszhao/PointWeb">Code</a>]
			[<a href="http://youtu.be/CaobqpsUP_4">Video</a>]
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR Oral</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""><source src="teaser/upsnet.mp4" type="video/mp4"></video></div>
			<div class="col-md-9">
			<b><font color="black">UPSNet: A Unified Panoptic Segmentation Network</font></b><br>
			Yuwen Xiong*, Renjie Liao*, <b>Hengshuang Zhao*</b>, Rui Hu, Min Bai, Ersin Yumer, Raquel Urtasun. (*: equal contribution)<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2019. <b><font color="firebrick">Oral</font></b></br>
			[<a href="http://arxiv.org/abs/1901.03784">Paper</a>]
			[<a href="http://github.com/uber-research/UPSNet">Code</a>]
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ECCV</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""><source src="teaser/psanet.mp4" type="video/mp4"></video></div>
			<div class="col-md-9">
			<b><font color="black">PSANet: Point-wise Spatial Attention Network for Scene Parsing</font></b><br>
			<b>Hengshuang Zhao*</b>, Yi Zhang*, Shu Liu, Jianping Shi, Chen Change Loy, Dahua Lin, Jiaya Jia. (*: equal contribution)<br>
			European Conference on Computer Vision (<b>ECCV</b>), 2018.</br>
			Ranked 1st place in the CVPR 2018 <a href="http://bdd-data.berkeley.edu/wad-2018.html">WAD Drivable Area Segmentation Challenge</a>.</br>
			[<a href="project/psanet/index.html">Project</a>]
			[<a href="paper/eccv18_psanet.pdf">Paper</a>]
			[<a href="http://github.com/hszhao/PSANet">Caffe</a>]
			[<a href="http://github.com/hszhao/semseg">PyTorch</a>]
			[<a href="http://youtu.be/l5xu1DI6pDk">Video</a>]
			[<a href="paper/eccv18_psanet_supp.pdf">Supp</a>]
			[<a href="http://docs.google.com/presentation/d/1_brKNBtv8nVu_jOwFRGwVkEPAq8B8hEngBSQuZCWaZA/edit?usp=sharing">Slides in WAD @ CVPR 2018</a>]
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ECCV</div><img class="img-fluid img-rounded" src="teaser/cais.png"></div>
			<div class="col-md-9">
			<b><font color="black">Compositing-aware Image Search</font></b><br>
			<b>Hengshuang Zhao</b>, Xiaohui Shen, Zhe Lin, Kalyan Sunkavalli, Brian Price, Jiaya Jia.<br>
			European Conference on Computer Vision (<b>ECCV</b>), 2018.</br>
			[<a href="project/cais/index.html">Project</a>]
			[<a href="paper/eccv18_cais.pdf">Paper</a>]
			[<a href="paper/eccv18_cais_supp.pdf">Supp</a>]
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ECCV</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""><source src="teaser/segstereo.mp4" type="video/mp4"></video></div>
			<div class="col-md-9">
			<b><font color="black">SegStereo: Exploiting Semantic Information for Disparity Estimation</font></b><br>
			Guorun Yang*, <b>Hengshuang Zhao*</b>, Jianping Shi, Zhidong Deng, Jiaya Jia. (*: equal contribution)<br>
			European Conference on Computer Vision (<b>ECCV</b>), 2018.</br>
			[<a href="project/segstereo/index.html">Project</a>]
			[<a href="paper/eccv18_segstereo.pdf">Paper</a>]
			[<a href="http://github.com/yangguorun/SegStereo">Code</a>]
			[<a href="http://youtu.be/bfrlFpJQHT8">Video</a>]
			[<a href="paper/eccv18_segstereo_supp.pdf">Supp</a>]
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ECCV</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""><source src="teaser/icnet.mp4" type="video/mp4"></video></div>
			<div class="col-md-9">
			<b><font color="black">ICNet for Real-Time Semantic Segmentation on High-Resolution Images</font></b><br>
			<b>Hengshuang Zhao</b>, Xiaojuan Qi, Xiaoyong Shen, Jianping Shi, Jiaya Jia.<br>
			European Conference on Computer Vision (<b>ECCV</b>), 2018.</br>
			[<a href="project/icnet/index.html">Project</a>]
			[<a href="paper/eccv18_icnet.pdf">Paper</a>]
			[<a href="http://github.com/hszhao/ICNet">Code</a>]
			[<a href="http://youtu.be/qWl9idsCuLQ">Video</a>]
			[<a href="paper/eccv18_icnet_supp.pdf">Supp</a>]
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/pspnet.png"></div>
			<div class="col-md-9">
			<b><font color="black">Pyramid Scene Parsing Network</font></b><br>
			<b>Hengshuang Zhao</b>, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, Jiaya Jia.</br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2017.</br>
			Ranked 1st place in the ECCV 2016 <a href="http://image-net.org/challenges/LSVRC/2016/results">ImageNet Scene Parsing Challenge</a>.</br>
			Ranked 1st place in the CVPR 2017 <a href="http://blog.mapillary.com/product/2017/06/13/lsun-challenge.html">LSUN Semantic Segmentation Challenge</a>.</br>
			[<a href="project/pspnet/index.html">Project</a>]
			[<a href="http://arxiv.org/abs/1612.01105">Paper</a>]
			[<a href="http://github.com/hszhao/PSPNet">Caffe</a>]
			[<a href="http://github.com/hszhao/semseg">PyTorch</a>]
			[<a href="http://youtu.be/rB1BmBOkKTw">Video</a>]
			[<a href="http://image-net.org/challenges/talks/2016/SenseCUSceneParsing.pdf">Slides in ILSVRC2016@ECCV2016</a>]
			</div>
		</div><hr>
		
		<div class="row">
			<div class="col-md-3"><div class="badge">ECCV</div><img class="img-fluid img-rounded" src="teaser/weakseg.png"></div>
			<div class="col-md-9">
			<b><font color="black">Augmented Feedback in Semantic Segmentation under Image Level Supervision</font></b><br>
			Xiaojuan Qi, Zhengzhe Liu, Jianping Shi, <b>Hengshuang Zhao</b>, Jiaya Jia.</br>
			European Conference on Computer Vision (<b>ECCV</b>), 2016.</br>
			[<a href="paper/eccv16_weakseg.pdf">Paper</a>]
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">AO</div><img class="img-fluid img-rounded" src="teaser/gpusteger.png"></div>
			<div class="col-md-9">
			<b><font color="black">Rapid and Automatic 3D Body Measurement System based on a GPU-steger Line Detector</font></b><br>
			Xingjian Liu, <b>Hengshuang Zhao</b>, Guomin Zhan, Kai Zhong, Zhongwei Li, YuhJin Chao, Yusheng Shi.</br>
			Applied Optics, 2016.</br>
			[<a href="paper/ao2016_gpusteger.pdf">Paper</a>]
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">SPIE</div><img class="img-fluid img-rounded" src="teaser/conoscopic.png"></div>
			<div class="col-md-9">
			<b><font color="black">A High-reflective Surface Measurement Method based on Conoscopic Holography Technology</font></b><br>
			Xu Cheng, Zhongwei Li, Yusheng Shi, <b>Hengshuang Zhao</b>, Guomin Zhan.</br>
			Optical Metrology and Inspection for Industrial Applications III, 2014.</br>
			[<a href="http://spie.org/Publications/Proceedings/Paper/10.1117/12.2071706">Paper</a>]
			</div>
		</div>

	</div><br>
	<script>var scroll = new SmoothScroll('a[href*="#"]', {speed: 1000});</script>
</body>
</html>