<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <link href="main.css" rel="stylesheet" media="all">
    <meta name="description" content="PSANet: Point-wise Spatial Attention Network for Scene Parsing" />
    <meta name="keywords" content="PSANet: Point-wise Spatial Attention Network for Scene Parsing">
    <script>
    function buttonSwitch(id, text) {
        old_src = document.getElementById(id).src;
        ind = old_src.lastIndexOf('/');
        document.getElementById(id).src = old_src.substr(0, ind + 1) + text;
    }
    </script>
    <title>PSANet: Point-wise Spatial Attention Network for Scene Parsing</title>
</head>

<body>
    <div id="top_arrow" style="position: fixed; bottom: 10px; right: 10px;">
        <a href="#title">
<img src="./figures/top_arrow.jpg" style="border: 0pt none ; width: 26px; height: 32px;"/></a>
    </div>
    <h2 id="title" class="auto-style1">PSANet: Point-wise Spatial Attention Network for Scene Parsing</h2>
    <p class="auto-style7" align="center">
        <a href="https://hszhao.github.io" target="_blank">Hengshuang Zhao</a><sup>1*</sup>&nbsp;&nbsp;&nbsp;
        <a href="https://github.com/EthanZhangYi" target="_blank">Yi Zhang</a><sup>2*</sup>&nbsp;&nbsp;&nbsp;
        <a href="http://shuliu.me" target="_blank">Shu Liu</a><sup>1</sup>&nbsp;&nbsp;&nbsp;
        <a href="http://shijianping.me" target="_blank">Jianping Shi</a><sup>3</sup>&nbsp;&nbsp;&nbsp;
        <a href="http://personal.ie.cuhk.edu.hk/~ccloy" target="_blank">Chen Change Loy</a><sup>4</sup>&nbsp;&nbsp;&nbsp;
        <a href="http://dahua.me" target="_blank">Dahua Lin</a><sup>2</sup>&nbsp;&nbsp;&nbsp;
        <a href="http://www.cse.cuhk.edu.hk/leojia" target="_blank">Jiaya Jia</a><sup>1,5</sup>&nbsp;&nbsp;&nbsp;
    </p>
    <p class="auto-style7" align="center">
        <sup>1</sup> The Chinese Univeristy of Hong Kong&nbsp;&nbsp;&nbsp;&nbsp;
        <sup>2</sup> CUHK-Sensetime Joint Lab, The Chinese Univeristy of Hong Kong&nbsp;&nbsp;&nbsp;&nbsp;
        <br>
        <sup>3</sup> SenseTime Research&nbsp;&nbsp;&nbsp;&nbsp;
        <sup>4</sup> Nanyang Technological University&nbsp;&nbsp;&nbsp;&nbsp;
        <sup>5</sup> Tencent Youtu Lab&nbsp;&nbsp;&nbsp;&nbsp; [* indicates equal contribution]
    </p>
    <p align="center">
        <table style="width:960px" align="center">
            <tr>
                <td><img width=960px alt="" src="figures/psanet.png"></td>
            </tr>
            <tr>
                <td>
                    <p class="auto-style5-c" align="justify">Architecture of the proposed PSA module.</p>
                </td>
            </tr>
            <!--<tr>
                <td><img width=960px alt="" src="figures/architecture.png"></td>
            </tr>
            <tr>
            	<td>
                    <p class="auto-style5">Network structure of ResNet-FCN-backbone with PSA module incorporated. Deep supervision is also adopted for better performance.</p>
                </td>
            </tr>-->
        </table>
        <p class="auto-style4"><strong>Abstract</span></strong></p>
        <p class="auto-style5">We notice information flow in convolutional neural networks is restricted inside local neighborhood regions due to the physical design of convolutional filters, which limits the overall understanding of complex scenes. In this paper, we propose the point-wise spatial attention network (PSANet) to relax the local neighborhood constraint. Each position on the feature map is connected to all the other ones through a self-adaptively learned attention mask. Moreover, information propagation in bi-direction for scene parsing is enabled. Information at other positions can be collected to help the prediction of the current position and vice versa, information at the current position can be distributed to assist the prediction of other ones. Our proposed approach achieves top performance on various competitive scene parsing datasets, including ADE20K, PASCAL VOC 2012 and Cityscapes, demonstrating its effectiveness and generality.</p>
        <p class="auto-style4"><strong>Download</span></strong></p>
        <table cellSpacing=4 cellPadding=2 border=0 style="width: 90%">
            <tr COLSPAN="2">
                <td align="center" valign="center">
                    <img style="padding:0; clear:both; " src="figures/paper.png" align="middle" alt="Snapshot for paper" class="pdf" width="200" />
                </td>
                <td align="left" class="auto-style5">"PSANet: Point-wise Spatial Attention Network for Scene Parsing&quot;
                    <br> Hengshuang Zhao*,Yi Zhang*,Shu Liu,Jianping Shi,Chen Change Loy,Dahua Lin,Jiaya Jia.
                    <br>
                    <em>European Conference on Computer Vision</em> (<b>ECCV</b>), 2018.</br>
                    Ranked <text style="color:red">1st place</text> in <a href="http://bdd-data.berkeley.edu/wad-2018.html">WAD Drivable Area Segmentation Challenge 2018</a>.
                    <br>
                    <img alt="" height="32" src="figures/pdf.png">&nbsp;&nbsp;[<a href="../../paper/eccv18_psanet.pdf">Paper</a>]&nbsp;&nbsp;[<a href="../../paper/eccv18_psanet_supp.pdf">Supp</a>]&nbsp;&nbsp;[<a href="../../paper/eccv18_psanet_bib.txt">Bib</a>]<br><br>
                    <img alt="" height="32" src="figures/github.png">&nbsp;&nbsp;[<a href="https://github.com/hszhao/PSANet">Caffe</a>][<a href="https://github.com/hszhao/semseg">PyTorch</a>]<br><br>
					<img alt="" height="32" src="figures/ppt.png">&nbsp;&nbsp;[<a href="https://docs.google.com/presentation/d/1_brKNBtv8nVu_jOwFRGwVkEPAq8B8hEngBSQuZCWaZA/edit?usp=sharing">Slides in WAD2018@CVPR2018</a>]<br><br>
                </td>
            </tr>
        </table>

        <p class="auto-style4"><strong>Architcture</span></strong></p>
        <table style="width:960px" align="center">
            <tr>
                <td><img width=450px alt="" src="figures/attention.png"></td>
                <td><img width=450px alt="" src="figures/architecture.png"></td>
            </tr>
            <tr>
            	<td>
                    <p class="auto-style5-c">Illustration of Point-wise Spatial Attention.</p>
                </td>
                <td>
                    <p class="auto-style5">Network structure of ResNet-FCN-backbone with PSA module incorporated. Deep supervision is also adopted for better performance.</p>
                </td>
            </tr>
        </table>

        <p class="auto-style4"><strong>Performance</span></strong></p>
        <table style="width:960px" align="center">
            <tr>
                <td><img width=450px alt="" src="figures/result1.png"></td>
                <td><img width=450px alt="" src="figures/result2.png"></td>
            </tr>
            <tr>
                <td><img width=450px alt="" src="figures/result3.png"></td>
                <td><img width=450px alt="" src="figures/result4.png"></td>
            </tr>
        </table>

        <p class="auto-style4"><strong>Visualization</span></strong></p>
        <table style="width:960px" align="center">
            <tr>
                <td><img width=960px alt="" src="figures/ade20k.png"></td>
            </tr>
            <tr>
            	<td>
                    <p class="auto-style5">Visual improvement on validation set of ADE20K. The proposed PSANet gets more accurate and detailed parsing results. 'PSA-COL' denotes PSANet with 'COLLECT' branch and 'PSA-COL-DIS' stands for bi-direction information flow mode, which further enhances the prediction.</p>
                </td>
            </tr>
            <tr>
                <td><img width=960px alt="" src="figures/visualization.png"></td>
            </tr>
            <tr>
            	<td>
                    <p class="auto-style5">Visualization of learned masks by PSANet. Masks are sensitive to location and category information that harvest different contextual information.</p>
                </td>
            </tr>
        </table>
        <p align="center">[<a href="../../paper/eccv18_psanet_supp.pdf">More Visualization</a>]</p>
        <p id="video" , class="auto-style4"><strong>Video</strong></p>
        <p class="auto-style5">Demo video processed by PSANet(with IBNNet and PSPNet) on <a href="http://bdd-data.berkeley.edu/">BDD</a> dataset:</p>
        <iframe width="960" height="540" src="https://www.youtube.com/embed/l5xu1DI6pDk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        <p class="auto-style1">
            <font color="#999999">Last update: Aug. 18, 2018</font>
        </p>
</body>

</html>
