<!DOCTYPE html>
<html lang="en">

<head>
	<!-- Required meta tags -->
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
	<!-- Bootstrap core CSS -->
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/css/bootstrap.min.css" integrity="sha384-/Y6pD6FV/Vv2HJnA6t+vslU6fwYXjCFtcEpHbNJ0lyAFsXTsjBbfaDjzALeQsN6M" crossorigin="anonymous">
	<!-- https://fontawesome.com/cheatsheet -->
	<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons/css/academicons.min.css">
	<!-- Custom styles for this template -->
	<link rel="stylesheet" href="css/style.css">
	<link rel="icon" href="picture/hku.png">
    <script src="js/main.js"></script>
    <script src="js/scroll.js"></script>
    <meta name="keywords" content="Hengshuang Zhao, HKU, MIT, Oxford, CUHK, HUST, Computer Vision, Machine, Learning, Artificial Intelligence"> 
	<meta name="description" content="Personal page of Hengshuang Zhao">
</head>

<title>Hengshuang Zhao</title>

<body>
	<nav class="navbar navbar-expand-md navbar-dark fixed-top bg-dark" id="Home">
		<div class="container">
		<a class="navbar-brand" href="index.html">Hengshuang Zhao</a>
		<div class="collapse navbar-collapse" id="navbarToggle">
			<ul class="navbar-nav ml-auto">
				<li class="nav-item">
					<a class="nav-link" href="index.html">Home</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="publication.html">Publication</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="lab.html">Lab</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="award.html">Award</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="service.html">Service</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="talk.html">Talk</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="more.html">More</a>
				</li>
			</ul>
		</div>
		</div>
	</nav>

	<div class="container" style="padding-top: 20px; font-size: 17px">
		<div class="row">
			<div class="col-md-3", style="padding-right: 40px; padding-top: 0px">
				<img class="img-responsive img-rounded" src="picture/hengshuangzhao.png" alt="" style="max-width: 220px; solid black"><br>
			</div>
			<div class="col-md-5", style="padding-right: 40px; padding-top: 0px">
				<h2>Hengshuang Zhao</h2>
				<p>
					<br>Assistant Professor<br>
					Rm424, Chow Yei Ching Building<br>
					School of Computing and Data Science<br>
					The University of Hong Kong<br>
					Email: hszhao[at]cs.hku.hk<br>
					<a href="http://scholar.google.com/citations?user=4uE10I0AAAAJ&hl" target="_blank"><font color="black"><i class="ai ai-google-scholar ai-lg"></i></font></a>&nbsp;&nbsp;&nbsp;/&nbsp;&nbsp;
					<a href="http://github.com/hszhao" target="_blank"><font color="black"><i class="fab fa-github fa-lg"></i></font></a>&nbsp;&nbsp;&nbsp;/&nbsp;&nbsp;
					<a href="https://x.com/HengshuangZhao" target="_blank"><font color="black"><i class="fab fa-twitter fa-lg"></i></font></a>&nbsp;&nbsp;&nbsp;/&nbsp;&nbsp;
					<a href="https://linkedin.com/in/hengshuang-zhao-347b8391" target="_blank"><font color="black"><i class="fab fa-linkedin fa-lg"></i></font></a>&nbsp;&nbsp;&nbsp;/&nbsp;&nbsp;
					<a href="http://facebook.com/hengshuang.zhao" target="_blank"><font color="black"><i class="fab fa-facebook fa-lg"></i></font></a>
				</p>
			</div>
			<div class="col-md-4", style="padding-right: 40px; padding-top: 15px">
                <font color="black"><h5>Research Interests:</h5><br>
			  		<!--<a>Computer Vision</a><br>
			  		<a>Scene Understanding</a><br>
			  		<a>Multimodal Learning</a><br>
					<a>Generative Modeling</a><br>
					<a>Autonomous Driving</a><br>
					<a>Embodied AI</a><br>-->
					<a>Computer Vision</a><br>
			  		<a>Multimodal AI</a><br>
			  		<a>Spatial AI</a><br>
					<a>Generative AI</a><br>
					<a>Embodied AI</a><br>
					<a>Physical AI</a><br>
				</font>
			</div>
		</div>
		<div class="row">
			<div class="col-md-12">
			<p>
				<div style="text-align:justify">I am an Assistant Professor at the <a href="https://www.cds.hku.hk">School of Computing and Data Science</a> of <a href="https://www.hku.hk">The University of Hong Kong</a>. Previously, I have spent wonderful times as a Postdoctoral Researcher at <a href="https://www.csail.mit.edu">Computer Science and Artificial Intelligence Laboratory (CSAIL)</a> of <a href="https://www.mit.edu">MIT</a>, supervised by <a href="http://mit.edu/torralba">Prof. Antonio Torralba</a>, at <a href="https://torrvision.com">Torr Vision Group</a> of <a href="http://www.ox.ac.uk">University of Oxford</a> (beautiful Oxford), supervised by <a href="http://www.robots.ox.ac.uk/~phst">Prof. Philip Torr</a>. I obtained my Ph.D. degree from <a href="http://www.cuhk.edu.hk">The Chinese University of Hong Kong</a>, supervised by <a href="http://www.cse.cuhk.edu.hk/~leojia">Prof. Jiaya Jia</a>, and my bachelor's degree from <a href="https://www.hust.edu.cn">Huazhong University of Science and Technology</a>. During Ph.D., I have spent wonderful times as a Research Intern, working with <a href="http://xiaohuishen.github.io">Dr. Xiaohui Shen</a>, <a href="http://sites.google.com/site/zhelin625">Dr. Zhe Lin</a>, <a href="http://www.kalyans.org">Dr. Kalyan Sunkavalli</a>, <a href="http://www.brianpricephd.com">Dr. Brian Price</a> at Adobe (San Jose), <a href="http://www.cs.toronto.edu/~urtasun">Prof. Raquel Urtasun</a> at Uber (Toronto), and <a href="http://vladlen.info">Dr. Vladlen Koltun</a> at Intel (Santa Clara).</div>
			</p>
			<p>
				<div style="text-align:justify">My general research interests cover the broad area of computer vision, machine learning, and artificial intelligence, with special emphasis on building intelligent visual systems. My research goal is to utilize artificial intelligence techniques to make machines perceive, understand, imagine, and interact with the surrounding environment, and ultimately make high positive impacts on various fields. Our current research interests and focus include: 1. visual scene understanding, perception, reconstruction, representation learning, multimodal learning; 2. generative modeling, visual content creation, generation, and manipulation (image/video/3d); 3. autonomous driving, embodied ai, robot learning, LLM applications etc.
			</p>
			<p>
				<div style="text-align:justify"><font color="red">Prospective students:</font> Our <a href="lab.html">lab</a> is actively looking for self-motivated Ph.D. students, postdoctoral researchers, research assistants, and visiting scholars, working together on exciting and cutting-edge computer vision, machine learning, and artificial intelligence projects. If you are interested in working with me, please drop me an email with your resume. Available Ph.D. scholarships and opportunities include <a href="https://cerg1.ugc.edu.hk/hkpfs/index.html">Hong Kong PhD Fellowship Scheme (HKPFS)</a>, <a href="https://gradsch.hku.hk/prospective_students/fees_scholarships_and_financial_support/hku_presidential_phd_scholar_programme">HKU Presidential PhD Scholar Programme (HKUPS)</a>, <a href="https://gradsch.hku.hk/prospective_students/fees_scholarships_and_financial_support/postgraduate_scholarships">Postgraduate Scholarships (PGS)</a>, <a href="https://gradsch.hku.hk/prospective_students/programmes/joint_educational_placement_with_bici">HKU-BICI</a>, and <a href="https://gradsch.hku.hk/prospective_students/programmes/HKU_ASTRI">HKU-ASTRI</a>.</div>
			</p>
			</div>
		</div>
	</div>

  <!-- News -->
	<div class="container" style="padding-top: 20px; font-size: 17px">
		<h3 id="News" style="padding-top: 80px; margin-top: -80px; margin-left: -18px;">News</h3>
		<ul>
			<li>Awarded the prestigious Excellent Young Scientists Fund by the National Natural Science Foundation of China.</li>
			<li>Awarded the World Artificial Intelligence Conference (WAIC) Bright Star Award 2025, and Youth Outstanding Paper Award 2024&2025.</li>
			<li>Members awarded the prestigious NSFC Young Student Basic Research Program (PhD candidate) (Xi), CVPR Doctoral Consortium Award (Xiaoyang), ByteDance Scholarship (Lihe), Ant InTech Scholarship - Future (Lihe), Ant InTech Award (Hengshuang).</li>
			<li>8 papers in CVPR 2025, 4 papers in SIGGRAPH 2025, 6 papers in ICML 2025, 5 papers in ICCV 2025, 11 papers in NeurIPS 2025.</li>
			<li>I am recognized as one of the most influential scholars in computer vision by AI 2000 in <a href="https://www.aminer.org/profile/hengshuang-zhao/542c02afdabfae216e61f9c4">2022, 2023, 2024, and 2025</a>.</li>
			<!--<li>We have 8 papers in CVPR'25, 4 papers in SIGGRAPH'25, 6 papers in ICML'25, 5 papers in ICCV'25, and 11 papers in NeurIPS'25.</li>-->
			<li>I am selected as an Oral Session Chair for the ICCV 2025 on <a href="https://media.eventhosts.cc/Conferences/ICCV2025/ProgramOverview.pdf">Multi-modal Learning</a>.</li>
			<li>We are organizing the ICCV 2025 Workshop on <a href="https://higen-2025.github.io">Human-Interactive Generation and Editing</a>.</li>
			<li>We are organizing the CVPR 2025 Tutorial on <a href="https://sites.google.com/view/cvpr25-point-cloud-tutorial">The 2nd Point Cloud Tutorial: All You Need To Know About 3D Point Cloud</a>.</li>
			<li>Invited talk at the CVPR 2025 Workshop on <a href="https://sites.google.com/view/pixfoundation">PixFoundation: Workshop on Pixel-level Vision Foundation Models</a>.</li>
			<li>Invited talk at the CVPR 2025 Workshop on <a href="https://pvuw.github.io">Pixel-level Video Understanding in the Wild Challenge</a>.</li>
			<li>Invited talk at the ICLR 2025 Workshop on <a href="https://embodiedcity.github.io/iclr25-workshop">Embodied Intelligence with Large Language Models In Open City Environment</a>.</li>
			<li>Our <a href="https://depth-anything-v2.github.io">Depth Anything</a> is integrated into <a href="https://developer.apple.com/machine-learning/models">Apple Core ML Models</a> for fantastic applications.</li>
			<li>Our <a href="https://depth-anything.github.io">Depth Anything</a> won the CVPR 2024 <a href="https://cvpr.thecvf.com/Conferences/2024/News/Wrap_Release">Best Demo Honorable Mention</a>.</li>
			<li>Our <a href="https://arxiv.org/abs/2312.10035">Point Transformer V3</a> won the CVPR 2024 <a href="https://waymo.com/open/challenges">Waymo 3D Semantic Segmentation Challenge Champion</a>.</li>
			<li>Invited talk at the ECCV 2024 Workshop on <a href="https://syntheticdata4cv.wordpress.com">Synthetic Data for Computer Vision</a>.</li>
			<li>Invited talk at the ECCV 2024 Workshop on <a href="https://lsvos.github.io">Large-scale Video Object Segmentation</a>.</li>
			<li>We are organizing the ICML 2024 Workshop on <a href="icml-mfm-eai.github.io">Multimodal Foundation Models for Embodied Agents</a>.</li>
			<li>Invited talk at the CVPR 2024 <a href="https://sites.google.com/view/cvpr24-ac-workshop">Area Chair Workshop</a>.</li>
			<li>We are organizing the CVPR 2024 Tutorial on <a href="https://sites.google.com/view/pcn-cvpr24tutorial/homepage">All You Need To Know About Point Cloud Understanding</a>.</li>
			<!--<li>I serve as an Area Chair for CVPR 2023, NeurIPS 2023, WACV 2023, CVPR 2024, ECCV 2024, NeurIPS 2024, ACMMM 2024.</li>-->
			<!--<li>I serve as an Area Chair for CVPR 2023, NeurIPS 2023, CVPR 2024, ECCV 2024, NeurIPS 2024, ICLR 2025, CVPR 2025, ICCV 2025.</li>-->
			<li>I serve as an Area Chair for CVPR 2023~26, NeurIPS 2023~25, AAAI 2023~26, ECCV 2024, ICLR 2025~26, ICCV 2025.</li>
			<!--<li>I serve as a Senior Program Committee for AAAI 2023, AAAI 2024, and AAAI 2025.</li>-->
			<li>I serve as an Associate Editor for Pattern Recognition, Science China Information Sciences, and a Guest Editor for IEEE TCSVT.</li>
			<li><font color="red">Pinned projects:</font> 1. New innovations: <a href="https://depth-anything.github.io">Depth Anything V1</a>&<a href="https://depth-anything-v2.github.io">V2</a>, <a href="https://arxiv.org/abs/2312.10035">Point Transformer V3</a>, <a href="https://tonyxuqaq.github.io/projects/DriveGPT4">DriveGPT4 V1&V2</a>, <a href="https://gpt4point.github.io">GPT4Point</a>, <a href="https://gpt4scene.github.io">GPT4Scene</a>, <a href="https://arxiv.org/abs/2402.18573">UniMODE</a>, <a href="https://ali-vilab.github.io/AnyDoor-Page">AnyDoor</a>, <a href="https://videoanydoor.github.io">VideoAnydoor</a>, <a href="https://xavierchen34.github.io/UniReal-Page">UniReal</a>; 2. Highly optimized codebase available for 3D scene understanding <a href="https://github.com/Pointcept/Pointcept">Pointcept (PTv1&PTv2&PTv3&MSC&PPT)</a>; 3. Highly optimized codebase available for semantic segmentation <a href="http://github.com/hszhao/semseg">semseg (PSPNet&PSANet)</a>.</li>
		</ul>
	</div>

	<!-- Publication -->
	<div class="container" style="padding-top: 20px; font-size: 17px;">
		<h3 id="Publications" style="padding-top: 80px; margin-top: -80px;">Publications</h3>
		<p><a href="http://scholar.google.com/citations?user=4uE10I0AAAAJ&hl">Google Scholar</a> and <a href="paper/bib.txt">Bib</a>.</p>

		<div class="row">
			<div class="col-md-3"><div class="badge">NeurIPS Oral</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""><source src="teaser/2025_neurips_playerone.mp4" type="video/mp4"></video></div>
			<div class="col-md-9">
			<b><font color="black">PlayerOne: Egocentric World Simulator</font></b><br>
			Yuanpeng Tu, Hao Luo, Xi Chen, Xiang Bai, Fan Wang, <b>Hengshuang Zhao</b>.<br>
			Neural Information Processing Systems (<b>NeurIPS</b>), 2025. <b><font color="firebrick">Oral</font></b></br>
			[<a href="https://playerone-hku.github.io">Project</a>]
			[<a href="https://arxiv.org/abs/2506.09995">Paper</a>]
			[<a href="https://github.com/yuanpengtu/PlayerOne">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<!--<div class="col-md-3"><div class="badge">NeurIPS</div><img class="img-fluid img-rounded" src="teaser/2025_neurips_concerto.png"></div>-->
			<div class="col-md-3"><div class="badge">NeurIPS</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""><source src="teaser/2025_neurips_concerto.mov" type="video/mp4"></video></div>
			<div class="col-md-9">
			<b><font color="black">Concerto: Joint 2D-3D Self-Supervised Learning Emerges Spatial Representations</font></b><br>
			Yujia Zhang, Xiaoyang Wu, Yixing Lao, Chengyao Wang, Zhuotao Tian, Naiyan Wang, <b>Hengshuang Zhao</b>.<br>
			Neural Information Processing Systems (<b>NeurIPS</b>), 2025.</br>
			[Project]
			[Paper]
			[Code]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">NeurIPS</div><img class="img-fluid img-rounded" src="teaser/2025_neurips_mico.png"></div>
			<div class="col-md-9">
			<b><font color="black">MiCo: Multi-image Contrast for Reinforcement Visual Reasoning</font></b><br>
			Xi Chen, Mingkang Zhu, Shaoteng Liu, Xiaoyang Wu, Xiaogang Xu, Yu Liu, Xiang Bai, <b>Hengshuang Zhao</b>.<br>
			Neural Information Processing Systems (<b>NeurIPS</b>), 2025.</br>
			[Project]
			[<a href="https://arxiv.org/abs/2506.22434">Paper</a>]
			[Code]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">NeurIPS</div><img class="img-fluid img-rounded" src="teaser/2025_neurips_starr1.png"></div>
			<div class="col-md-9">
			<b><font color="black">STAR-R1: Improving Video Perception via Spatio-Temporal Aggregated Reinforcement</font></b><br>
			Rongkun Zheng, Lu Qi, Xi Chen, Yi Wang, Kun Wang, <b>Hengshuang Zhao</b>.<br>
			Neural Information Processing Systems (<b>NeurIPS</b>), 2025.</br>
			[Project]
			[Paper]
			[Code]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">NeurIPS</div><img class="img-fluid img-rounded" src="teaser/2025_neurips_segvar.png"></div>
			<div class="col-md-9">
			<b><font color="black">Seg-VAR: Image Segmentation with Visual Autoregressive Modeling</font></b><br>
			Rongkun Zheng, Lu Qi, Xi Chen, Yi Wang, Kun Wang, <b>Hengshuang Zhao</b>.<br>
			Neural Information Processing Systems (<b>NeurIPS</b>), 2025.</br>
			[Project]
			[Paper]
			[Code]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">NeurIPS</div><img class="img-fluid img-rounded" src="teaser/2025_neurips_rose.png"></div>
			<div class="col-md-9">
			<b><font color="black">ROSE: Remove Objects with Side Effects in Videos</font></b><br>
			Chenxuan Miao, Yutong Feng, Jianshu Zeng, Zixiang Gao, Hantang Liu, Yunfeng Yan, Donglian Qi, Xi Chen, Bin Wang, <b>Hengshuang Zhao</b>.<br>
			Neural Information Processing Systems (<b>NeurIPS</b>), 2025.</br>
			[<a href="https://rose2025-inpaint.github.io">Project</a>]
			[<a href="https://arxiv.org/abs/2508.18633">Paper</a>]
			[<a href="https://github.com/Kunbyte-AI/ROSE">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">NeurIPS</div><img class="img-fluid img-rounded" src="teaser/2025_neurips_visionthink.jpg"></div>
			<div class="col-md-9">
			<b><font color="black">VisionThink: Smart and Efficient Vision Language Model via Reinforcement Learning</font></b><br>
			Senqiao Yang, Junyi Li, Xin Lai, Jinming Wu, Wei Li, Zejun Ma, Bei Yu, <b>Hengshuang Zhao</b>, Jiaya Jia.<br>
			Neural Information Processing Systems (<b>NeurIPS</b>), 2025.</br>
			[<a href="https://github.com/dvlab-research/VisionThink">Project</a>]
			[<a href="https://arxiv.org/abs/2507.13348">Paper</a>]
			[<a href="https://github.com/dvlab-research/VisionThink">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">NeurIPS Spotlight</div><img class="img-fluid img-rounded" src="teaser/2025_neurips_orientanythingv2.png"></div>
			<div class="col-md-9">
			<b><font color="black">Orient Anything V2: Unifying Orientation and Rotation Understanding</font></b><br>
			Zehan Wang, Ziang Zhang, Jiayang Xu, Jialei Wang, Tianyu Pang, Chao Du, <b>Hengshuang Zhao</b>, Zhou Zhao.<br>
			Neural Information Processing Systems (<b>NeurIPS</b>), 2025. <b><font color="firebrick">Spotlight</font></b></br>
			[Project]
			[Paper]
			[Code]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">NeurIPS</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""><source src="teaser/2025_neurips_litereality.mp4" type="video/mp4"></video></div>
			<div class="col-md-9">
			<b><font color="black">LiteReality: Graphic-Ready 3D Scene Reconstruction from RGB-D Scans</font></b><br>
			Zhening Huang, Xiaoyang Wu, Fangcheng Zhong, <b>Hengshuang Zhao</b>, Matthias Nießner, Joan Lasenby.<br>
			Neural Information Processing Systems (<b>NeurIPS</b>), 2025.</br>
			[<a href="https://litereality.github.io">Project</a>]
			[<a href="https://arxiv.org/abs/2507.02861">Paper</a>]
			[<a href="https://github.com/LiteReality/LiteReality">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<!--<div class="col-md-3"><div class="badge">NeurIPS</div><img class="img-fluid img-rounded" src="teaser/2025_neurips_mover.png"></div>-->
			<div class="col-md-3"><div class="badge">NeurIPS</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""><source src="teaser/2025_neurips_mover.mp4" type="video/mp4"></video></div>
			<div class="col-md-9">
			<b><font color="black">Mover: Motion-controllable Video Generation via Latent Trajectory Guidance</font></b><br>
			Ruihang Chu, Yefei He, Zhekai Chen, Shiwei Zhang, Xiaogang Xu, Bin Xia, Dingdong Wang, Hongwei Yi, Xihui Liu, <b>Hengshuang Zhao</b>, Yu Liu, Yingya Zhang, Yujiu Yang.<br>
			Neural Information Processing Systems (<b>NeurIPS</b>), 2025.</br>
			[Project]
			[Paper]
			[Code]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">NeurIPSDB</div><img class="img-fluid img-rounded" src="teaser/2025_neurips_genspace.png"></div>
			<div class="col-md-9">
			<b><font color="black">GenSpace: Benchmarking Spatially-Aware Image Generation</font></b><br>
			Zehan Wang, Jiayang Xu, Ziang Zhang, Tianyu Pang, Chao Du, <b>Hengshuang Zhao</b>, Zhou Zhao.<br>
			Neural Information Processing Systems (<b>NeurIPS</b>) Datasets and Benchmarks Track, 2025.</br>
			[Project]
			[<a href="https://arxiv.org/abs/2505.24870">Paper</a>]
			[Code]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">SIGGRAPH Asia</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""><source src="teaser/2025_siggraphasia_diffcamera.mp4" type="video/mp4"></video></div>
			<div class="col-md-9">
			<b><font color="black">DiffCamera: Arbitrary Refocusing on Images</font></b><br>
			Yiyang Wang, Xi Chen, Xiaogang Xu, Yu Liu, <b>Hengshuang Zhao</b>.<br>
			<b>SIGGRAPH Asia</b>, 2025.</br>
			[Project]
			[Paper]
			[Code]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">EMNLP</div><img class="img-fluid img-rounded" src="teaser/2025_emnlp_enhancing.png"></div>
			<div class="col-md-9">
			<b><font color="black">Enhancing LLM Knowledge Learning through Generalization</font></b><br>
			Mingkang Zhu, Xi Chen, Zhongdao Wang, Bei Yu, <b>Hengshuang Zhao</b>, Jiaya Jia.<br>
			Empirical Methods in Natural Language Processing (<b>EMNLP</b>), 2025.</br>
			[Project]
			[<a href="https://arxiv.org/abs/2503.03705v2">Paper</a>]
			[Code]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ICCV Highlight</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""><source src="teaser/2025_iccv_stabledepth.mp4" type="video/mp4"></video></div>
			<div class="col-md-9">
			<b><font color="black">StableDepth: Scene-Consistent and Scale-Invariant Monocular Depth</font></b><br>
			Zheng Zhang, Lihe Yang, Tianyu Yang, Chaohui Yu, Xiaoyang Guo, Yixing Lao, <b>Hengshuang Zhao</b>.<br>
			International Conference on Computer Vision (<b>ICCV</b>), 2025. <b><font color="firebrick">Highlight</font></b></br>
			[<a href="https://stabledepth.github.io">Project</a>]
			[Paper]
			[Code]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ICCV</div><img class="img-fluid img-rounded" src="teaser/2025_iccv_diffdoctor.png"></div>
			<div class="col-md-9">
			<b><font color="black">DiffDoctor: Diagnosing Image Diffusion Models Before Treating</font></b><br>
			Yiyang Wang, Xi Chen, Xiaogang Xu, Sihui Ji, Yu Liu, Yujun Shen, <b>Hengshuang Zhao</b>.<br>
			International Conference on Computer Vision (<b>ICCV</b>), 2025.</br>
			[<a href="https://chandlerwang14.github.io/DiffDoctor-Page">Project</a>]
			[<a href="https://arxiv.org/abs/2501.12382">Paper</a>]
			[<a href="https://github.com/ali-vilab/DiffDoctor">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ICCV</div><img class="img-fluid img-rounded" src="teaser/2025_iccv_villa.png"></div>
			<div class="col-md-9">
			<b><font color="black">ViLLa: Video Reasoning Segmentation with Large Language Model</font></b><br>
			Rongkun Zheng, Lu Qi, Xi Chen, Yi Wang, Kun Wang, Yu Qiao, <b>Hengshuang Zhao</b>.<br>
			International Conference on Computer Vision (<b>ICCV</b>), 2025.</br>
			[<a href="https://arxiv.org/abs/2407.14500">Paper</a>]
			[<a href="https://github.com/rkzheng99/ViLLa">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ICCV</div><img class="img-fluid img-rounded" src="teaser/2025_iccv_disco.png"></div>
			<div class="col-md-9">
			<b><font color="black">DisCo: Towards Distinct and Coherent Visual Encapsulation in Video MLLMs</font></b><br>
			Jiahe Zhao, Rongkun Zheng, Yi Wang, Helin Wang, <b>Hengshuang Zhao</b>.<br>
			International Conference on Computer Vision (<b>ICCV</b>), 2025.</br>
			[<a href="https://arxiv.org/abs/2507.10302">Paper</a>]
			[<a href="https://github.com/ZJHTerry18/DisCo">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ICCV</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""><source src="teaser/2025_iccv_hermes.mp4" type="video/mp4"></video></div>
			<div class="col-md-9">
			<b><font color="black">HERMES: A Unified Self-Driving World Model for Simultaneous 3D Scene Understanding and Generation</font></b><br>
			Xin Zhou, Dingkang Liang, Sifan Tu, Xiwu Chen, Yikang Ding, Dingyuan Zhang, Feiyang Tan, <b>Hengshuang Zhao</b>, Xiang Bai.<br>
			International Conference on Computer Vision (<b>ICCV</b>), 2025.</br>
			[<a href="https://lmd0311.github.io/HERMES">Project</a>]
			[<a href="https://arxiv.org/abs/2501.14729">Paper</a>]
			[<a href="https://github.com/LMD0311/HERMES">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">SIGGRAPH</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""><source src="teaser/2025_siggraph_videoanydoor.mp4" type="video/mp4"></video></div>
			<div class="col-md-9">
			<b><font color="black">VideoAnydoor: High-fidelity Video Object Insertion with Precise Motion Control</font></b><br>
			Yuanpeng Tu, Hao Luo, Xi Chen, Sihui Ji, Xiang Bai, <b>Hengshuang Zhao</b>.<br>
			<b>SIGGRAPH</b>, 2025.</br>
			[<a href="https://videoanydoor.github.io">Project</a>]
			[<a href="https://arxiv.org/abs/2501.01427">Paper</a>]
			[<a href="https://github.com/yuanpengtu/VideoAnydoor">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">SIGGRAPH</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""><source src="teaser/2025_siggraph_layerflow.mp4" type="video/mp4"></video></div>
			<div class="col-md-9">
			<b><font color="black">LayerFlow: A Unified Model for Layer-aware Video Generation</font></b><br>
			Sihui Ji, Hao Luo, Xi Chen, Yuanpeng Tu, Yiyang Wang, <b>Hengshuang Zhao</b>.<br>
			<b>SIGGRAPH</b>, 2025.</br>
			[<a href="https://sihuiji.github.io/LayerFlow-Page">Project</a>]
			[<a href="https://arxiv.org/abs/2506.04228">Paper</a>]
			[<a href="https://github.com/SihuiJi/LayerFlow">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">SIGGRAPH</div><img class="img-fluid img-rounded" src="teaser/2025_siggraph_fashioncomposer.gif"></div>
			<div class="col-md-9">
			<b><font color="black">FashionComposer: Compositional Fashion Image Generation</font></b><br>
			Sihui Ji, Yiyang Wang, Xi Chen, Xiaogang Xu, Hao Luo, <b>Hengshuang Zhao</b>.<br>
			<b>SIGGRAPH</b>, 2025.</br>
			[<a href="https://sihuiji.github.io/FashionComposer-Page">Project</a>]
			[<a href="https://arxiv.org/abs/2412.14168">Paper</a>]
			[<a href="https://github.com/SihuiJi/FashionComposer">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">SIGGRAPH</div><img class="img-fluid img-rounded" src="teaser/2025_siggraph_dreammask.png"></div>
			<div class="col-md-9">
			<b><font color="black">DreamMask: Boosting Open-vocabulary Panoptic Segmentation with Synthetic Data</font></b><br>
			Yuanpeng Tu, Xi Chen, Ser-Nam Lim, <b>Hengshuang Zhao</b>.<br>
			<b>SIGGRAPH</b>, 2025.</br>
			[<a href="https://yuanpengtu.github.io/Dreammask-Page">Project</a>]
			[<a href="https://arxiv.org/abs/2501.02048">Paper</a>]
			[Code]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ICML</div><img class="img-fluid img-rounded" src="teaser/2025_icml_vip.png"></div>
			<div class="col-md-9">
			<b><font color="black">VIP: Vision Instructed Pre-training for Robotic Manipulation</font></b><br>
			Zhuoling Li, Liangliang Ren, Jinrong Yang, Yong Zhao, Xiaoyang Wu, Zhenhua Xu, Xiang Bai, <b>Hengshuang Zhao</b>.<br>
			International Conference on Machine Learning (<b>ICML</b>), 2025.</br>
			[<a href="https://lizhuoling.github.io/VIRT_webpage">Project</a>]
			[<a href="https://arxiv.org/abs/2410.07169">Paper</a>]
			[<a href="https://github.com/Lizhuoling/VIRT">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ICML</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""><source src="teaser/2025_icml_larm.mp4" type="video/mp4"></video></div>
			<div class="col-md-9">
			<b><font color="black">LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence</font></b><br>
			Zhuoling Li, Xiaogang Xu, Zhenhua Xu, Ser-Nam Lim, <b>Hengshuang Zhao</b>.<br>
			International Conference on Machine Learning (<b>ICML</b>), 2025.</br>
			[<a href="https://lizhuoling.github.io/LARM_webpage">Project</a>]
			[<a href="https://arxiv.org/pdf/2405.17424">Paper</a>]
			[<a href="https://www.youtube.com/watch?v=7ajieVAuob8">Demo</a>]
			[<a href="https://www.youtube.com/watch?v=JxaqPpDPnec">Video</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ICML</div><img class="img-fluid img-rounded" src="teaser/2025_icml_haplovl.png"></div>
			<div class="col-md-9">
			<b><font color="black">HaploVL: A Single-Transformer Baseline for Multi-Modal Understanding</font></b><br>
			Rui Yang, Lin Song, Yicheng Xiao, Runhui Huang, Yixiao Ge, Ying Shan, <b>Hengshuang Zhao</b>.<br>
			International Conference on Machine Learning (<b>ICML</b>), 2025.</br>
			[<a href="https://arxiv.org/abs/2503.14694">Paper</a>]
			[<a href="https://github.com/Tencent/HaploVLM">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ICML</div><img class="img-fluid img-rounded" src="teaser/2025_icml_bood.png"></div>
			<div class="col-md-9">
			<b><font color="black">BOOD: Boundary-based Out-Of-Distribution Data Generation</font></b><br>
			Qilin Liao, Shuo Yang, Bo Zhao, Ping Luo, <b>Hengshuang Zhao</b>.<br>
			International Conference on Machine Learning (<b>ICML</b>), 2025.</br>
			[Paper]
			[Code]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ICML</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""><source src="teaser/2025_icml_orientanything.mp4" type="video/mp4"></video></div>
			<div class="col-md-9">
			<b><font color="black">Orient Anything: Learning Robust Object Orientation Estimation from Rendering 3D Models</font></b><br>
			Zehan Wang, Ziang Zhang, Tianyu Pang, Chao Du, <b>Hengshuang Zhao</b>, Zhou Zhao.<br>
			International Conference on Machine Learning (<b>ICML</b>), 2025.</br>
			[<a href="https://orient-anything.github.io">Project</a>]
			[<a href="https://arxiv.org/abs/2412.18605">Paper</a>]
			[<a href="https://github.com/SpatialVision/Orient-Anything">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ICML</div><img class="img-fluid img-rounded" src="teaser/2025_icml_tgdpo.png"></div>
			<div class="col-md-9">
			<b><font color="black">TGDPO: Harnessing Token-Level Reward Guidance for Enhancing Direct Preference Optimization</font></b><br>
			Mingkang Zhu, Xi Chen, Zhongdao Wang, Bei Yu, <b>Hengshuang Zhao</b>, Jiaya Jia.<br>
			International Conference on Machine Learning (<b>ICML</b>), 2025.</br>
			[Paper]
			[Code]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR Highlight</div><img class="img-fluid img-rounded" src="teaser/2025_cvpr_unireal.png"></div>
			<div class="col-md-9">
			<b><font color="black">UniReal: Universal Image Generation and Editing via Learning Real-world Dynamics</font></b><br>
			Xi Chen, Zhifei Zhang, He Zhang, Yuqian Zhou, Soo Ye Kim, Qing Liu, Yijun Li, Jianming Zhang, Nanxuan Zhao, Yilin Wang, Hui Ding, Zhe Lin, <b>Hengshuang Zhao</b>.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2025. <b><font color="firebrick">Highlight</font></b></br>
			[<a href="https://xavierchen34.github.io/UniReal-Page">Project</a>]
			[<a href="https://arxiv.org/abs/2412.07774">Paper</a>]
			[Code]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR Highlight</div><img class="img-fluid img-rounded" src="teaser/2025_cvpr_drivegpt4v2.png"></div>
			<div class="col-md-9">
			<b><font color="black">DriveGPT4-V2: Harnessing Large Language Model Capabilities for Enhanced Closed-Loop Autonomous Driving</font></b><br>
			Zhenhua Xu, Yan Bai, Yujia Zhang, Zhuoling Li, Fei Xia, Kwan-Yee K. Wong, Jianqiang Wang, <b>Hengshuang Zhao</b>.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2025. <b><font color="firebrick">Highlight</font></b></br>
			[Project]
			[Paper]
			[Code]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR Highlight</div><img class="img-fluid img-rounded" src="teaser/2025_cvpr_sonata.png"></div>
			<div class="col-md-9">
			<b><font color="black">Sonata: Self-Supervised Learning of Reliable Point Representations</font></b><br>
			Xiaoyang Wu, Daniel DeTone, Duncan Frost, Tianwei Shen, Chris Xie, Nan Yang, Jakob Engel, Richard Newcombe, <b>Hengshuang Zhao</b>, Julian Straub.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2025. <b><font color="firebrick">Highlight</font></b></br>
			[<a href="https://xywu.me/sonata">Project</a>]
			[<a href="https://arxiv.org/abs/2503.16429">Paper</a>]
			[<a href="https://github.com/facebookresearch/sonata">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/2025_cvpr_spatialclip.png"></div>
			<div class="col-md-9">
			<b><font color="black">SpatialCLIP: Learning 3D-aware Image Representations from Spatially Discriminative Language</font></b><br>
			Zehan Wang, Sashuai zhou, Shaoxuan He, Haifeng Huang, Lihe Yang, Ziang Zhang, Xize Cheng, Shengpeng Ji, Tao Jin, <b>Hengshuang Zhao</b>, Zhou Zhao.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2025.</br>
			[Project]
			[Paper]
			[Code]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/2025_cvpr_panda.png"></div>
			<div class="col-md-9">
			<b><font color="black">PanDA: Towards Panoramic Depth Anything with Unlabeled Panoramas and Mobius Spatial Augmentation</font></b><br>
			Zidong Cao, Jinjing Zhu, Weiming Zhang, Hao Ai, Haotian Bai, <b>Hengshuang Zhao</b>, Lin Wang.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2025.</br>
			[<a href="https://caozidong.github.io/PanDA_Depth">Project</a>]
			[<a href="https://arxiv.org/abs/2406.13378">Paper</a>]
			[<a href="https://github.com/caozidong/PanDA">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/2025_cvpr_view2cap.png"></div>
			<div class="col-md-9">
			<b><font color="black">Empowering Large Language Models with 3D Situation Awareness</font></b><br>
			Zhihao Yuan, Yibo Peng, Jinke Ren, Yinghong Liao, Yatong Han, Chun-Mei Feng, <b>Hengshuang Zhao</b>, Guanbin Li, Shuguang Cui, Zhen Li.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2025.</br>
			[<a href="https://arxiv.org/abs/2503.23024">Paper</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/2025_cvpr_hiresllava.png"></div>
			<div class="col-md-9">
			<b><font color="black">HiRes-LLaVA: Restoring Fragmentation Input in High-Resolution Large Vision-Language Models</font></b><br>
			Runhui Huang, Xinpeng Ding, Chunwei Wang, Jianhua Han, Yulong Liu, <b>Hengshuang Zhao</b>, Hang Xu, Lu Hou, Wei Zhang, Xiaodan Liang.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2025.</br>
			[<a href="https://arxiv.org/abs/2407.08706">Paper</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/2025_cvpr_emova.png"></div>
			<div class="col-md-9">
			<b><font color="black">EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions</font></b><br>
			Kai Chen, Yunhao Gou, Runhui Huang, Zhili Liu, Daxin Tan, Jing Xu, Chunwei Wang, Yi Zhu, Yihan Zeng, Kuo Yang, Dingdong Wang, Kun Xiang, Haoyuan Li, Haoli Bai, Jianhua Han, Xiaohui Li, Weike Jin, Nian Xie, Yu Zhang, James T. Kwok, <b>Hengshuang Zhao</b>, Xiaodan Liang, Dit-Yan Yeung, Xiao Chen, Zhenguo Li, Wei Zhang, Qun Liu, Jun Yao, Lanqing Hong, Lu Hou, Hang Xu.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2025.</br>
			[<a href="https://emova-ollm.github.io">Project</a>]
			[<a href="https://arxiv.org/abs/2409.18042">Paper</a>]
			[<a href="https://github.com/emova-ollm/EMOVA">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ICLR</div><img class="img-fluid img-rounded" src="teaser/2025_iclr_omnibind.png"></div>
			<div class="col-md-9">
			<b><font color="black">OmniBind: Large-scale Omni Multimodal Representation via Binding Spaces</font></b><br>
			Zehan Wang, Ziang Zhang, Minjie Hong, Hang Zhang, Luping Liu, Rongjie Huang, Xize Cheng, Shengpeng Ji, Tao Jin, <b>Hengshuang Zhao</b>, Zhou Zhao.<br>
			International Conference on Machine Learning (<b>ICLR</b>), 2025.</br>
			[<a href="https://omnibind.github.io">Project</a>]
			[<a href="https://openreview.net/pdf?id=l2izo0z7gu">Paper</a>]
			[<a href="https://github.com/zehanwang01/OmniBind">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">TPAMI</div><img class="img-fluid img-rounded" src="teaser/2025_tpami_gpt4point2.png"></div>
			<div class="col-md-9">
			<b><font color="black">GPT4Point++: Advancing Unified Point-Language Understanding and Generation</font></b><br>
			Zhangyang Qi, Ye Fang, Zeyi Sun, Xiaoyang Wu, Tong Wu, Jiaqi Wang, Dahua Lin, <b>Hengshuang Zhao</b>.<br>
			IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>), 2025.</br>
			[<a href="https://gpt4point.github.io">Project</a>]
			[<a href="https://arxiv.org/abs/2312.02980">Paper</a>]
			[<a href="https://github.com/Pointcept/GPT4Point">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">TPAMI</div><img class="img-fluid img-rounded" src="teaser/2025_tpami_anydoor2.png"></div>
			<div class="col-md-9">
			<b><font color="black">AnyDoor: Zero-shot Image Customization with Region-to-region Reference</font></b><br>
			Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, <b>Hengshuang Zhao</b>.<br>
			IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>), 2025.</br>
			[<a href="https://ali-vilab.github.io/AnyDoor-Page">Project</a>]
			[<a href="https://ieeexplore.ieee.org/document/10976616">Paper</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">TPAMI</div><img class="img-fluid img-rounded" src="teaser/2025_tpami_unimode2.png"></div>
			<div class="col-md-9">
			<b><font color="black">Towards Unified 3D Object Detection via Algorithm and Data Unification</font></b><br>
			Zhuoling Li, Xiaogang Xu, Ser-Nam Lim, <b>Hengshuang Zhao</b>.<br>
			IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>), 2025.</br>
			[<a href="https://lizhuoling.github.io/UniMODE_webpage">Project</a>]
			[<a href="https://arxiv.org/abs/2402.18573">Paper</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">TPAMI</div><img class="img-fluid img-rounded" src="teaser/2025_tpami_unimatchv2.png"></div>
			<div class="col-md-9">
			<b><font color="black">UniMatch V2: Pushing the Limit of Semi-Supervised Semantic Segmentation</font></b><br>
			Lihe Yang, Zhen Zhao, <b>Hengshuang Zhao</b>.<br>
			IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>), 2025.</br>
			[<a href="https://arxiv.org/abs/2410.10777">Paper</a>]
			[<a href="https://github.com/LiheYoung/UniMatch-V2">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">TPAMI</div><img class="img-fluid img-rounded" src="teaser/2025_tpami_dreamcomposer2.png"></div>
			<div class="col-md-9">
			<b><font color="black">DreamComposer++: Empowering Diffusion Models with Multi-View Conditions for 3D Content Generation</font></b><br>
			Yunhan Yang, Shuo Chen, Yukun Huang, Xiaoyang Wu, Yuan-Chen Guo, Edmund Y. Lam, <b>Hengshuang Zhao</b>, Tong He, Xihui Liu.<br>
			IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>), 2025.</br>
			[<a href="https://ieeexplore.ieee.org/document/10993319">Paper</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">TPAMI</div><img class="img-fluid img-rounded" src="teaser/2025_tpami_ponderv2.png"></div>
			<div class="col-md-9">
			<b><font color="black">PonderV2: Improved 3D Representation with A Universal Pre-training Paradigm</font></b><br>
			Haoyi Zhu, Honghui Yang, Xiaoyang Wu, Di Huang, Sha Zhang, Xianglong He, <b>Hengshuang Zhao</b>, Chunhua Shen, Yu Qiao, Tong He, Wanli Ouyang.<br>
			IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>), 2025.</br>
			[<a href="https://ieeexplore.ieee.org/document/10969802">Paper</a>]
			[<a href="https://github.com/OpenGVLab/PonderV2">Code</a>]
            </div>
		</div><hr>
		
		<div class="row">
			<div class="col-md-3"><div class="badge">NeurIPS</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""><source src="teaser/2024_neurips_depthanythingv2.mp4" type="video/mp4"></video></div>
			<div class="col-md-9">
			<b><font color="black">Depth Anything V2</font></b><br>
			Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, <b>Hengshuang Zhao</b>.<br>
			Neural Information Processing Systems (<b>NeurIPS</b>), 2024.</br>
			[<a href="https://depth-anything-v2.github.io">Project</a>]
			[<a href="https://arxiv.org/abs/2406.09414">Paper</a>]
			[<a href="https://github.com/DepthAnything/Depth-Anything-V2">Code</a>]
			[<a href="https://huggingface.co/spaces/depth-anything/Depth-Anything-V2">Demo</a>]
			[<a href="https://x.com/_akhaliq/status/1801432403665125738">Media</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">NeurIPS</div><img class="img-fluid img-rounded" src="teaser/2024_neurips_mimicbrush.gif"></div>
			<div class="col-md-9">
			<b><font color="black">Zero-shot Image Editing with Reference Imitation</font></b><br>
			Xi Chen, Yutong Feng, Mengting Chen, Yiyang Wang, Shilong Zhang, Yu Liu, Yujun Shen, <b>Hengshuang Zhao</b>.<br>
			Neural Information Processing Systems (<b>NeurIPS</b>), 2024.</br>
			[<a href="https://xavierchen34.github.io/MimicBrush-Page">Project</a>]
			[<a href="https://arxiv.org/abs/2406.07547">Paper</a>]
			[<a href="https://github.com/ali-vilab/MimicBrush">Code</a>]
			[<a href="https://huggingface.co/spaces/xichenhku/MimicBrush">Demo</a>]
			[<a href="https://x.com/_akhaliq/status/1800726257098760584">Media</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">NeurIPS</div><img class="img-fluid img-rounded" src="teaser/2024_neurips_lit.jpg"></div>
			<div class="col-md-9">
			<b><font color="black">LiT: Unifying LiDAR "Languages" with LiDAR Translator</font></b><br>
			Yixing Lao, Tao Tang, Xiaoyang Wu, Peng Chen, Kaicheng Yu, <b>Hengshuang Zhao</b>.<br>
			Neural Information Processing Systems (<b>NeurIPS</b>), 2024.</br>
			[<a href="https://yxlao.github.io/lit">Project</a>]
			[<a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/aa76025af7f8d69338c4b5ee29f66e70-Paper-Conference.pdf">Paper</a>]
			[<a href="https://github.com/yxlao/lit">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">NeurIPS</div><img class="img-fluid img-rounded" src="teaser/2024_neurips_syncvis.jpg"></div>
			<div class="col-md-9">
			<b><font color="black">SyncVIS: Synchronized Video Instance Segmentation</font></b><br>
			Rongkun Zheng, Lu Qi, Xi Chen, Yi Wang, Kun Wang, Yu Qiao, <b>Hengshuang Zhao</b>.<br>
			Neural Information Processing Systems (<b>NeurIPS</b>), 2024.</br>
			[<a href="https://arxiv.org/abs/2412.00882">Paper</a>]
			[<a href="https://github.com/rkzheng99/SyncVIS">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">NeurIPS</div><img class="img-fluid img-rounded" src="teaser/2024_neurips_onedet3d.png"></div>
			<div class="col-md-9">
			<b><font color="black">One for All: Multi-Domain Joint Training for Point Cloud Based 3D Object Detection</font></b><br>
			Zhenyu Wang, Yali Li, <b>Hengshuang Zhao<sup>†</sup></b>, Shengjin Wang. (†: corresponding)<br>
			Neural Information Processing Systems (<b>NeurIPS</b>), 2024.</br>
			[<a href="https://arxiv.org/abs/2411.01584">Paper</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">NeurIPS</div><img class="img-fluid img-rounded" src="teaser/2024_neurips_lion.jpg"></div>
			<div class="col-md-9">
			<b><font color="black">LION: Linear Group RNN for 3D Object Detection in Point Clouds</font></b><br>
			Zhe Liu, Jinghua Hou, Xinyu Wang, Xiaoqing Ye, Jingdong Wang, <b>Hengshuang Zhao</b>, Xiang Bai.<br>
			Neural Information Processing Systems (<b>NeurIPS</b>), 2024.</br>
			[<a href="https://happinesslz.github.io/projects/LION">Project</a>]
			[<a href="https://arxiv.org/abs/2407.18232">Paper</a>]
			[<a href="https://github.com/happinesslz/LION">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ECCV</div><img class="img-fluid img-rounded" src="teaser/2024_eccv_livephoto.gif"></div>
			<div class="col-md-9">
			<b><font color="black">LivePhoto: Real Image Animation with Text-guided Motion Control</font></b><br>
			Xi Chen, Zhiheng Liu, Mengting Chen, Yutong Feng, Yu Liu, Yujun Shen, <b>Hengshuang Zhao</b>.<br>
			European Conference on Computer Vision (<b>ECCV</b>), 2024.<br>
			[<a href="https://xavierchen34.github.io/LivePhoto-Page">Project</a>]
			[<a href="https://arxiv.org/abs/2312.02928">Paper</a>]
			[<a href="https://github.com/XavierCHEN34/LivePhoto">Code</a>]
			[<a href="https://www.youtube.com/watch?v=M2vzrTYAsQI">Video</a>]
            </div>
		</div><hr>

		<div class="row">
			<!-- <div class="col-md-3"><div class="badge">ECCV</div><img class="img-fluid img-rounded" src="teaser/2024_eccv_pixelgs.png"></div> -->
			<div class="col-md-3"><div class="badge">ECCV</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""><source src="teaser/2024_eccv_pixelgs.mp4" type="video/mp4"></video></div>
			<div class="col-md-9">
			<b><font color="black">Pixel-GS: Density Control with Pixel-aware Gradient for 3D Gaussian Splatting</font></b><br>
			Zheng Zhang, Wenbo Hu, Yixing Lao, Tong He, <b>Hengshuang Zhao</b>.<br>
			European Conference on Computer Vision (<b>ECCV</b>), 2024.<br>
			[<a href="https://pixelgs.github.io">Project</a>]
			[<a href="https://arxiv.org/abs/2403.15530">Paper</a>]
			[<a href="https://github.com/zhengzhang01/Pixel-GS">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ECCV</div><img class="img-fluid img-rounded" src="teaser/2024_eccv_insmapper.png"></div>
			<div class="col-md-9">
			<b><font color="black">InsMapper: Exploring Inner-instance Information for Vectorized HD Mapping</font></b><br>
			Zhenhua Xu, Kwan-Yee. K. Wong, <b>Hengshuang Zhao</b>.<br>
			European Conference on Computer Vision (<b>ECCV</b>), 2024.<br>
			[<a href="https://tonyxuqaq.github.io/InsMapper">Project</a>]
			[<a href="https://arxiv.org/abs/2308.08543">Paper</a>]
			[<a href="https://github.com/TonyXuQAQ/InsMapper">Code</a>]
			[<a href="https://youtu.be/MapB7TrNnLY">Video</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ECCV</div><img class="img-fluid img-rounded" src="teaser/2024_eccv_ovuni3detr.png"></div>
			<div class="col-md-9">
			<b><font color="black">OV-Uni3DETR: Towards Unified Open-Vocabulary 3D Object Detection via Cycle-Modality Propagation</font></b><br>
			Zhenyu Wang, Yali Li, Taichi Liu, <b>Hengshuang Zhao<sup>†</sup></b>, Shengjin Wang. (†: corresponding)<br>
			European Conference on Computer Vision (<b>ECCV</b>), 2024.<br>
			[<a href="https://arxiv.org/abs/2403.19580">Paper</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ECCV</div><img class="img-fluid img-rounded" src="teaser/2024_eccv_logosticker.png"></div>
			<div class="col-md-9">
			<b><font color="black">LogoSticker: Inserting Logos into Diffusion Models for Customized Generation</font></b><br>
			Mingkang Zhu, Xi Chen, Zhongdao Wang, <b>Hengshuang Zhao</b>, Jiaya Jia.<br>
			European Conference on Computer Vision (<b>ECCV</b>), 2024.<br>
			[<a href="https://mingkangz.github.io/logosticker">Project</a>]
			[<a href="https://arxiv.org/abs/2407.13752">Paper</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ECCV</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""><source src="teaser/2024_eccv_openins3d.mp4" type="video/mp4"></video></div>
			<div class="col-md-9">
			<b><font color="black">OpenIns3D: Snap and Lookup for 3D Open-vocabulary Instance Segmentation</font></b><br>
			Zhening Huang, Xiaoyang Wu, Xi Chen, <b>Hengshuang Zhao<sup>†</sup></b>, Lei Zhu, Joan Lasenby. (†: corresponding)<br>
			European Conference on Computer Vision (<b>ECCV</b>), 2024.<br>
			[<a href="https://zheninghuang.github.io/OpenIns3D">Project</a>]
			[<a href="https://arxiv.org/abs/2309.00616">Paper</a>]
			[<a href="https://github.com/Pointcept/OpenIns3D">Code</a>]
			[<a href="https://www.youtube.com/watch?v=kwlMJkEfTyY">Video</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ECCV</div><img class="img-fluid img-rounded" src="teaser/2024_eccv_diki.png"></div>
			<div class="col-md-9">
			<b><font color="black">Mind the Interference: Retaining Pre-trained Knowledge in Parameter Efficient Continual Learning of Vision-Language Models</font></b><br>
			Longxiang Tang, Zhuotao Tian, Kai Li, Chunming He, Hantao Zhou, <b>Hengshuang Zhao</b>, Xiu Li, Jiaya Jia.<br>
			European Conference on Computer Vision (<b>ECCV</b>), 2024.<br>
			[<a href="https://arxiv.org/abs/2407.05342">Paper</a>]
			[<a href="https://github.com/lloongx/DIKI">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""><source src="teaser/2024_cvpr_depthanything.mp4" type="video/mp4"></video></div>
			<div class="col-md-9">
			<b><font color="black">Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data</font></b><br>
			Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, <b>Hengshuang Zhao</b>.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024.</br>
			[<a href="https://depth-anything.github.io">Project</a>]
			[<a href="https://arxiv.org/abs/2401.10891">Paper</a>]
			[<a href="https://github.com/LiheYoung/Depth-Anything">Code</a>]
			[<a href="https://huggingface.co/spaces/LiheYoung/Depth-Anything">Demo</a>]
			[<a href="https://twitter.com/_akhaliq/status/1749284669936275463">Media</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/2024_cvpr_anydoor.gif"></div>
			<div class="col-md-9">
			<b><font color="black">AnyDoor: Zero-shot Object-level Image Customization</font></b><br>
			Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, <b>Hengshuang Zhao</b>.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024.</br>
			[<a href="https://ali-vilab.github.io/AnyDoor-Page">Project</a>]
			[<a href="https://arxiv.org/abs/2307.09481">Paper</a>]
			[<a href="https://github.com/ali-vilab/AnyDoor">Code</a>]
			[<a href="https://huggingface.co/spaces/xichenhku/AnyDoor-online">Demo</a>]
			[<a href="https://twitter.com/_akhaliq/status/1738772616142303728">Media</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR Oral</div><img class="img-fluid img-rounded" src="teaser/2024_cvpr_pointtransformerv3.png"></div>
			<div class="col-md-9">
			<b><font color="black">Point Transformer V3: Simpler, Faster, Stronger</font></b><br>
			Xiaoyang Wu, Li Jiang, Peng-Shuai Wang, Zhijian Liu, Xihui Liu, Yu Qiao, Wanli Ouyang, Tong He, <b>Hengshuang Zhao</b>.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024. <b><font color="firebrick">Oral</font></b></br>
			Ranked 1st place in the CVPR 2024 <a href="https://waymo.com/open/challenges">Waymo 3D Semantic Segmentation Challenge</a>.</br>
			[<a href="https://arxiv.org/abs/2312.10035">Paper</a>]
			[<a href="https://github.com/Pointcept/PointTransformerV3">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/2024_cvpr_ppt.png"></div>
			<div class="col-md-9">
			<b><font color="black">Towards Large-scale 3D Representation Learning with Multi-dataset Point Prompt Training</font></b><br>
			Xiaoyang Wu, Zhuotao Tian, Xin Wen, Bohao Peng, Xihui Liu, Kaicheng Yu, <b>Hengshuang Zhao</b>.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024.</br>
			[<a href="https://arxiv.org/abs/2308.09718">Paper</a>]
			[<a href="https://github.com/Pointcept/Pointcept">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR Highlight</div><img class="img-fluid img-rounded" src="teaser/2024_cvpr_gpt4point.png"></div>
			<div class="col-md-9">
			<b><font color="black">GPT4Point: A Unified Framework for Point-Language Understanding and Generation</font></b><br>
			Zhangyang Qi, Ye Fang, Zeyi Sun, Xiaoyang Wu, Tong Wu, Jiaqi Wang, Dahua Lin, <b>Hengshuang Zhao</b>.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024. <b><font color="firebrick">Highlight</font></b></br>
			[<a href="https://gpt4point.github.io">Project</a>]
			[<a href="https://arxiv.org/abs/2312.02980">Paper</a>]
			[<a href="https://github.com/Pointcept/GPT4Point">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR Highlight</div><img class="img-fluid img-rounded" src="teaser/2024_cvpr_unimode.png"></div>
			<div class="col-md-9">
			<b><font color="black">UniMODE: Universal Monocular 3D Object Detection</font></b><br>
			Zhuoling Li, Xiaogang Xu, Ser-Nam Lim, <b>Hengshuang Zhao</b>.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024. <b><font color="firebrick">Highlight</font></b></br>
			[<a href="https://arxiv.org/abs/2402.18573">Paper</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/2024_cvpr_groupcontrast.png"></div>
			<div class="col-md-9">
			<b><font color="black">GroupContrast: Semantic-aware Self-supervised Representation Learning for 3D Understanding</font></b><br>
			Chengyao Wang, Li Jiang, Xiaoyang Wu, Zhuotao Tian, Bohao Peng, <b>Hengshuang Zhao<sup>†</sup></b>, Jiaya Jia. (†: corresponding)<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024.</br>
			[<a href="https://arxiv.org/abs/2403.09639">Paper</a>]
			[<a href="https://github.com/dvlab-research/GroupContrast">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/2024_cvpr_oacnns.png"></div>
			<div class="col-md-9">
			<b><font color="black">OA-CNNs: Omni-Adaptive Sparse CNNs for 3D Semantic Segmentation</font></b><br>
			Bohao Peng, Xiaoyang Wu, Li Jiang, Yukang Chen, <b>Hengshuang Zhao</b>, Zhuotao Tian, Jiaya Jia.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024.</br>
			[<a href="https://arxiv.org/abs/2403.14418">Paper</a>]
			[<a href="https://github.com/Pointcept/Pointcept">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""><source src="teaser/2024_cvpr_dreamcomposer.mov" type="video/mp4"></video></div>
			<div class="col-md-9">
			<b><font color="black">DreamComposer: Controllable 3D Object Generation via Multi-View Conditions</font></b><br>
			Yunhan Yang, Yukun Huang, Xiaoyang Wu, Yuan-Chen Guo, Song-Hai Zhang, <b>Hengshuang Zhao</b>, Tong He, Xihui Liu.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024.</br>
			[<a href="https://yhyang-myron.github.io/DreamComposer">Project</a>]
			[<a href="https://arxiv.org/abs/2312.03611">Paper</a>]
			[<a href="https://github.com/yhyang-myron/DreamComposer">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/2024_cvpr_zsvg3d.png"></div>
			<div class="col-md-9">
			<b><font color="black">Visual Programming for Zero-shot Open-Vocabulary 3D Visual Grounding</font></b><br>
			Zhihao Yuan, Jinke Ren, Chun-Mei Feng, <b>Hengshuang Zhao</b>, Shuguang Cui, Zhen Li.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024.</br>
			[<a href="https://curryyuan.github.io/ZSVG3D">Project</a>]
			[<a href="https://arxiv.org/abs/2311.15383">Paper</a>]
			[<a href="https://github.com/CurryYuan/ZSVG3D">Code</a>]
			[<a href="https://youtu.be/YGP1R3IgVWU">Video</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/2024_cvpr_unipad.png"></div>
			<div class="col-md-9">
			<b><font color="black">UniPAD: A Universal Pre-training Paradigm for Autonomous Driving</font></b><br>
			Honghui Yang, Sha Zhang, Di Huang, Xiaoyang Wu, Haoyi Zhu, Tong He, Shixiang Tang, <b>Hengshuang Zhao</b>, Qibo Qiu, Binbin Lin, Xiaofei He, Wanli Ouyang.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024.</br>
			[<a href="https://arxiv.org/abs/2310.08370">Paper</a>]
			[<a href="https://github.com/Nightmare-n/UniPAD">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ICLR Highlight</div><img class="img-fluid img-rounded" src="teaser/2024_iclr_iba.png"></div>
			<div class="col-md-9">
			<b><font color="black">Influencer Backdoor Attack on Semantic Segmentation</font></b><br>
			Haoheng Lan, Jindong Gu, Philip Torr, <b>Hengshuang Zhao</b>.<br>
			International Conference on Learning Representations (<b>ICLR</b>), 2024. <b><font color="firebrick">Highlight</font></b></br>
			[<a href="https://arxiv.org/abs/2303.12054">Paper</a>]
			[<a href="https://github.com/Maxproto/IBA">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">3DV</div><img class="img-fluid img-rounded" src="teaser/2024_3dv_ocbev.png"></div>
			<div class="col-md-9">
			<b><font color="black">OCBEV: Object-Centric BEV Transformer for Multi-View 3D Object Detection</font></b><br>
			Zhangyang Qi, Jiaqi Wang, Xiaoyang Wu, <b>Hengshuang Zhao</b>.<br>
			International Conference on 3D Vision (<b>3DV</b>), 2024.</br>
			[<a href="https://arxiv.org/abs/2306.01738">Paper</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">TPAMI</div><img class="img-fluid img-rounded" src="teaser/2024_tpami_lavt2.png"></div>
			<div class="col-md-9">
			<b><font color="black">Language-Aware Vision Transformer for Referring Segmentation</font></b><br>
			Zhao Yang, Jiaqi Wang, Xubing Ye, Yansong Tang, Kai Chen, <b>Hengshuang Zhao</b>, Philip Torr.<br>
			IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>), 2024.</br>
			[<a href="https://ieeexplore.ieee.org/document/10694805">Paper</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">TPAMI</div><img class="img-fluid img-rounded" src="teaser/2024_tpami_unidetector2.png"></div>
			<div class="col-md-9">
			<b><font color="black">UniDetector: Towards Universal Object Detection with Heterogeneous Supervision</font></b><br>
			Zhenyu Wang, Yali Li, Xi Chen, Ser-Nam Lim, Antonio Torralba, <b>Hengshuang Zhao<sup>†</sup></b>, Shengjin Wang. (†: corresponding)<br>
			IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>), 2024.</br>
			[<a href="https://ieeexplore.ieee.org/document/10552883">Paper</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">RA-L</div><img class="img-fluid img-rounded" src="teaser/2024_ral_drivegpt4.gif"></div>
			<div class="col-md-9">
			<b><font color="black">DriveGPT4: Interpretable End-to-end Autonomous Driving via Large Language Model</font></b><br>
			Zhenhua Xu, Yujia Zhang, Enze Xie, Zhen Zhao, Yong Guo, Kwan-Yee. K. Wong, Zhenguo Li, <b>Hengshuang Zhao</b>.<br>
			IEEE Robotics and Automation Letters (<b>RA-L</b>), 2024.</br>
			[<a href="https://tonyxuqaq.github.io/projects/DriveGPT4">Project</a>]
			[<a href="https://arxiv.org/abs/2310.01412">Paper</a>]
			[<a href="https://drive.google.com/drive/folders/1PsGL7ZxMMz1ZPDS5dZSjzjfPjuPHxVL5">Code</a>]
			[<a href="https://youtu.be/CPoskBNjHlk">Video</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">RA-L</div><img class="img-fluid img-rounded" src="teaser/2024_ral_grouplane.png"></div>
			<div class="col-md-9">
			<b><font color="black">GroupLane: End-to-End 3D Lane Detection With Channel-Wise Grouping</font></b><br>
			Zhuoling Li, Chunrui Han, Zheng Ge, Jinrong Yang, En Yu, Haoqian Wang, Xiangyu Zhang, <b>Hengshuang Zhao</b>.<br>
			IEEE Robotics and Automation Letters (<b>RA-L</b>), 2024.</br>
			[<a href="https://ieeexplore.ieee.org/abstract/document/10706836">Paper</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">NeurIPS</div><img class="img-fluid img-rounded" src="teaser/2023_neurips_freemask.png"></div>
			<div class="col-md-9">
			<b><font color="black">FreeMask: Synthetic Images with Dense Annotations Make Stronger Segmentation Models</font></b><br>
			Lihe Yang, Xiaogang Xu, Bingyi Kang, Yinghuan Shi, <b>Hengshuang Zhao</b>.<br>
			Neural Information Processing Systems (<b>NeurIPS</b>), 2023.</br>
			[<a href="https://arxiv.org/abs/2310.15160">Paper</a>]
			[<a href="https://github.com/LiheYoung/FreeMask">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">NeurIPS</div><img class="img-fluid img-rounded" src="teaser/2023_neurips_uni3detr.png"></div>
			<div class="col-md-9">
			<b><font color="black">Uni3DETR: Unified 3D Detection Transformer</font></b><br>
			Zhenyu Wang, Yali Li, Xi Chen, <b>Hengshuang Zhao<sup>†</sup></b>, Shengjin Wang. (†: corresponding)<br>
			Neural Information Processing Systems (<b>NeurIPS</b>), 2023.</br>
			[<a href="https://arxiv.org/abs/2310.05699">Paper</a>]
			[<a href="https://github.com/zhenyuw16/Uni3DETR">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">NeurIPS</div><img class="img-fluid img-rounded" src="teaser/2023_neurips_tmtvis.png"></div>
			<div class="col-md-9">
			<b><font color="black">TMT-VIS: Taxonomy-aware Multi-dataset Joint Training for Video Instance Segmentation</font></b><br>
			Rongkun Zheng, Lu Qi, Xi Chen, Yi Wang, Kun Wang, Yu Qiao, <b>Hengshuang Zhao</b>.<br>
			Neural Information Processing Systems (<b>NeurIPS</b>), 2023.</br>
			[<a href="https://arxiv.org/abs/2312.06630">Paper</a>]
			[<a href="https://github.com/rkzheng99/TMT-VIS">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">NeurIPS</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""><source src="teaser/2023_neurips_corresnerf.mp4" type="video/mp4"></video></div>
			<div class="col-md-9">
			<b><font color="black">CorresNeRF: Image Correspondence Priors for Neural Radiance Fields</font></b><br>
			Yixing Lao, Xiaogang Xu, Zhipeng Cai, Xihui Liu, <b>Hengshuang Zhao</b>.<br>
			Neural Information Processing Systems (<b>NeurIPS</b>), 2023.</br>
			[<a href="https://yxlao.github.io/corres-nerf">Project</a>]
			[<a href="https://arxiv.org/abs/2312.06642">Paper</a>]
			[<a href="https://github.com/yxlao/corres-nerf">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ICCV</div><img class="img-fluid img-rounded" src="teaser/2023_iccv_opsnet.gif"></div>
			<div class="col-md-9">
			<b><font color="black">Open-vocabulary Panoptic Segmentation with Embedding Modulation</font></b><br>
			Xi Chen, Shuang Li, Ser-Nam Lim, Antonio Torralba, <b>Hengshuang Zhao</b>.<br>
			International Conference on Computer Vision (<b>ICCV</b>), 2023.</br>
			[<a href="https://opsnet-page.github.io">Project</a>]
			[<a href="https://arxiv.org/abs/2303.11324">Paper</a>]
			[<a href="https://github.com/XavierCHEN34/OPSNet">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ICCV</div><img class="img-fluid img-rounded" src="teaser/2023_iccv_shrinkmatch.png"></div>
			<div class="col-md-9">
			<b><font color="black">Shrinking Class Space for Enhanced Certainty in Semi-Supervised Learning</font></b><br>
			Lihe Yang, Zhen Zhao, Lei Qi, Yu Qiao, Yinghuan Shi, <b>Hengshuang Zhao</b>.<br>
			International Conference on Computer Vision (<b>ICCV</b>), 2023.</br>
			[<a href="https://arxiv.org/abs/2308.06777">Paper</a>]
			[<a href="https://github.com/LiheYoung/ShrinkMatch">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ICCV</div><img class="img-fluid img-rounded" src="teaser/2023_iccv_bt2.png"></div>
			<div class="col-md-9">
			<b><font color="black">BT<sup>2</sup>: Backward-compatible Training with Basis Transformation</font></b><br>
			Yifei Zhou, Zilu Li, Abhinav Shrivastava, <b>Hengshuang Zhao</b>, Antonio Torralba, Taipeng Tian, Ser-Nam Lim.<br>
			International Conference on Computer Vision (<b>ICCV</b>), 2023.</br>
			[<a href="https://arxiv.org/abs/2211.03989">Paper</a>]
			[<a href="https://github.com/YifeiZhou02/BT-2">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ICCVW</div><img class="img-fluid img-rounded" src="teaser/2023_iccvw_sam3d.png"></div>
			<div class="col-md-9">
			<b><font color="black">SAM3D: Segment Anything in 3D Scenes</font></b><br>
			Yunhan Yang, Xiaoyang Wu, Tong He, <b>Hengshuang Zhao</b>, Xihui Liu.<br>
			International Conference on Computer Vision Workshop (<b>ICCVW</b>), 2023.</br>
			[<a href="https://arxiv.org/abs/2306.03908">Paper</a>]
			[<a href="https://github.com/Pointcept/SegmentAnything3D">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/2023_cvpr_msc.png"></div>
			<div class="col-md-9">
			<b><font color="black">Masked Scene Contrast: A Scalable Framework for Unsupervised 3D Representation Learning</font></b><br>
			Xiaoyang Wu, Xin Wen, Xihui Liu, <b>Hengshuang Zhao</b>.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2023.</br>
			[<a href="https://arxiv.org/abs/2303.14191">Paper</a>]
			[<a href="https://github.com/Pointcept/Pointcept">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/2023_cvpr_unidetector.png"></div>
			<div class="col-md-9">
			<b><font color="black">Detecting Everything in the Open World: Towards Universal Object Detection</font></b><br>
			Zhenyu Wang, Yali Li, Xi Chen, Ser-Nam Lim, Antonio Torralba, <b>Hengshuang Zhao<sup>†</sup></b>, Shengjin Wang. (†: corresponding)<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2023.</br>
			[<a href="https://arxiv.org/abs/2303.11749">Paper</a>]
			[<a href="https://github.com/zhenyuw16/UniDetector">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/2023_cvpr_modsquad.png"></div>
			<div class="col-md-9">
			<b><font color="black">Mod-Squad: Designing Mixtures of Experts As Modular Multi-Task Learners</font></b><br>
			Zitian Chen, Yikang Shen, Mingyu Ding, Zhenfang Chen, <b>Hengshuang Zhao</b>, Erik Learned-Miller, Chuang Gan.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2023.</br>
			[<a href="https://vis-www.cs.umass.edu/mod-squad">Proj</a>]
			[<a href="https://arxiv.org/abs/2212.08066">Paper</a>]
			[<a href="https://github.com/UMass-Foundation-Model/Mod-Squad">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">AAAI</div><img class="img-fluid img-rounded" src="teaser/2023_aaai_sadlr.png"></div>
			<div class="col-md-9">
			<b><font color="black">Semantics-Aware Dynamic Localization and Refinement for Referring Image Segmentation</font></b><br>
			Zhao Yang, Jiaqi Wang, Yansong Tang, Kai Chen, <b>Hengshuang Zhao</b>, Philip Torr.<br>
			AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2023.</br>
			[<a href="https://arxiv.org/abs/2303.06345">Paper</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">IJCAI</div><img class="img-fluid img-rounded" src="teaser/2023_ijcai_uada.png"></div>
			<div class="col-md-9">
			<b><font color="black">Universal Adaptive Data Augmentation</font></b><br>
			Xiaogang Xu, <b>Hengshuang Zhao</b>.<br>
			International Joint Conferences on Artificial Intelligence (<b>IJCAI</b>), 2023.</br>
			[<a href="https://arxiv.org/abs/2207.06658">Paper</a>]
			[<a href="https://github.com/xiaogang00/UADA">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">IJCV</div><img class="img-fluid img-rounded" src="teaser/2023_ijcv_physformer++.png"></div>
			<div class="col-md-9">
			<b><font color="black">PhysFormer++: Facial Video-based Physiological Measurement with SlowFast Temporal Difference Transformer</font></b><br>
			Zitong Yu, Yuming Shen, Jingang Shi, <b>Hengshuang Zhao</b>, Yawen Cui, Jiehua Zhang, Philip Torr, Guoying Zhao.<br>
			International Journal of Computer Vision (<b>IJCV</b>), 2023.</br>
			[<a href="https://arxiv.org/abs/2302.03548">Paper</a>]
			[<a href="https://github.com/ZitongYu/PhysFormer">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">NeurIPS</div><img class="img-fluid img-rounded" src="teaser/2022_neurips_pointtransformerv2.png"></div>
			<div class="col-md-9">
			<b><font color="black">Point Transformer V2: Grouped Vector Attention and Partition-based Pooling</font></b><br>
			Xiaoyang Wu, Yixing Lao, Li Jiang, Xihui Liu, <b>Hengshuang Zhao</b>.<br>
			Neural Information Processing Systems (<b>NeurIPS</b>), 2022.</br>
			[<a href="https://arxiv.org/abs/2210.05666">Paper</a>]
			[<a href="https://github.com/Gofinge/PointTransformerV2">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ECCV</div><img class="img-fluid img-rounded" src="teaser/2022_eccv_mtformer.png"></div>
			<div class="col-md-9">
			<b><font color="black">MTFormer: Multi-Task Learning via Transformer and Cross-Task Reasoning</font></b><br>
			Xiaogang Xu*, <b>Hengshuang Zhao*</b>, Vibhav Vineet, Ser-Nam Lim, Antonio Torralba. (*: equal contribution)<br>
			European Conference on Computer Vision (<b>ECCV</b>), 2022.</br>
			[<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136870299.pdf">Paper</a>]			
			[<a href="https://github.com/xiaogang00/MTFormer">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ECCV</div><img class="img-fluid img-rounded" src="teaser/2022_eccv_segpgd.png"></div>
			<div class="col-md-9">
			<b><font color="black">SegPGD: An Effective and Efficient Adversarial Attack for Evaluating and Boosting Segmentation Robustness</font></b><br>
			Jindong Gu, <b>Hengshuang Zhao</b>, Volker Tresp, Philip Torr.<br>
			European Conference on Computer Vision (<b>ECCV</b>), 2022.</br>
			[<a href="https://arxiv.org/abs/2207.12391">Paper</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ECCV</div><img class="img-fluid img-rounded" src="teaser/2022_eccv_decouplenet.png"></div>
			<div class="col-md-9">
			<b><font color="black">DecoupleNet: Decoupled Network for Domain Adaptive Semantic Segmentation</font></b><br>
			Xin Lai, Zhuotao Tian, Xiaogang Xu, Yingcong Chen, Shu Liu, <b>Hengshuang Zhao</b>, Liwei wang, Jiaya Jia.<br>
			European Conference on Computer Vision (<b>ECCV</b>), 2022.</br>
			[<a href="https://arxiv.org/abs/2207.09988">Paper</a>]
			[<a href="https://github.com/dvlab-research/DecoupleNet">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">RSSW</div><img class="img-fluid img-rounded" src="teaser/2022_rssw_vsn.png"></div>
			<div class="col-md-9">
			<b><font color="black">Towards Visual Social Navigation in Photo-realistic Indoor Scenes</font></b><br>
			Feng Gao, <b>Hengshuang Zhao</b>, Yu Wang.<br>
			Robotics: Science and Systems (<b>RSS</b>) Workshop, 2022.</br>
			[<a href="https://social-intelligence-human-ai.github.io/RSS2022/docs/camready_3.pdf">Paper</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/2022_cvpr_focalclick.png"></div>
			<div class="col-md-9">
			<b><font color="black">FocalClick: Towards Practical Interactive Image Segmentation</font></b><br>
			Xi Chen, Zhiyan Zhao, Yilei Zhang, Manni Duan, Donglian Qi, <b>Hengshuang Zhao</b>.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2022.</br>
			[<a href="https://arxiv.org/abs/2204.02574">Paper</a>]
			[<a href="https://github.com/XavierCHEN34/ClickSEG">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/2022_cvpr_lavt.png"></div>
			<div class="col-md-9">
			<b><font color="black">LAVT: Language-Aware Vision Transformer for Referring Image Segmentation</font></b><br>
			Zhao Yang, Jiaqi Wang, Yansong Tang, Kai Chen, <b>Hengshuang Zhao</b>, Philip Torr.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2022.</br>
			[<a href="https://arxiv.org/abs/2112.02244">Paper</a>]
			[<a href="https://github.com/yz93/LAVT-RIS">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/2022_cvpr_gfsseg.png"></div>
			<div class="col-md-9">
			<b><font color="black">Generalized Few-shot Semantic Segmentation</font></b><br>
			Zhuotao Tian, Xin Lai, Li Jiang, Michelle Shu, <b>Hengshuang Zhao</b>, Jiaya Jia.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2022.</br>
			[<a href="https://arxiv.org/abs/2010.05210">Paper</a>]
			[<a href="https://github.com/dvlab-research/GFS-Seg">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/2022_cvpr_physformer.png"></div>
			<div class="col-md-9">
			<b><font color="black">PhysFormer: Facial Video-based Physiological Measurement with Temporal Difference Transformer</font></b><br>
			Zitong Yu, Yuming Shen, Jingang Shi, <b>Hengshuang Zhao</b>, Philip Torr, Guoying Zhao.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2022.</br>
			[<a href="https://arxiv.org/abs/2111.12082">Paper</a>]
			[<a href="https://github.com/ZitongYu/PhysFormer">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/2022_cvpr_stratifiedtransformer.png"></div>
			<div class="col-md-9">
			<b><font color="black">Stratified Transformer for 3D Point Cloud Segmentation</font></b><br>
			Xin Lai, Jianhui Liu, Li Jiang, Liwei Wang, <b>Hengshuang Zhao</b>, Shu Liu, Xiaojuan Qi, Jiaya Jia.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2022.</br>
			[<a href="https://arxiv.org/abs/2203.14508">Paper</a>]
			[<a href="https://github.com/dvlab-research/Stratified-Transformer">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ICRA</div><img class="img-fluid img-rounded" src="teaser/2022_icra_pvcl.png"></div>
			<div class="col-md-9">
			<b><font color="black">Prototype-Voxel Contrastive Learning for LiDAR Point Cloud Panoptic Segmentation</font></b><br>
			Minzhe Liu, Zhou Qiang, <b>Hengshuang Zhao</b>, Jianing Li, Yuan Du, Kurt Keutzer, Li Du, Shanghang Zhang.<br>
			International Conference on Robotics and Automation (<b>ICRA</b>), 2022.</br>
			[<a href="paper/icra22_pvcl.pdf">Paper</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">TPAMI</div><img class="img-fluid img-rounded" src="teaser/2022_tpami_panopticfcn2.png"></div>
			<div class="col-md-9">
			<b><font color="black">Fully Convolutional Networks for Panoptic Segmentation with Point-based Supervision</font></b><br>
			Yanwei Li, <b>Hengshuang Zhao</b>, Xiaojuan Qi, Yukang Chen, Lu Qi, Liwei Wang, Zeming Li, Jian Sun, Jiaya Jia.<br>
			IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>), 2022. </br>
			[<a href="https://arxiv.org/abs/2108.07682">Paper</a>]
			[<a href="https://github.com/dvlab-research/PanopticFCN">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">TPAMI</div><img class="img-fluid img-rounded" src="teaser/2022_tpami_entity.png"></div>
			<div class="col-md-9">
			<b><font color="black">Open World Entity Segmentation</font></b><br>
			Lu Qi, Jason Kuen, Yi Wang, Jiuxiang Gu, <b>Hengshuang Zhao</b>, Philip Torr, Zhe Lin, Jiaya Jia.<br>
			IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>), 2022. </br>
			[<a href="http://luqi.info/Entity_Web">Project</a>]
			[<a href="https://arxiv.org/abs/2107.14228">Paper</a>]
			[<a href="https://github.com/dvlab-research/Entity">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">TPAMI</div><img class="img-fluid img-rounded" src="teaser/2022_tpami_apd.png"></div>
			<div class="col-md-9">
			<b><font color="black">Adaptive Perspective Distillation for Semantic Segmentation</font></b><br>
			Zhuotao Tian, Pengguang Chen, Xin Lai, Li Jiang, Shu Liu, <b>Hengshuang Zhao</b>, Bei Yu, Ming-Chang Yang, Jiaya Jia.<br>
			IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>), 2022. </br>
			[<a href="paper/tpami22_apd.pdf">Paper</a>]
			[<a href="https://github.com/dvlab-research/APD">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">TPAMI</div><img class="img-fluid img-rounded" src="teaser/2022_tpami_set.png"></div>
			<div class="col-md-9">
			<b><font color="black">Patch-based Separable Transformer for Visual Recognition</font></b><br>
			Shuyang Sun, Xiaoyu Yue, <b>Hengshuang Zhao</b>, Philip Torr, Song Bai.<br>
			IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>), 2022. </br>
			[<a href="paper/tpami22_set.pdf">Paper</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">NeurIPS</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""><source src="teaser/2021_neurips_unitrack.mp4" type="video/mp4"></video></div>
			<div class="col-md-9">
			<b><font color="black">Do Different Tracking Tasks Require Different Appearance Models?</font></b><br>
			Zhongdao Wang, <b>Hengshuang Zhao</b>, Yali Li, Shengjin Wang, Philip Torr, Luca Bertinetto.<br>
			Neural Information Processing Systems (<b>NeurIPS</b>), 2021.</br>
			[<a href="https://zhongdao.github.io/UniTrack">Project</a>]
			[<a href="https://arxiv.org/abs/2107.02156">Paper</a>]
			[<a href="https://github.com/Zhongdao/UniTrack">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">BMVC</div><img class="img-fluid img-rounded" src="teaser/2021_bmvc_hinet.gif"></div>
			<div class="col-md-9">
			<b><font color="black">Hierarchical Interaction Network for Video Object Segmentation from Referring Expressions</font></b><br>
			Zhao Yang, Yansong Tang, Luca Bertinetto, <b>Hengshuang Zhao</b>, Philip Torr.<br>
			British Machine Vision Conference (<b>BMVC</b>), 2021.</br>
			[<a href="https://www.bmvc2021-virtualconference.com/conference/papers/paper_0386.html">Paper</a>]
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ICCV Oral</div><img class="img-fluid img-rounded" src="teaser/2021_iccv_pointtransformer.png"></div>
			<div class="col-md-9">
			<b><font color="black">Point Transformer</font></b><br>
			<b>Hengshuang Zhao</b>, Li Jiang, Jiaya Jia, Philip Torr, Vladlen Koltun.<br>
			International Conference on Computer Vision (<b>ICCV</b>), 2021. <b><font color="firebrick">Oral</font></b></br>
			[<a href="http://arxiv.org/abs/2012.09164">Paper</a>]
			[<a href="https://github.com/Gofinge/PointTransformerV2/tree/main/pcr/models/point_transformer">Code</a>]
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ICCV</div><img class="img-fluid img-rounded" src="teaser/2021_iccv_ddcat.png"></div>
			<div class="col-md-9">
			<b><font color="black">Dynamic Divide-and-Conquer Adversarial Training for Robust Semantic Segmentation</font></b><br>
			Xiaogang Xu, <b>Hengshuang Zhao</b>, Jiaya Jia.<br>
			International Conference on Computer Vision (<b>ICCV</b>), 2021.</br>
			[<a href="http://arxiv.org/abs/2003.06555">Paper</a>]
			[<a href="https://github.com/dvlab-research/Robust-Semantic-Segmentation">Code</a>]
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR Oral</div><img class="img-fluid img-rounded" src="teaser/2021_cvpr_bpnet.png"></div>
			<div class="col-md-9">
			<b><font color="black">Bidirectional Projection Network for Cross Dimension Scene Understanding</font></b><br>
			Wenbo Hu*, <b>Hengshuang Zhao*</b>, Li Jiang, Jiaya Jia, Tien-Tsin Wong. (*: equal contribution)<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2021. <b><font color="firebrick">Oral</font></b></br>
			[<a href="https://wbhu.github.io/project/BPNet">Project</a>]
			[<a href="https://arxiv.org/abs/2103.14326">Paper</a>]
			[<a href="https://github.com/wbhu/BPNet">Code</a>]
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR Oral</div><img class="img-fluid img-rounded" src="teaser/2021_cvpr_panopticfcn.png"></div>
			<div class="col-md-9">
			<b><font color="black">Fully Convolutional Networks for Panoptic Segmentation</font></b><br>
			Yanwei Li, <b>Hengshuang Zhao</b>, Xiaojuan Qi, Liwei Wang, Zeming Li, Jian Sun, Jiaya Jia.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2021. <b><font color="firebrick">Oral</font></b></br>
			[<a href="http://arxiv.org/abs/2012.00720">Paper</a>]
			[<a href="http://github.com/yanwei-li/PanopticFCN">Code</a>]
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/2021_cvpr_reviewkd.png"></div>
			<div class="col-md-9">
			<b><font color="black">Distilling Knowledge via Knowledge Review</font></b><br>
			Pengguang Chen, Shu Liu, <b>Hengshuang Zhao</b>, Jiaya Jia.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2021.</br>
			[<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Distilling_Knowledge_via_Knowledge_Review_CVPR_2021_paper.pdf">Paper</a>]
			[<a href="https://github.com/dvlab-research/ReviewKD">Code</a>]
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/2021_cvpr_paconv.png"></div>
			<div class="col-md-9">
			<b><font color="black">PAConv: Position Adaptive Convolution with Dynamic Kernel Assembling on Point Clouds</font></b><br>
			Mutian Xu, Runyu Ding, <b>Hengshuang Zhao</b>, Xiaojuan Qi.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2021.</br>
			[<a href="https://arxiv.org/abs/2103.14635">Paper</a>]
			[<a href="https://github.com/CVMI-Lab/PAConv">Code</a>]
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/2021_cvpr_setr.png"></div>
			<div class="col-md-9">
			<b><font color="black">Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers</font></b><br>
			Sixiao Zheng, Jiachen Lu, <b>Hengshuang Zhao</b>, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip Torr, Li Zhang.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2021.</br>
			[<a href="https://fudan-zvg.github.io/SETR">Project</a>]
			[<a href="https://arxiv.org/abs/2012.15840">Paper</a>]
			[<a href="https://github.com/fudan-zvg/SETR">Code</a>]
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/2021_cvpr_cac.png"></div>
			<div class="col-md-9">
			<b><font color="black">Semi-supervised Semantic Segmentation with Directional Context-aware Consistency</font></b><br>
			Xin Lai, Zhuotao Tian, Li Jiang, Shu Liu, <b>Hengshuang Zhao</b>, Liwei Wang, Jiaya Jia.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2021.</br>
			[<a href="https://arxiv.org/abs/2106.14133">Paper</a>]
			[<a href="https://github.com/dvlab-research/Context-Aware-Consistency">Code</a>]
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">IJCAI</div><img class="img-fluid img-rounded" src="teaser/2021_ijcai_dccdn.png"></div>
			<div class="col-md-9">
			<b><font color="black">Dual-Cross Central Difference Network for Face Anti-Spoofing</font></b><br>
			Zitong Yu, Yunxiao Qin, <b>Hengshuang Zhao</b>, Xiaobai Li, Guoying Zhao.<br>
			International Joint Conference on Artificial Intelligence (<b>IJCAI</b>), 2021.</br>
			[<a href="https://arxiv.org/abs/2105.01290">Paper</a>]
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/2020_cvpr_san.png"></div>
			<div class="col-md-9">
			<b><font color="black">Exploring Self-attention for Image Recognition</font></b><br>
			<b>Hengshuang Zhao</b>, Jiaya Jia, Vladlen Koltun.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2020.</br>
			[<a href="paper/cvpr20_san.pdf">Paper</a>]
			[<a href="http://github.com/hszhao/SAN">Code</a>]
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR Oral</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""><source src="teaser/2020_cvpr_pointgroup.mp4" type="video/mp4"></video></div>
			<div class="col-md-9">
			<b><font color="black">PointGroup: Dual-Set Point Grouping for 3D Instance Segmentation</font></b><br>
			Li Jiang*, <b>Hengshuang Zhao*</b>, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, Jiaya Jia. (*: equal contribution)<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2020. <b><font color="firebrick">Oral</font></b></br>
			[<a href="http://arxiv.org/abs/2004.01658">Paper</a>]
			[<a href="http://github.com/dvlab-research/PointGroup">Code</a>]
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">TPAMI</div><img class="img-fluid img-rounded" src="teaser/2020_tpami_pfenet.png"></div>
			<div class="col-md-9">
			<b><font color="black">Prior Guided Feature Enrichment Network for Few-Shot Segmentation</font></b><br>
			Zhuotao Tian, <b>Hengshuang Zhao<sup>†</sup></b>, Michelle Shu, Zhicheng Yang, Ruiyu Li, Jiaya Jia. (†: corresponding)<br>
			IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>), 2020. </br>
			[<a href="paper/tpami20_pfenet.pdf">Paper</a>]
			[<a href="https://github.com/dvlab-research/PFENet">Code</a>]
			</div>
		</div><hr>

		<!--
		<div class="row">
			<div class="col-md-3"><div class="badge">arXiv</div><img class="img-fluid img-rounded" src="teaser/2020_arxiv_gridmask.png"></div>
			<div class="col-md-9">
			<b><font color="black">GridMask Data Augmentation</font></b><br>
			Pengguang Chen, Shu Liu, <b>Hengshuang Zhao</b>, Jiaya Jia.<br>
			arXiv, 2020.</br>
			[<a href="http://arxiv.org/abs/2001.04086">Paper</a>]
			[<a href="https://github.com/dvlab-research/GridMask">Code</a>]
			</div>
		</div><hr>-->

		<div class="row">
			<div class="col-md-3"><div class="badge">ICCV</div><img class="img-fluid img-rounded" src="teaser/2019_iccv_pointedge.png"></div>
			<div class="col-md-9">
			<b><font color="black">Hierarchical Point-Edge Interaction Network for Point Cloud Semantic Segmentation</font></b><br>
			Li Jiang, <b>Hengshuang Zhao</b>, Shu Liu, Xiaoyong Shen, Chi-Wing Fu, Jiaya Jia.<br>
			International Conference on Computer Vision (<b>ICCV</b>), 2019. </br>
			[<a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Jiang_Hierarchical_Point-Edge_Interaction_Network_for_Point_Cloud_Semantic_Segmentation_ICCV_2019_paper.pdf">Paper</a>]
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""><source src="teaser/2019_cvpr_pointweb.mp4" type="video/mp4"></video></div>
			<div class="col-md-9">
			<b><font color="black">PointWeb: Enhancing Local Neighborhood Features for Point Cloud Processing</font></b><br>
			<b>Hengshuang Zhao*</b>, Li Jiang*, Chi-Wing Fu, and Jiaya Jia. (*: equal contribution)<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2019.</br>
			[<a href="paper/cvpr19_pointweb.pdf">Paper</a>]
			[<a href="http://github.com/hszhao/PointWeb">Code</a>]
			[<a href="http://youtu.be/CaobqpsUP_4">Video</a>]
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR Oral</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""><source src="teaser/2019_cvpr_upsnet.mp4" type="video/mp4"></video></div>
			<div class="col-md-9">
			<b><font color="black">UPSNet: A Unified Panoptic Segmentation Network</font></b><br>
			Yuwen Xiong*, Renjie Liao*, <b>Hengshuang Zhao*</b>, Rui Hu, Min Bai, Ersin Yumer, Raquel Urtasun. (*: equal contribution)<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2019. <b><font color="firebrick">Oral</font></b></br>
			[<a href="http://arxiv.org/abs/1901.03784">Paper</a>]
			[<a href="http://github.com/uber-research/UPSNet">Code</a>]
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ECCV</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""><source src="teaser/2018_eccv_psanet.mp4" type="video/mp4"></video></div>
			<div class="col-md-9">
			<b><font color="black">PSANet: Point-wise Spatial Attention Network for Scene Parsing</font></b><br>
			<b>Hengshuang Zhao*</b>, Yi Zhang*, Shu Liu, Jianping Shi, Chen Change Loy, Dahua Lin, Jiaya Jia. (*: equal contribution)<br>
			European Conference on Computer Vision (<b>ECCV</b>), 2018.</br>
			Ranked 1st place in the CVPR 2018 <a href="http://bdd-data.berkeley.edu/wad-2018.html">WAD Drivable Area Segmentation Challenge</a>.</br>
			[<a href="project/psanet/index.html">Project</a>]
			[<a href="paper/eccv18_psanet.pdf">Paper</a>]
			[<a href="http://github.com/hszhao/PSANet">Caffe</a>]
			[<a href="http://github.com/hszhao/semseg">PyTorch</a>]
			[<a href="http://youtu.be/l5xu1DI6pDk">Video</a>]
			[<a href="paper/eccv18_psanet_supp.pdf">Supp</a>]
			[<a href="http://docs.google.com/presentation/d/1_brKNBtv8nVu_jOwFRGwVkEPAq8B8hEngBSQuZCWaZA/edit?usp=sharing">Slides in WAD @ CVPR 2018</a>]
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ECCV</div><img class="img-fluid img-rounded" src="teaser/2018_eccv_cais.png"></div>
			<div class="col-md-9">
			<b><font color="black">Compositing-aware Image Search</font></b><br>
			<b>Hengshuang Zhao</b>, Xiaohui Shen, Zhe Lin, Kalyan Sunkavalli, Brian Price, Jiaya Jia.<br>
			European Conference on Computer Vision (<b>ECCV</b>), 2018.</br>
			[<a href="project/cais/index.html">Project</a>]
			[<a href="paper/eccv18_cais.pdf">Paper</a>]
			[<a href="paper/eccv18_cais_supp.pdf">Supp</a>]
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ECCV</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""><source src="teaser/2018_eccv_segstereo.mp4" type="video/mp4"></video></div>
			<div class="col-md-9">
			<b><font color="black">SegStereo: Exploiting Semantic Information for Disparity Estimation</font></b><br>
			Guorun Yang*, <b>Hengshuang Zhao*</b>, Jianping Shi, Zhidong Deng, Jiaya Jia. (*: equal contribution)<br>
			European Conference on Computer Vision (<b>ECCV</b>), 2018.</br>
			[<a href="project/segstereo/index.html">Project</a>]
			[<a href="paper/eccv18_segstereo.pdf">Paper</a>]
			[<a href="http://github.com/yangguorun/SegStereo">Code</a>]
			[<a href="http://youtu.be/bfrlFpJQHT8">Video</a>]
			[<a href="paper/eccv18_segstereo_supp.pdf">Supp</a>]
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ECCV</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""><source src="teaser/2018_eccv_icnet.mp4" type="video/mp4"></video></div>
			<div class="col-md-9">
			<b><font color="black">ICNet for Real-Time Semantic Segmentation on High-Resolution Images</font></b><br>
			<b>Hengshuang Zhao</b>, Xiaojuan Qi, Xiaoyong Shen, Jianping Shi, Jiaya Jia.<br>
			European Conference on Computer Vision (<b>ECCV</b>), 2018.</br>
			[<a href="project/icnet/index.html">Project</a>]
			[<a href="paper/eccv18_icnet.pdf">Paper</a>]
			[<a href="http://github.com/hszhao/ICNet">Code</a>]
			[<a href="http://youtu.be/qWl9idsCuLQ">Video</a>]
			[<a href="paper/eccv18_icnet_supp.pdf">Supp</a>]
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/2017_cvpr_pspnet.png"></div>
			<div class="col-md-9">
			<b><font color="black">Pyramid Scene Parsing Network</font></b><br>
			<b>Hengshuang Zhao</b>, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, Jiaya Jia.</br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2017.</br>
			Ranked 1st place in the ECCV 2016 <a href="http://image-net.org/challenges/LSVRC/2016/results">ImageNet Scene Parsing Challenge</a>.</br>
			Ranked 1st place in the CVPR 2017 <a href="http://blog.mapillary.com/product/2017/06/13/lsun-challenge.html">LSUN Semantic Segmentation Challenge</a>.</br>
			[<a href="project/pspnet/index.html">Project</a>]
			[<a href="http://arxiv.org/abs/1612.01105">Paper</a>]
			[<a href="http://github.com/hszhao/PSPNet">Caffe</a>]
			[<a href="http://github.com/hszhao/semseg">PyTorch</a>]
			[<a href="http://youtu.be/rB1BmBOkKTw">Video</a>]
			[<a href="http://image-net.org/challenges/talks/2016/SenseCUSceneParsing.pdf">Slides in ILSVRC2016@ECCV2016</a>]
			</div>
		</div><hr>

	</div><br>
	<script>var scroll = new SmoothScroll('a[href*="#"]', {speed: 1000});</script>
</body>
</html>