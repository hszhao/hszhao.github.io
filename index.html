<!DOCTYPE html>
<html lang="en">

<head>
	<!-- Required meta tags -->
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
	<!-- Bootstrap core CSS -->
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/css/bootstrap.min.css" integrity="sha384-/Y6pD6FV/Vv2HJnA6t+vslU6fwYXjCFtcEpHbNJ0lyAFsXTsjBbfaDjzALeQsN6M" crossorigin="anonymous">
	<!-- https://fontawesome.com/cheatsheet -->
	<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons/css/academicons.min.css">
	<!-- Custom styles for this template -->
	<link rel="stylesheet" href="css/style.css">
	<link rel="icon" href="picture/hku.png">
    <script src="js/main.js"></script>
    <script src="js/scroll.js"></script>
    <meta name="keywords" content="Hengshuang Zhao, HKU, MIT, Oxford, CUHK, HUST, Computer Vision, Machine, Learning, Artificial Intelligence"> 
	<meta name="description" content="Personal page of Hengshuang Zhao">
</head>

<title>Hengshuang Zhao</title>

<body>
	<nav class="navbar navbar-expand-md navbar-dark fixed-top bg-dark" id="Home">
		<div class="container">
		<a class="navbar-brand" href="index.html">Hengshuang Zhao</a>
		<div class="collapse navbar-collapse" id="navbarToggle">
			<ul class="navbar-nav ml-auto">
				<li class="nav-item">
					<a class="nav-link" href="index.html">Home</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="lab.html">Lab</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="publication.html">Publication</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="award.html">Award</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="service.html">Service</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="talk.html">Talk</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="more.html">More</a>
				</li>
			</ul>
		</div>
		</div>
	</nav>

	<div class="container" style="padding-top: 20px; font-size: 17px">
		<div class="row">
			<div class="col-md-3", style="padding-right: 40px; padding-top: 0px">
				<img class="img-responsive img-rounded" src="picture/hengshuangzhao.png" alt="" style="max-width: 220px; solid black"><br>
			</div>
			<div class="col-md-5", style="padding-right: 40px; padding-top: 0px">
				<h2>Hengshuang Zhao</h2>
				<p>
					<br>Assistant Professor<br>
					Rm424, Chow Yei Ching Building<br>
					Department of Computer Science<br>
					The University of Hong Kong<br>
					Email: hszhao[at]cs.hku.hk<br>
					<a href="http://scholar.google.com/citations?user=4uE10I0AAAAJ&hl" target="_blank"><font color="black"><i class="ai ai-google-scholar ai-lg"></i></font></a>&nbsp;&nbsp;&nbsp;/&nbsp;&nbsp;
					<a href="http://github.com/hszhao" target="_blank"><font color="black"><i class="fab fa-github fa-lg"></i></font></a>&nbsp;&nbsp;&nbsp;/&nbsp;&nbsp;
					<a href="https://x.com/HengshuangZhao" target="_blank"><font color="black"><i class="fab fa-twitter fa-lg"></i></font></a>&nbsp;&nbsp;&nbsp;/&nbsp;&nbsp;
					<a href="https://linkedin.com/in/hengshuang-zhao-347b8391" target="_blank"><font color="black"><i class="fab fa-linkedin fa-lg"></i></font></a>&nbsp;&nbsp;&nbsp;/&nbsp;&nbsp;
					<a href="http://facebook.com/hengshuang.zhao" target="_blank"><font color="black"><i class="fab fa-facebook fa-lg"></i></font></a>
				</p>
			</div>
			<div class="col-md-4", style="padding-right: 40px; padding-top: 15px">
                <font color="black"><h5>Research Interests:</h5><br>
			  		<a>Computer Vision</a><br>
			  		<a>Scene Understanding</a><br>
			  		<a>Multimodal Learning</a><br>
					<a>Generative Modeling</a><br>
					<a>Autonomous Driving</a><br>
					<a>Embodied AI</a><br>
				</font>
			</div>
		</div>
		<div class="row">
			<div class="col-md-12">
			<p>
				<div style="text-align:justify">I am an Assistant Professor at the <a href="https://www.cs.hku.hk">Department of Computer Science</a> of <a href="https://www.hku.hk">The University of Hong Kong</a>. Previously, I have spent wonderful times as a Postdoctoral Researcher at <a href="https://www.csail.mit.edu">Computer Science and Artificial Intelligence Laboratory (CSAIL)</a> of <a href="https://www.mit.edu">MIT</a>, supervised by <a href="http://mit.edu/torralba">Prof. Antonio Torralba</a>, at <a href="https://torrvision.com">Torr Vision Group</a> of <a href="http://www.ox.ac.uk">University of Oxford</a> (beautiful Oxford), supervised by <a href="http://www.robots.ox.ac.uk/~phst">Prof. Philip Torr</a>. I obtained my Ph.D. degree from <a href="http://www.cuhk.edu.hk">The Chinese University of Hong Kong</a>, supervised by <a href="http://www.cse.cuhk.edu.hk/~leojia">Prof. Jiaya Jia</a>, and my bachelor's degree from <a href="https://www.hust.edu.cn">Huazhong University of Science and Technology</a>. During Ph.D., I have spent wonderful times as a Research Intern, working with <a href="http://xiaohuishen.github.io">Dr. Xiaohui Shen</a>, <a href="http://sites.google.com/site/zhelin625">Dr. Zhe Lin</a>, <a href="http://www.kalyans.org">Dr. Kalyan Sunkavalli</a>, <a href="http://www.brianpricephd.com">Dr. Brian Price</a> at Adobe (San Jose), <a href="http://www.cs.toronto.edu/~urtasun">Prof. Raquel Urtasun</a> at Uber (Toronto), and <a href="http://vladlen.info">Dr. Vladlen Koltun</a> at Intel (Santa Clara).</div>
			</p>
			<p>
				<div style="text-align:justify">My general research interests cover the broad area of computer vision, machine learning and artificial intelligence, with special emphasis on building intelligent visual systems. My research goal is to utilize artificial intelligence techniques to make machines perceive, understand, imagine, and interact with the surrounding environment, and ultimately make high positive impacts on various fields. Our current research interests and focus include: 1. visual scene understanding, perception, reconstruction, representation learning, multimodal learning; 2. generative modeling, visual content creation, generation, and manipulation (image/video/3d); 3. autonomous driving, embodied ai, robot learning, LLM applications etc.
			</p>
			<p>
				<div style="text-align:justify"><font color="red">Prospective students:</font> I am looking for self-motivated Ph.D. students, postdoctoral reseachers, research assistants, and visiting scholars, working together on exciting and cutting-edge computer vision, machine learning and artificial intelligence projects. If you are interested in working with me, please drop me an email with your resume. Available Ph.D. scholarships and opportunities include <a href="https://cerg1.ugc.edu.hk/hkpfs/index.html">Hong Kong PhD Fellowship Scheme (HKPFS)</a>, <a href="https://gradsch.hku.hk/prospective_students/fees_scholarships_and_financial_support/hku_presidential_phd_scholar_programme">HKU Presidential PhD Scholar Programme (HKUPS)</a>, and <a href="https://gradsch.hku.hk/prospective_students/fees_scholarships_and_financial_support/postgraduate_scholarships">Postgraduate Scholarships (PGS)</a>.</div>
			</p>
			</div>
		</div>
	</div>

  <!-- News -->
	<div class="container" style="padding-top: 20px; font-size: 17px">
		<h3 id="News" style="padding-top: 80px; margin-top: -80px; margin-left: -18px;">News</h3>
		<ul>
			<li>Our <a href="https://depth-anything-v2.github.io">Depth Anything</a> is integrated into <a href="https://developer.apple.com/machine-learning/models">Apple Core ML Models</a> for fantastic applications.</li>
			<li>Our <a href="https://depth-anything.github.io">Depth Anything</a> won the CVPR 2024 <a href="https://cvpr.thecvf.com/Conferences/2024/News/Wrap_Release">Best Demo Honorable Mention</a>.</li>
			<li>Our <a href="https://arxiv.org/abs/2312.10035">Point Transformer V3</a> won the CVPR 2024 <a href="https://waymo.com/open/challenges">Waymo 3D Semantic Segmentation Challenge Champion</a>.</li>
			<li>Invited talk at the CVPR 2024 <a href="https://sites.google.com/view/cvpr24-ac-workshop">Area Chair Workshop</a>.</li>
			<li>Invited talk at the ECCV 2024 Workshop on <a href="https://syntheticdata4cv.wordpress.com">Synthetic Data for Computer Vision</a>.</li>
			<li>Invited talk at the ECCV 2024 Workshop on <a href="https://lsvos.github.io">Large-scale Video Object Segmentation</a>.</li>
			<li>We are organizing the CVPR 2024 Tutorial on <a href="https://sites.google.com/view/pcn-cvpr24tutorial/homepage">All You Need To Know About Point Cloud Understanding</a>.</li>
			<li>We are organizing the ICML 2024 Workshop on <a href="icml-mfm-eai.github.io">Multimodal Foundation Models for Embodied Agents</a>.</li>
			<li>I am recognized as one of the most influential scholars in computer vision by AI 2000 in <a href="https://www.aminer.org/profile/hengshuang-zhao/542c02afdabfae216e61f9c4">2022, 2023, and 2024</a>.</li>
			<!--<li>I serve as an Area Chair for CVPR 2023, NeurIPS 2023, WACV 2023, CVPR 2024, ECCV 2024, NeurIPS 2024, ACMMM 2024.</li>-->
			<li>I serve as an Area Chair for CVPR 2023, NeurIPS 2023, CVPR 2024, ECCV 2024, NeurIPS 2024, ICLR 2025, CVPR 2025.</li>
			<li>I serve as a Senior Program Committee for AAAI 2023, AAAI 2024, and AAAI 2025.</li>
			<li>I serve as an Associate Editor for Pattern Recognition, and a Guest Editor for IEEE TCSVT.</li>
			<li><font color="red">Pinned projects:</font> 1. New innovations: <a href="https://depth-anything.github.io">Depth Anything V1</a> & <a href="https://depth-anything-v2.github.io">V2</a>, <a href="https://arxiv.org/abs/2312.10035">Point Transformer V3</a>, <a href="https://gpt4point.github.io">GPT4Point</a>, <a href="https://arxiv.org/abs/2402.18573">UniMODE</a>, <a href="https://ali-vilab.github.io/AnyDoor-Page">AnyDoor</a>, <a href="https://xavierchen34.github.io/LivePhoto-Page">LivePhoto</a>, <a href="https://xavierchen34.github.io/MimicBrush-Page">MimicBrush</a>, <a href="https://pixelgs.github.io">Pixel-GS</a>; 2. Highly optimized codebase available for 3D scene understanding <a href="https://github.com/Pointcept/Pointcept">Pointcept (PTv1&PTv2&PTv3&MSC&PPT)</a>; 3. Highly optimized codebase available for semantic segmentation <a href="http://github.com/hszhao/semseg">semseg (PSPNet&PSANet)</a>.</li>
		</ul>
	</div>

	<!-- Publications -->
	<div class="container" style="padding-top: 20px; font-size: 17px;">
		<h3 id="Publications" style="padding-top: 80px; margin-top: -80px; margin-left: -18px;">Selected Publications</h3>
		<p><a href="http://scholar.google.com/citations?user=4uE10I0AAAAJ&hl">Google Scholar</a> and <a href="publication.html">Full List</a>.</p>

		<div class="row">
			<div class="col-md-3"><div class="badge">NeurIPS</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""><source src="teaser/depthanythingv2.mp4" type="video/mp4"></video></div>
			<div class="col-md-9">
			<b><font color="black">Depth Anything V2</font></b><br>
			Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, <b>Hengshuang Zhao</b>.<br>
			Neural Information Processing Systems (<b>NeurIPS</b>), 2024.</br>
			[<a href="https://depth-anything-v2.github.io">Project</a>]
			[<a href="https://arxiv.org/abs/2406.09414">Paper</a>]
			[<a href="https://github.com/DepthAnything/Depth-Anything-V2">Code</a>]
			[<a href="https://huggingface.co/spaces/depth-anything/Depth-Anything-V2">Demo</a>]
			[<a href="https://x.com/_akhaliq/status/1801432403665125738">Media</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">NeurIPS</div><img class="img-fluid img-rounded" src="teaser/mimicbrush.gif"></div>
			<div class="col-md-9">
			<b><font color="black">Zero-shot Image Editing with Reference Imitation</font></b><br>
			Xi Chen, Yutong Feng, Mengting Chen, Yiyang Wang, Shilong Zhang, Yu Liu, Yujun Shen, <b>Hengshuang Zhao</b>.<br>
			Neural Information Processing Systems (<b>NeurIPS</b>), 2024.</br>
			[<a href="https://xavierchen34.github.io/MimicBrush-Page">Project</a>]
			[<a href="https://arxiv.org/abs/2406.07547">Paper</a>]
			[<a href="https://github.com/ali-vilab/MimicBrush">Code</a>]
			[<a href="https://huggingface.co/spaces/xichenhku/MimicBrush">Demo</a>]
			[<a href="https://x.com/_akhaliq/status/1800726257098760584">Media</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">NeurIPS</div><img class="img-fluid img-rounded" src="teaser/lit.jpg"></div>
			<div class="col-md-9">
			<b><font color="black">LiT: Unifying LiDAR "Languages" with LiDAR Translator</font></b><br>
			Yixing Lao, Tao Tang, Xiaoyang Wu, Peng Chen, Kaicheng Yu, <b>Hengshuang Zhao</b>.<br>
			Neural Information Processing Systems (<b>NeurIPS</b>), 2024.</br>
			[Project]
			[Paper]
			[Code]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">NeurIPS</div><img class="img-fluid img-rounded" src="teaser/syncvis.jpg"></div>
			<div class="col-md-9">
			<b><font color="black">SyncVIS: Synchronized Video Instance Segmentation</font></b><br>
			Rongkun Zheng, Lu Qi, Xi Chen, Yi Wang, Kun Wang, Yu Qiao, <b>Hengshuang Zhao</b>.<br>
			Neural Information Processing Systems (<b>NeurIPS</b>), 2024.</br>
			[Project]
			[Paper]
			[Code]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">NeurIPS</div><img class="img-fluid img-rounded" src="teaser/onedet3d.png"></div>
			<div class="col-md-9">
			<b><font color="black">One for All: Multi-Domain Joint Training for Point Cloud Based 3D Object Detection</font></b><br>
			Zhenyu Wang, Yali Li, <b>Hengshuang Zhao<sup>†</sup></b>, Shengjin Wang. (†: corresponding)<br>
			Neural Information Processing Systems (<b>NeurIPS</b>), 2024.</br>
			[Project]
			[Paper]
			[Code]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">NeurIPS</div><img class="img-fluid img-rounded" src="teaser/lion.jpg"></div>
			<div class="col-md-9">
			<b><font color="black">LION: Linear Group RNN for 3D Object Detection in Point Clouds</font></b><br>
			Zhe Liu, Jinghua Hou, Xinyu Wang, Xiaoqing Ye, Jingdong Wang, <b>Hengshuang Zhao</b>, Xiang Bai.<br>
			Neural Information Processing Systems (<b>NeurIPS</b>), 2024.</br>
			[<a href="https://happinesslz.github.io/projects/LION">Project</a>]
			[<a href="https://arxiv.org/abs/2407.18232">Paper</a>]
			[<a href="https://github.com/happinesslz/LION">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">arXiv</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""><source src="teaser/larm.mp4" type="video/mp4"></video></div>
			<div class="col-md-9">
			<b><font color="black">LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence</font></b><br>
			Zhuoling Li, Xiaogang Xu, Zhenhua Xu, Ser-Nam Lim, <b>Hengshuang Zhao</b>.<br>
			arXiv, 2024.</br>
			[<a href="https://lizhuoling.github.io/LARM_webpage">Project</a>]
			[<a href="https://arxiv.org/pdf/2405.17424">Paper</a>]
			[<a href="https://www.youtube.com/watch?v=7ajieVAuob8">Demo</a>]
			[<a href="https://www.youtube.com/watch?v=JxaqPpDPnec">Video</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ECCV</div><img class="img-fluid img-rounded" src="teaser/livephoto.gif"></div>
			<div class="col-md-9">
			<b><font color="black">LivePhoto: Real Image Animation with Text-guided Motion Control</font></b><br>
			Xi Chen, Zhiheng Liu, Mengting Chen, Yutong Feng, Yu Liu, Yujun Shen, <b>Hengshuang Zhao</b>.<br>
			European Conference on Computer Vision (<b>ECCV</b>), 2024.<br>
			[<a href="https://xavierchen34.github.io/LivePhoto-Page">Project</a>]
			[<a href="https://arxiv.org/abs/2312.02928">Paper</a>]
			[<a href="https://github.com/XavierCHEN34/LivePhoto">Code</a>]
			[<a href="https://www.youtube.com/watch?v=M2vzrTYAsQI">Video</a>]
            </div>
		</div><hr>

		<div class="row">
			<!-- <div class="col-md-3"><div class="badge">ECCV</div><img class="img-fluid img-rounded" src="teaser/pixelgs.png"></div> -->
			<div class="col-md-3"><div class="badge">ECCV</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""><source src="teaser/pixelgs.mp4" type="video/mp4"></video></div>
			<div class="col-md-9">
			<b><font color="black">Pixel-GS: Density Control with Pixel-aware Gradient for 3D Gaussian Splatting</font></b><br>
			Zheng Zhang, Wenbo Hu, Yixing Lao, Tong He, <b>Hengshuang Zhao</b>.<br>
			European Conference on Computer Vision (<b>ECCV</b>), 2024.<br>
			[<a href="https://pixelgs.github.io">Project</a>]
			[<a href="https://arxiv.org/abs/2403.15530">Paper</a>]
			[<a href="https://github.com/zhengzhang01/Pixel-GS">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ECCV</div><img class="img-fluid img-rounded" src="teaser/insmapper.png"></div>
			<div class="col-md-9">
			<b><font color="black">InsMapper: Exploring Inner-instance Information for Vectorized HD Mapping</font></b><br>
			Zhenhua Xu, Kwan-Yee. K. Wong, <b>Hengshuang Zhao</b>.<br>
			European Conference on Computer Vision (<b>ECCV</b>), 2024.<br>
			[<a href="https://tonyxuqaq.github.io/InsMapper">Project</a>]
			[<a href="https://arxiv.org/abs/2308.08543">Paper</a>]
			[<a href="https://github.com/TonyXuQAQ/InsMapper">Code</a>]
			[<a href="https://youtu.be/MapB7TrNnLY">Video</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ECCV</div><img class="img-fluid img-rounded" src="teaser/ovuni3detr.png"></div>
			<div class="col-md-9">
			<b><font color="black">OV-Uni3DETR: Towards Unified Open-Vocabulary 3D Object Detection via Cycle-Modality Propagation</font></b><br>
			Zhenyu Wang, Yali Li, Taichi Liu, <b>Hengshuang Zhao<sup>†</sup></b>, Shengjin Wang. (†: corresponding)<br>
			European Conference on Computer Vision (<b>ECCV</b>), 2024.<br>
			[<a href="https://arxiv.org/abs/2403.19580">Paper</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ECCV</div><img class="img-fluid img-rounded" src="teaser/logosticker.png"></div>
			<div class="col-md-9">
			<b><font color="black">LogoSticker: Inserting Logos into Diffusion Models for Customized Generation</font></b><br>
			Mingkang Zhu, Xi Chen, Zhongdao Wang, <b>Hengshuang Zhao</b>, Jiaya Jia.<br>
			European Conference on Computer Vision (<b>ECCV</b>), 2024.<br>
			[Project]
			[Paper]
			[Code]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ECCV</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""><source src="teaser/openins3d.mp4" type="video/mp4"></video></div>
			<div class="col-md-9">
			<b><font color="black">OpenIns3D: Snap and Lookup for 3D Open-vocabulary Instance Segmentation</font></b><br>
			Zhening Huang, Xiaoyang Wu, Xi Chen, <b>Hengshuang Zhao<sup>†</sup></b>, Lei Zhu, Joan Lasenby. (†: corresponding)<br>
			European Conference on Computer Vision (<b>ECCV</b>), 2024.<br>
			[<a href="https://zheninghuang.github.io/OpenIns3D">Project</a>]
			[<a href="https://arxiv.org/abs/2309.00616">Paper</a>]
			[<a href="https://github.com/Pointcept/OpenIns3D">Code</a>]
			[<a href="https://www.youtube.com/watch?v=kwlMJkEfTyY">Video</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ECCV</div><img class="img-fluid img-rounded" src="teaser/diki.png"></div>
			<div class="col-md-9">
			<b><font color="black">Mind the Interference: Retaining Pre-trained Knowledge in Parameter Efficient Continual Learning of Vision-Language Models</font></b><br>
			Longxiang Tang, Zhuotao Tian, Kai Li, Chunming He, Hantao Zhou, <b>Hengshuang Zhao</b>, Xiu Li, Jiaya Jia.<br>
			European Conference on Computer Vision (<b>ECCV</b>), 2024.<br>
			[<a href="https://arxiv.org/abs/2407.05342">Paper</a>]
			[<a href="https://github.com/lloongx/DIKI">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">TPAMI</div><img class="img-fluid img-rounded" src="teaser/unidetector2.png"></div>
			<div class="col-md-9">
			<b><font color="black">UniDetector: Towards Universal Object Detection with Heterogeneous Supervision</font></b><br>
			Zhenyu Wang, Yali Li, Xi Chen, Ser-Nam Lim, Antonio Torralba, <b>Hengshuang Zhao<sup>†</sup></b>, Shengjin Wang. (†: corresponding)<br>
			IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>), 2024.</br>
			[<a href="https://www.computer.org/csdl/journal/tp/5555/01/10552883/1XH2IgI36Cc">Paper</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">RA-L</div><img class="img-fluid img-rounded" src="teaser/drivegpt4.gif"></div>
			<div class="col-md-9">
			<b><font color="black">DriveGPT4: Interpretable End-to-end Autonomous Driving via Large Language Model</font></b><br>
			Zhenhua Xu, Yujia Zhang, Enze Xie, Zhen Zhao, Yong Guo, Kwan-Yee. K. Wong, Zhenguo Li, <b>Hengshuang Zhao</b>.<br>
			IEEE Robotics and Automation Letters (<b>RA-L</b>), 2024.</br>
			[<a href="https://tonyxuqaq.github.io/projects/DriveGPT4">Project</a>]
			[<a href="https://arxiv.org/abs/2310.01412">Paper</a>]
			[Code]
			[<a href="https://youtu.be/CPoskBNjHlk">Video</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""><source src="teaser/depthanything.mp4" type="video/mp4"></video></div>
			<div class="col-md-9">
			<b><font color="black">Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data</font></b><br>
			Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, <b>Hengshuang Zhao</b>.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024.</br>
			[<a href="https://depth-anything.github.io">Project</a>]
			[<a href="https://arxiv.org/abs/2401.10891">Paper</a>]
			[<a href="https://github.com/LiheYoung/Depth-Anything">Code</a>]
			[<a href="https://huggingface.co/spaces/LiheYoung/Depth-Anything">Demo</a>]
			[<a href="https://twitter.com/_akhaliq/status/1749284669936275463">Media</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/anydoor.gif"></div>
			<div class="col-md-9">
			<b><font color="black">AnyDoor: Zero-shot Object-level Image Customization</font></b><br>
			Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, <b>Hengshuang Zhao</b>.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024.</br>
			[<a href="https://ali-vilab.github.io/AnyDoor-Page">Project</a>]
			[<a href="https://arxiv.org/abs/2307.09481">Paper</a>]
			[<a href="https://github.com/ali-vilab/AnyDoor">Code</a>]
			[<a href="https://huggingface.co/spaces/xichenhku/AnyDoor-online">Demo</a>]
			[<a href="https://twitter.com/_akhaliq/status/1738772616142303728">Media</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR Oral</div><img class="img-fluid img-rounded" src="teaser/pointtransformerv3.png"></div>
			<div class="col-md-9">
			<b><font color="black">Point Transformer V3: Simpler, Faster, Stronger</font></b><br>
			Xiaoyang Wu, Li Jiang, Peng-Shuai Wang, Zhijian Liu, Xihui Liu, Yu Qiao, Wanli Ouyang, Tong He, <b>Hengshuang Zhao</b>.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024. <b><font color="firebrick">Oral</font></b></br>
			Ranked 1st place in the CVPR 2024 <a href="https://waymo.com/open/challenges">Waymo 3D Semantic Segmentation Challenge</a>.</br>
			[<a href="https://arxiv.org/abs/2312.10035">Paper</a>]
			[<a href="https://github.com/Pointcept/PointTransformerV3">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/ppt.png"></div>
			<div class="col-md-9">
			<b><font color="black">Towards Large-scale 3D Representation Learning with Multi-dataset Point Prompt Training</font></b><br>
			Xiaoyang Wu, Zhuotao Tian, Xin Wen, Bohao Peng, Xihui Liu, Kaicheng Yu, <b>Hengshuang Zhao</b>.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024.</br>
			[<a href="https://arxiv.org/abs/2308.09718">Paper</a>]
			[<a href="https://github.com/Pointcept/Pointcept">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR Highlight</div><img class="img-fluid img-rounded" src="teaser/gpt4point.png"></div>
			<div class="col-md-9">
			<b><font color="black">GPT4Point: A Unified Framework for Point-Language Understanding and Generation</font></b><br>
			Zhangyang Qi, Ye Fang, Zeyi Sun, Xiaoyang Wu, Tong Wu, Jiaqi Wang, Dahua Lin, <b>Hengshuang Zhao</b>.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024. <b><font color="firebrick">Highlight</font></b></br>
			[<a href="https://gpt4point.github.io">Project</a>]
			[<a href="https://arxiv.org/abs/2312.02980">Paper</a>]
			[<a href="https://github.com/Pointcept/GPT4Point">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR Highlight</div><img class="img-fluid img-rounded" src="teaser/unimode.png"></div>
			<div class="col-md-9">
			<b><font color="black">UniMODE: Universal Monocular 3D Object Detection</font></b><br>
			Zhuoling Li, Xiaogang Xu, Ser-Nam Lim, <b>Hengshuang Zhao</b>.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024. <b><font color="firebrick">Highlight</font></b></br>
			[<a href="https://arxiv.org/abs/2402.18573">Paper</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/groupcontrast.png"></div>
			<div class="col-md-9">
			<b><font color="black">GroupContrast: Semantic-aware Self-supervised Representation Learning for 3D Understanding</font></b><br>
			Chengyao Wang, Li Jiang, Xiaoyang Wu, Zhuotao Tian, Bohao Peng, <b>Hengshuang Zhao<sup>†</sup></b>, Jiaya Jia. (†: corresponding)<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024.</br>
			[<a href="https://arxiv.org/abs/2403.09639">Paper</a>]
			[<a href="https://github.com/dvlab-research/GroupContrast">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/oacnns.png"></div>
			<div class="col-md-9">
			<b><font color="black">OA-CNNs: Omni-Adaptive Sparse CNNs for 3D Semantic Segmentation</font></b><br>
			Bohao Peng, Xiaoyang Wu, Li Jiang, Yukang Chen, <b>Hengshuang Zhao</b>, Zhuotao Tian, Jiaya Jia.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024.</br>
			[<a href="https://arxiv.org/abs/2403.14418">Paper</a>]
			[<a href="https://github.com/Pointcept/Pointcept">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""><source src="teaser/dreamcomposer.mov" type="video/mp4"></video></div>
			<div class="col-md-9">
			<b><font color="black">DreamComposer: Controllable 3D Object Generation via Multi-View Conditions</font></b><br>
			Yunhan Yang, Yukun Huang, Xiaoyang Wu, Yuan-Chen Guo, Song-Hai Zhang, <b>Hengshuang Zhao</b>, Tong He, Xihui Liu.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024.</br>
			[<a href="https://yhyang-myron.github.io/DreamComposer">Project</a>]
			[<a href="https://arxiv.org/abs/2312.03611">Paper</a>]
			[<a href="https://github.com/yhyang-myron/DreamComposer">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/zsvg3d.png"></div>
			<div class="col-md-9">
			<b><font color="black">Visual Programming for Zero-shot Open-Vocabulary 3D Visual Grounding</font></b><br>
			Zhihao Yuan, Jinke Ren, Chun-Mei Feng, <b>Hengshuang Zhao</b>, Shuguang Cui, Zhen Li.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024.</br>
			[<a href="https://curryyuan.github.io/ZSVG3D">Project</a>]
			[<a href="https://arxiv.org/abs/2311.15383">Paper</a>]
			[<a href="https://github.com/CurryYuan/ZSVG3D">Code</a>]
			[<a href="https://youtu.be/YGP1R3IgVWU">Video</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/unipad.png"></div>
			<div class="col-md-9">
			<b><font color="black">UniPAD: A Universal Pre-training Paradigm for Autonomous Driving</font></b><br>
			Honghui Yang, Sha Zhang, Di Huang, Xiaoyang Wu, Haoyi Zhu, Tong He, Shixiang Tang, <b>Hengshuang Zhao</b>, Qibo Qiu, Binbin Lin, Xiaofei He, Wanli Ouyang.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024.</br>
			[<a href="https://arxiv.org/abs/2310.08370">Paper</a>]
			[<a href="https://github.com/Nightmare-n/UniPAD">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ICLR Highlight</div><img class="img-fluid img-rounded" src="teaser/iba.png"></div>
			<div class="col-md-9">
			<b><font color="black">Influencer Backdoor Attack on Semantic Segmentation</font></b><br>
			Haoheng Lan, Jindong Gu, Philip Torr, <b>Hengshuang Zhao</b>.<br>
			International Conference on Learning Representations (<b>ICLR</b>), 2024. <b><font color="firebrick">Highlight</font></b></br>
			[<a href="https://arxiv.org/abs/2303.12054">Paper</a>]
			[<a href="https://github.com/Maxproto/IBA">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">NeurIPS</div><img class="img-fluid img-rounded" src="teaser/freemask.png"></div>
			<div class="col-md-9">
			<b><font color="black">FreeMask: Synthetic Images with Dense Annotations Make Stronger Segmentation Models</font></b><br>
			Lihe Yang, Xiaogang Xu, Bingyi Kang, Yinghuan Shi, <b>Hengshuang Zhao</b>.<br>
			Neural Information Processing Systems (<b>NeurIPS</b>), 2023.</br>
			[<a href="https://arxiv.org/abs/2310.15160">Paper</a>]
			[<a href="https://github.com/LiheYoung/FreeMask">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">NeurIPS</div><img class="img-fluid img-rounded" src="teaser/uni3detr.png"></div>
			<div class="col-md-9">
			<b><font color="black">Uni3DETR: Unified 3D Detection Transformer</font></b><br>
			Zhenyu Wang, Yali Li, Xi Chen, <b>Hengshuang Zhao<sup>†</sup></b>, Shengjin Wang. (†: corresponding)<br>
			Neural Information Processing Systems (<b>NeurIPS</b>), 2023.</br>
			[<a href="https://arxiv.org/abs/2310.05699">Paper</a>]
			[<a href="https://github.com/zhenyuw16/Uni3DETR">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">NeurIPS</div><img class="img-fluid img-rounded" src="teaser/tmtvis.png"></div>
			<div class="col-md-9">
			<b><font color="black">TMT-VIS: Taxonomy-aware Multi-dataset Joint Training for Video Instance Segmentation</font></b><br>
			Rongkun Zheng, Lu Qi, Xi Chen, Yi Wang, Kun Wang, Yu Qiao, <b>Hengshuang Zhao</b>.<br>
			Neural Information Processing Systems (<b>NeurIPS</b>), 2023.</br>
			[<a href="https://arxiv.org/abs/2312.06630">Paper</a>]
			[<a href="https://github.com/rkzheng99/TMT-VIS">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">NeurIPS</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""><source src="teaser/corresnerf.mp4" type="video/mp4"></video></div>
			<div class="col-md-9">
			<b><font color="black">CorresNeRF: Image Correspondence Priors for Neural Radiance Fields</font></b><br>
			Yixing Lao, Xiaogang Xu, Zhipeng Cai, Xihui Liu, <b>Hengshuang Zhao</b>.<br>
			Neural Information Processing Systems (<b>NeurIPS</b>), 2023.</br>
			[<a href="https://yxlao.github.io/corres-nerf">Project</a>]
			[<a href="https://arxiv.org/abs/2312.06642">Paper</a>]
			[<a href="https://github.com/yxlao/corres-nerf">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ICCV</div><img class="img-fluid img-rounded" src="teaser/opsnet.gif"></div>
			<div class="col-md-9">
			<b><font color="black">Open-vocabulary Panoptic Segmentation with Embedding Modulation</font></b><br>
			Xi Chen, Shuang Li, Ser-Nam Lim, Antonio Torralba, <b>Hengshuang Zhao</b>.<br>
			International Conference on Computer Vision (<b>ICCV</b>), 2023.</br>
			[<a href="https://opsnet-page.github.io">Project</a>]
			[<a href="https://arxiv.org/abs/2303.11324">Paper</a>]
			[<a href="https://github.com/XavierCHEN34/OPSNet">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ICCV</div><img class="img-fluid img-rounded" src="teaser/shrinkmatch.png"></div>
			<div class="col-md-9">
			<b><font color="black">Shrinking Class Space for Enhanced Certainty in Semi-Supervised Learning</font></b><br>
			Lihe Yang, Zhen Zhao, Lei Qi, Yu Qiao, Yinghuan Shi, <b>Hengshuang Zhao</b>.<br>
			International Conference on Computer Vision (<b>ICCV</b>), 2023.</br>
			[<a href="https://arxiv.org/abs/2308.06777">Paper</a>]
			[<a href="https://github.com/LiheYoung/ShrinkMatch">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ICCVW</div><img class="img-fluid img-rounded" src="teaser/sam3d.png"></div>
			<div class="col-md-9">
			<b><font color="black">SAM3D: Segment Anything in 3D Scenes</font></b><br>
			Yunhan Yang, Xiaoyang Wu, Tong He, <b>Hengshuang Zhao</b>, Xihui Liu.<br>
			International Conference on Computer Vision Workshop (<b>ICCVW</b>), 2023.</br>
			[<a href="https://arxiv.org/abs/2306.03908">Paper</a>]
			[<a href="https://github.com/Pointcept/SegmentAnything3D">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/msc.png"></div>
			<div class="col-md-9">
			<b><font color="black">Masked Scene Contrast: A Scalable Framework for Unsupervised 3D Representation Learning</font></b><br>
			Xiaoyang Wu, Xin Wen, Xihui Liu, <b>Hengshuang Zhao</b>.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2023.</br>
			[<a href="https://arxiv.org/abs/2303.14191">Paper</a>]
			[<a href="https://github.com/Pointcept/Pointcept">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/unidetector.png"></div>
			<div class="col-md-9">
			<b><font color="black">Detecting Everything in the Open World: Towards Universal Object Detection</font></b><br>
			Zhenyu Wang, Yali Li, Xi Chen, Ser-Nam Lim, Antonio Torralba, <b>Hengshuang Zhao<sup>†</sup></b>, Shengjin Wang. (†: corresponding)<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2023.</br>
			[<a href="https://arxiv.org/abs/2303.11749">Paper</a>]
			[<a href="https://github.com/zhenyuw16/UniDetector">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">NeurIPS</div><img class="img-fluid img-rounded" src="teaser/pointtransformerv2.png"></div>
			<div class="col-md-9">
			<b><font color="black">Point Transformer V2: Grouped Vector Attention and Partition-based Pooling</font></b><br>
			Xiaoyang Wu, Yixing Lao, Li Jiang, Xihui Liu, <b>Hengshuang Zhao</b>.<br>
			Neural Information Processing Systems (<b>NeurIPS</b>), 2022.</br>
			[<a href="https://arxiv.org/abs/2210.05666">Paper</a>]
			[<a href="https://github.com/Gofinge/PointTransformerV2">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ECCV</div><img class="img-fluid img-rounded" src="teaser/mtformer.png"></div>
			<div class="col-md-9">
			<b><font color="black">MTFormer: Multi-Task Learning via Transformer and Cross-Task Reasoning</font></b><br>
			Xiaogang Xu*, <b>Hengshuang Zhao*</b>, Vibhav Vineet, Ser-Nam Lim, Antonio Torralba. (*: equal contribution)<br>
			European Conference on Computer Vision (<b>ECCV</b>), 2022.</br>
			[<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136870299.pdf">Paper</a>]			
			[<a href="https://github.com/xiaogang00/MTFormer">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/focalclick.png"></div>
			<div class="col-md-9">
			<b><font color="black">FocalClick: Towards Practical Interactive Image Segmentation</font></b><br>
			Xi Chen, Zhiyan Zhao, Yilei Zhang, Manni Duan, Donglian Qi, <b>Hengshuang Zhao</b>.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2022.</br>
			[<a href="https://arxiv.org/abs/2204.02574">Paper</a>]
			[<a href="https://github.com/XavierCHEN34/ClickSEG">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">TPAMI</div><img class="img-fluid img-rounded" src="teaser/panopticfcn2.png"></div>
			<div class="col-md-9">
			<b><font color="black">Fully Convolutional Networks for Panoptic Segmentation with Point-based Supervision</font></b><br>
			Yanwei Li, <b>Hengshuang Zhao</b>, Xiaojuan Qi, Yukang Chen, Lu Qi, Liwei Wang, Zeming Li, Jian Sun, Jiaya Jia.<br>
			IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>), 2022. </br>
			[<a href="https://arxiv.org/abs/2108.07682">Paper</a>]
			[<a href="https://github.com/dvlab-research/PanopticFCN">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">TPAMI</div><img class="img-fluid img-rounded" src="teaser/entity.png"></div>
			<div class="col-md-9">
			<b><font color="black">Open World Entity Segmentation</font></b><br>
			Lu Qi, Jason Kuen, Yi Wang, Jiuxiang Gu, <b>Hengshuang Zhao</b>, Philip Torr, Zhe Lin, Jiaya Jia.<br>
			IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>), 2022. </br>
			[<a href="http://luqi.info/Entity_Web">Project</a>]
			[<a href="https://arxiv.org/abs/2107.14228">Paper</a>]
			[<a href="https://github.com/dvlab-research/Entity">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">NeurIPS</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""><source src="teaser/unitrack.mp4" type="video/mp4"></video></div>
			<div class="col-md-9">
			<b><font color="black">Do Different Tracking Tasks Require Different Appearance Models?</font></b><br>
			Zhongdao Wang, <b>Hengshuang Zhao</b>, Yali Li, Shengjin Wang, Philip Torr, Luca Bertinetto.<br>
			Neural Information Processing Systems (<b>NeurIPS</b>), 2021.</br>
			[<a href="https://zhongdao.github.io/UniTrack">Project</a>]
			[<a href="https://arxiv.org/abs/2107.02156">Paper</a>]
			[<a href="https://github.com/Zhongdao/UniTrack">Code</a>]
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ICCV Oral</div><img class="img-fluid img-rounded" src="teaser/pointtransformer.png"></div>
			<div class="col-md-9">
			<b><font color="black">Point Transformer</font></b><br>
			<b>Hengshuang Zhao</b>, Li Jiang, Jiaya Jia, Philip Torr, Vladlen Koltun.<br>
			International Conference on Computer Vision (<b>ICCV</b>), 2021. <b><font color="firebrick">Oral</font></b></br>
			[<a href="http://arxiv.org/abs/2012.09164">Paper</a>]
			[<a href="https://github.com/Gofinge/PointTransformerV2/tree/main/pcr/models/point_transformer">Code</a>]
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR Oral</div><img class="img-fluid img-rounded" src="teaser/bpnet.png"></div>
			<div class="col-md-9">
			<b><font color="black">Bidirectional Projection Network for Cross Dimension Scene Understanding</font></b><br>
			Wenbo Hu*, <b>Hengshuang Zhao*</b>, Li Jiang, Jiaya Jia, Tien-Tsin Wong. (*: equal contribution)<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2021. <b><font color="firebrick">Oral</font></b></br>
			[<a href="https://wbhu.github.io/project/BPNet">Project</a>]
			[<a href="https://arxiv.org/abs/2103.14326">Paper</a>]
			[<a href="https://github.com/wbhu/BPNet">Code</a>]
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR Oral</div><img class="img-fluid img-rounded" src="teaser/panopticfcn.png"></div>
			<div class="col-md-9">
			<b><font color="black">Fully Convolutional Networks for Panoptic Segmentation</font></b><br>
			Yanwei Li, <b>Hengshuang Zhao</b>, Xiaojuan Qi, Liwei Wang, Zeming Li, Jian Sun, Jiaya Jia.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2021. <b><font color="firebrick">Oral</font></b></br>
			[<a href="http://arxiv.org/abs/2012.00720">Paper</a>]
			[<a href="http://github.com/yanwei-li/PanopticFCN">Code</a>]
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/setr.png"></div>
			<div class="col-md-9">
			<b><font color="black">Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers</font></b><br>
			Sixiao Zheng, Jiachen Lu, <b>Hengshuang Zhao</b>, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip Torr, Li Zhang.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2021.</br>
			[<a href="https://fudan-zvg.github.io/SETR">Project</a>]
			[<a href="https://arxiv.org/abs/2012.15840">Paper</a>]
			[<a href="https://github.com/fudan-zvg/SETR">Code</a>]
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/san.png"></div>
			<div class="col-md-9">
			<b><font color="black">Exploring Self-attention for Image Recognition</font></b><br>
			<b>Hengshuang Zhao</b>, Jiaya Jia, Vladlen Koltun.<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2020.</br>
			[<a href="paper/cvpr20_san.pdf">Paper</a>]
			[<a href="http://github.com/hszhao/SAN">Code</a>]
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR Oral</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""><source src="teaser/pointgroup.mp4" type="video/mp4"></video></div>
			<div class="col-md-9">
			<b><font color="black">PointGroup: Dual-Set Point Grouping for 3D Instance Segmentation</font></b><br>
			Li Jiang*, <b>Hengshuang Zhao*</b>, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, Jiaya Jia. (*: equal contribution)<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2020. <b><font color="firebrick">Oral</font></b></br>
			[<a href="http://arxiv.org/abs/2004.01658">Paper</a>]
			[<a href="http://github.com/dvlab-research/PointGroup">Code</a>]
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""><source src="teaser/pointweb.mp4" type="video/mp4"></video></div>
			<div class="col-md-9">
			<b><font color="black">PointWeb: Enhancing Local Neighborhood Features for Point Cloud Processing</font></b><br>
			<b>Hengshuang Zhao*</b>, Li Jiang*, Chi-Wing Fu, and Jiaya Jia. (*: equal contribution)<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2019.</br>
			[<a href="paper/cvpr19_pointweb.pdf">Paper</a>]
			[<a href="http://github.com/hszhao/PointWeb">Code</a>]
			[<a href="http://youtu.be/CaobqpsUP_4">Video</a>]
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR Oral</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""><source src="teaser/upsnet.mp4" type="video/mp4"></video></div>
			<div class="col-md-9">
			<b><font color="black">UPSNet: A Unified Panoptic Segmentation Network</font></b><br>
			Yuwen Xiong*, Renjie Liao*, <b>Hengshuang Zhao*</b>, Rui Hu, Min Bai, Ersin Yumer, Raquel Urtasun. (*: equal contribution)<br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2019. <b><font color="firebrick">Oral</font></b></br>
			[<a href="http://arxiv.org/abs/1901.03784">Paper</a>]
			[<a href="http://github.com/uber-research/UPSNet">Code</a>]
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ECCV</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""><source src="teaser/psanet.mp4" type="video/mp4"></video></div>
			<div class="col-md-9">
			<b><font color="black">PSANet: Point-wise Spatial Attention Network for Scene Parsing</font></b><br>
			<b>Hengshuang Zhao*</b>, Yi Zhang*, Shu Liu, Jianping Shi, Chen Change Loy, Dahua Lin, Jiaya Jia. (*: equal contribution)<br>
			European Conference on Computer Vision (<b>ECCV</b>), 2018.</br>
			Ranked 1st place in the CVPR 2018 <a href="http://bdd-data.berkeley.edu/wad-2018.html">WAD Drivable Area Segmentation Challenge</a>.</br>
			[<a href="project/psanet/index.html">Project</a>]
			[<a href="paper/eccv18_psanet.pdf">Paper</a>]
			[<a href="http://github.com/hszhao/PSANet">Caffe</a>]
			[<a href="http://github.com/hszhao/semseg">PyTorch</a>]
			[<a href="http://youtu.be/l5xu1DI6pDk">Video</a>]
			[<a href="paper/eccv18_psanet_supp.pdf">Supp</a>]
			[<a href="http://docs.google.com/presentation/d/1_brKNBtv8nVu_jOwFRGwVkEPAq8B8hEngBSQuZCWaZA/edit?usp=sharing">Slides in WAD @ CVPR 2018</a>]
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ECCV</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""><source src="teaser/segstereo.mp4" type="video/mp4"></video></div>
			<div class="col-md-9">
			<b><font color="black">SegStereo: Exploiting Semantic Information for Disparity Estimation</font></b><br>
			Guorun Yang*, <b>Hengshuang Zhao*</b>, Jianping Shi, Zhidong Deng, Jiaya Jia. (*: equal contribution)<br>
			European Conference on Computer Vision (<b>ECCV</b>), 2018.</br>
			[<a href="project/segstereo/index.html">Project</a>]
			[<a href="paper/eccv18_segstereo.pdf">Paper</a>]
			[<a href="http://github.com/yangguorun/SegStereo">Code</a>]
			[<a href="http://youtu.be/bfrlFpJQHT8">Video</a>]
			[<a href="paper/eccv18_segstereo_supp.pdf">Supp</a>]
			</div>
		</div><hr>
		
		<div class="row">
			<div class="col-md-3"><div class="badge">ECCV</div><video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""><source src="teaser/icnet.mp4" type="video/mp4"></video></div>
			<div class="col-md-9">
			<b><font color="black">ICNet for Real-Time Semantic Segmentation on High-Resolution Images</font></b><br>
			<b>Hengshuang Zhao</b>, Xiaojuan Qi, Xiaoyong Shen, Jianping Shi, Jiaya Jia.<br>
			European Conference on Computer Vision (<b>ECCV</b>), 2018.</br>
			[<a href="project/icnet/index.html">Project</a>]
			[<a href="paper/eccv18_icnet.pdf">Paper</a>]
			[<a href="http://github.com/hszhao/ICNet">Code</a>]
			[<a href="http://youtu.be/qWl9idsCuLQ">Video</a>]
			[<a href="paper/eccv18_icnet_supp.pdf">Supp</a>]
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/pspnet.png"></div>
			<div class="col-md-9">
			<b><font color="black">Pyramid Scene Parsing Network</font></b><br>
			<b>Hengshuang Zhao</b>, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, Jiaya Jia.</br>
			Computer Vision and Pattern Recognition (<b>CVPR</b>), 2017.</br>
			Ranked 1st place in the ECCV 2016 <a href="http://image-net.org/challenges/LSVRC/2016/results">ImageNet Scene Parsing Challenge</a>.</br>
			Ranked 1st place in the CVPR 2017 <a href="http://blog.mapillary.com/product/2017/06/13/lsun-challenge.html">LSUN Semantic Segmentation Challenge</a>.</br>
			[<a href="project/pspnet/index.html">Project</a>]
			[<a href="http://arxiv.org/abs/1612.01105">Paper</a>]
			[<a href="http://github.com/hszhao/PSPNet">Caffe</a>]
			[<a href="http://github.com/hszhao/semseg">PyTorch</a>]
			[<a href="http://youtu.be/rB1BmBOkKTw">Video</a>]
			[<a href="http://image-net.org/challenges/talks/2016/SenseCUSceneParsing.pdf">Slides in ILSVRC2016@ECCV2016</a>]
			</div>
		</div>

	</div><br>
	<script>var scroll = new SmoothScroll('a[href*="#"]', {speed: 1000});</script>
</body>
</html>